# -*- coding: utf-8 -*-
from __future__ import unicode_literals

import cStringIO
from collections import defaultdict, OrderedDict
from copy import deepcopy
from decimal import Decimal
from functools import wraps
import hashlib
import io
import json
import logging
from operator import itemgetter
import os
import random
import re
from tempfile import SpooledTemporaryFile, NamedTemporaryFile
import time
import urllib
from urlparse import urlparse
from zipfile import ZipFile, ZipInfo

from aldjemy.core import get_engine, get_tables
import aldjemy.core
from django.conf import settings
from django.conf.urls import url
from django.core.cache import caches
from django.core.exceptions import ObjectDoesNotExist, PermissionDenied
from django.core.serializers.json import DjangoJSONEncoder
from django.db import connection
from django.db import transaction
from django.db.models import F, Q
from django.db.utils import ProgrammingError
from django.forms.models import model_to_dict
from django.http import Http404
from django.http.request import HttpRequest
from django.http.response import StreamingHttpResponse, HttpResponse
from django.utils import timezone
import six
from sqlalchemy import select, text, case
import sqlalchemy
from sqlalchemy.dialects import postgresql
from sqlalchemy.dialects.postgresql import array
from sqlalchemy.sql import and_, or_, not_, asc, desc, alias, Alias, func
from sqlalchemy.sql.elements import literal_column
from sqlalchemy.sql.expression import column, join, insert, delete, distinct, \
    exists, cast, union_all, union, bindparam
from sqlalchemy.sql.expression import nullslast
import sqlalchemy.sql.schema
import sqlalchemy.sql.sqltypes
import unicodecsv
import xlsxwriter

from db import WELL_ID_PATTERN, WELL_NAME_PATTERN, PLATE_PATTERN, \
    PLATE_RANGE_PATTERN, COPY_NAME_PATTERN
from db.models import ScreensaverUser, Screen, \
    ScreenResult, DataColumn, Library, Plate, Copy, \
    CopyWell, PlateLocation, Reagent, Well, Activity, \
    SmallMoleculeReagent, SilencingReagent, GeneSymbol, \
    NaturalProductReagent, Molfile, Gene, GeneGenbankAccessionNumber, \
    CherryPickRequest, CherryPickAssayPlate, CherryPickLiquidTransfer, \
    CachedQuery, UserChecklist, AttachedFile, \
    LabActivity, Screening, LibraryScreening, AssayPlate, \
    SmallMoleculeChembankId, SmallMoleculePubchemCid, SmallMoleculeChemblId, \
    SmallMoleculeCompoundName, ScreenCellLines, ScreenFundingSupports, \
    ScreenKeyword, ResultValue, AssayWell, Publication, ScreenerCherryPick, \
    LabCherryPick, CherryPickScreening, \
    LabAffiliation, UserAgreement, RawDataTransform, RawDataInputFile
from db.schema import VOCAB
from db.support import lims_utils, screen_result_importer, bin_packer, \
    raw_data_reader, plate_matrix_transformer
from db.support.plate_matrix_transformer import Collation
from db.support.screen_result_importer import PARTITION_POSITIVE_MAPPING, \
    CONFIRMED_POSITIVE_MAPPING
from lims.app_data import APP_PUBLIC_DATA
from reports import BadRequestError, ValidationError, InformationError, \
    ApiNotImplemented, MissingParam, _now
from reports import LIST_DELIMITER_SQL_ARRAY, LIST_DELIMITER_URL_PARAM, \
    HTTP_PARAM_USE_TITLES, HTTP_PARAM_USE_VOCAB, HTTP_PARAM_DATA_INTERCHANGE, \
    LIST_BRACKETS, HTTP_PARAM_RAW_LISTS, HEADER_APILOG_COMMENT, ValidationError, \
    CumulativeError, LIST_DELIMITER_SUB_ARRAY
from reports.api import API_PARAM_OVERRIDE, \
    API_PARAM_PATCH_PREVIEW_MODE, API_PARAM_NO_BACKGROUND, API_PARAM_PREVIEW_LOGS, \
    API_PARAM_SHOW_PREVIEW, DEBUG_AUTHORIZATION, write_authorization, read_authorization, \
    background_job
from reports.api import ApiLogResource, UserGroupAuthorization, \
    VocabularyResource, UserResource, UserGroupResource, \
    UserResourceAuthorization, FieldResource
import reports.api
from reports.api_base import un_cache, MultiAuthentication, \
    IccblBasicAuthentication, IccblSessionAuthentication, TRAILING_SLASH
from reports.models import Vocabulary, ApiLog, UserProfile
from reports.schema import DATE_FORMAT, API_RESULT_DATA, API_RESULT_META, \
    API_RESULT_OBJ
from reports.serialize import parse_val, XLSX_MIMETYPE, SDF_MIMETYPE, \
    XLS_MIMETYPE, JSON_MIMETYPE, CSV_MIMETYPE, ZIP_MIMETYPE, \
    INPUT_FILE_DESERIALIZE_LINE_NUMBER_KEY, csvutils, LimsJSONEncoder
from reports.serialize.csvutils import convert_list_vals
from reports.serialize.streaming_serializers import ChunkIterWrapper, \
    json_generator, cursor_generator, sdf_generator, generic_xlsx_response, \
    csv_generator, get_xls_response, image_generator, closing_iterator_wrapper, \
    FileWrapper1
from reports.serialize.xlsutils import LIST_DELIMITER_XLS, write_xls_image
from reports.serializers import LimsSerializer, \
    XLSSerializer, ScreenResultSerializer
from reports.sqlalchemy_resource import SqlAlchemyResource
from reports.sqlalchemy_resource import _concat, _concat_with_sep
from reports.utils import default_converter
from reports.utils import sort_nicely, alphanum_key
from reports.utils.django_requests import convert_request_method_to_put
import reports.utils.si_unit as si_unit
import schema as SCHEMA


# Schema shortcuts
FIELD = SCHEMA.FIELD
WELL = SCHEMA.WELL
SMALL_MOLECULE_REAGENT = SCHEMA.SMALL_MOLECULE_REAGENT

SCREEN_TYPE = SCHEMA.VOCAB.screen.screen_type
USER_ROLE = SCHEMA.VOCAB.screen.user_role
WELL_TYPE = SCHEMA.VOCAB.well.library_well_type
ASSAY_WELL_CONTROL = SCHEMA.VOCAB.assaywell.control_type
SCREEN_AVAILABILITY = SCHEMA.VOCAB.screen.screen_result_availability
ACCESS_LEVEL = SCHEMA.VOCAB.screen.user_access_level_granted
DSL = SCHEMA.VOCAB.screen.data_sharing_level
LCP_STATUS = SCHEMA.VOCAB.lab_cherry_pick.status
SCREENING_STATUS = SCHEMA.VOCAB.library.screening_status
API_ACTION = SCHEMA.VOCAB.apilog.api_action

##### API CONSTANTS
# TODO: move to an API-accessible properties file

PLATE_NUMBER_SQL_FORMAT = 'FM9900000'
PSYCOPG_NULL = '\\N'
MAX_SPOOLFILE_SIZE = 1024*1024
MAX_WELL_FINDER_COUNT = 384*10000

API_MSG_COPYWELLS_DEALLOCATED = 'Copy wells deallocated'
API_MSG_COPYWELLS_ALLOCATED = 'Copy wells allocated'
API_MSG_SCREENING_PLATES_UPDATED = 'Library Plates updated'
API_MSG_SCREENING_EXTANT_PLATE_COUNT = 'Extant plates'
API_MSG_SCREENING_ADDED_PLATE_COUNT = 'Added plates'
API_MSG_SCREENING_DELETED_PLATE_COUNT = 'Deleted plates'
API_MSG_SCREENING_TOTAL_PLATE_COUNT = 'Library Plate Count'
API_MSG_SCP_CREATED = 'Created'
API_MSG_SCP_UNSELECTED = 'Unselected'
API_MSG_SCP_RESELECTED = 'Reselected'
API_MSG_SCPS_DELETED = 'Screener Cherry Picks Removed'      
API_MSG_SCPS_CREATED = 'Screener Cherry Picks Created'
API_MSG_LCP_CHANGED = 'Changed'
API_MSG_LCP_DESELECTED = 'Deselected'
API_MSG_LCP_SELECTED = 'Selected'
API_MSG_LCP_MULTIPLE_SELECTIONS_SUBMITTED = 'Multiple lab cherry pick selections submitted'
API_MSG_LCPS_CREATED = 'Lab Cherry Picks Created'
API_MSG_LCPS_MUST_BE_DELETED = "Lab Cherry Picks must be deleted"
API_MSG_LCPS_ASSIGNED = 'Assigned to Copies'
API_MSG_LCPS_UNFULFILLED = 'Unfulfilled'
API_MSG_LCPS_INSUFFICIENT_VOLUME = 'Insufficient volume'
API_MSG_LCPS_VOLUME_OVERRIDDEN = 'Insufficient volume overridden'
API_MSG_LCPS_REMOVED = 'Lab Cherry Picks Removed'
API_MSG_LCP_PLATES_ASSIGNED = 'Source plates'
API_MSG_LCP_SOURCE_PLATES_ALLOCATED = 'Source plate count'
API_MSG_LCP_ASSAY_PLATES_PLATED = 'Cherry pick assay plates (plated)'
API_MSG_LCP_ASSAY_PLATES_CREATED = 'Cherry pick assay plates (created)'
API_MSG_CPR_ASSAY_PLATES_REMOVED = 'Cherry pick assay plates (removed)'
API_MSG_PLATING_CANCELED = 'Plating canceled'
API_MSG_CPR_PLATES_PLATED = 'Plates plated'
API_MSG_CPR_PLATES_SCREENED = 'Plates screened'
API_MSG_CPR_PLATED_CANCEL_DISALLOWED = 'Plating reservation may not be canceled after plates are plated'

API_MSG_CPR_CONCENTRATIONS = 'concentration_warnings'

# API_PARAM_PLATE_MAPPING_OVERRIDE = 'plate_mapping_override'
API_PARAM_SHOW_OTHER_REAGENTS = 'show_other_reagents'
API_PARAM_SHOW_ALTERNATE_SELECTIONS = 'show_alternate_selections'
API_PARAM_SHOW_COPY_WELLS = 'show_copy_wells'
API_PARAM_SHOW_RETIRED_COPY_WELlS = 'show_available_and_retired_copy_wells'
API_PARAM_SHOW_UNFULFILLED = 'show_unfulfilled'
API_PARAM_SHOW_INSUFFICIENT = 'show_insufficient'
API_PARAM_SHOW_MANUAL = 'show_manual'
API_PARAM_VOLUME_OVERRIDE = 'volume_override'
API_PARAM_SET_DESELECTED_TO_ZERO = 'set_deselected_to_zero'
API_PARAM_DC_IDS = 'dc_ids'
API_PARAM_SHOW_RESTRICTED = 'show_restricted'
API_PARAM_SHOW_ARCHIVED = 'show_archived'


logger = logging.getLogger(__name__)

DEBUG_SCREEN_ACCESS = False or logger.isEnabledFor(logging.DEBUG)
DEBUG_DC_ACCESS = False or logger.isEnabledFor(logging.DEBUG)
DEBUG_RV_CREATE = False or logger.isEnabledFor(logging.DEBUG)
DEBUG_SCREENRESULT = logger.isEnabledFor(logging.DEBUG)
DEBUG_LIB_LOAD = False or logger.isEnabledFor(logging.DEBUG)
DEBUG_PLATE_EDIT = False or logger.isEnabledFor(logging.DEBUG)
DEBUG_WELL_PARSE = False or logger.isEnabledFor(logging.DEBUG)
    
def _get_raw_time_string():
  return timezone.now().strftime("%Y%m%d%H%M%S")

class DbApiResource(reports.api.ApiResource):

    def __init__(self, **kwargs):
        super(DbApiResource,self).__init__(**kwargs)
        self.resource_resource = None
        self.apilog_resource = None
        self.field_resource = None

        self.smallmoleculeresource = None
        self.silencingreagentresource = None
        self.naturalproductresource = None

        # Create index tables (shared by resources)
        with get_engine().connect() as conn:
            self._create_well_query_index_table(conn)
            self._create_well_data_column_positive_index_table(conn)
            self._create_screen_overlap_table(conn)

    def clear_cache(self, request, **kwargs):
        logger.debug('clearing the cache from resource: %s' 
            % self._meta.resource_name)
        super(DbApiResource, self).clear_cache(request, **kwargs)
        self.get_cache().clear()
    
    def get_cache(self):
        return caches['db_cache']
    
    def get_reagent_resource(self, library_classification):

        if self.smallmoleculeresource is None:
            self.smallmoleculeresource = SmallMoleculeReagentResource()
        if self.silencingreagentresource is None:
            self.silencingreagentresource = SilencingReagentResource()
        if self.naturalproductresource is None:
            self.naturalproductresource = NaturalProductReagentResource()
        
        if library_classification == 'small_molecule': 
            return self.smallmoleculeresource
        elif library_classification == 'rnai':
            return self.silencingreagentresource
        elif library_classification == 'natural_product':
            return self.naturalproductresource
        else:
            raise Exception('unknown library type: %r', library_classification)

    @classmethod
    def get_table_def(cls,table_name):
        '''
        Work around to create table definitions for dynamic tables
        '''
        if table_name in get_tables():
            return get_tables()[table_name]
        else:
            if table_name == 'well_data_column_positive_index':
                return sqlalchemy.sql.schema.Table(
                    'well_data_column_positive_index', 
                    aldjemy.core.get_meta(),
                    sqlalchemy.Column('well_id', sqlalchemy.String),
                    sqlalchemy.Column('data_column_id', sqlalchemy.Integer),
                    sqlalchemy.Column('screen_id', sqlalchemy.Integer),
                    )
            elif table_name == 'screen_overlap':
                return sqlalchemy.sql.schema.Table(
                    'screen_overlap', 
                    aldjemy.core.get_meta(),
                    sqlalchemy.Column('screen_id', sqlalchemy.Integer),
                    sqlalchemy.Column('overlap_screen_id', sqlalchemy.Integer))
                
            elif table_name == 'well_query_index':
                return sqlalchemy.sql.schema.Table(
                    'well_query_index', 
                    aldjemy.core.get_meta(),
                    sqlalchemy.Column('id', sqlalchemy.Integer),
                    sqlalchemy.Column('well_id', sqlalchemy.Integer),
                    sqlalchemy.Column('query_id', sqlalchemy.Integer))
                
            else:
                raise Exception('unknown table: %r', table_name)
        

    def _create_well_query_index_table(self, conn):
        try:
            conn.execute(text('select * from "well_query_index"  limit 1; '))
            logger.debug('The well_query_index table exists')
            return
        except Exception as e:
            logger.info('creating the well_query_index table...')
       
        try:
            conn.execute(text(
                'CREATE TABLE "well_query_index" ('
                '"id" serial NOT NULL,  '
                '"well_id" text NOT NULL,'
                '"query_id" integer NOT NULL'
                ');'
            ))
            conn.execute(text(
                'ALTER TABLE well_query_index '
                'ADD CONSTRAINT well_query_unique UNIQUE(query_id, well_id) '))
            # 20180222 - Deferrable unique constraints are a new feature in PostgreSQL 9.0
            #  'DEFERRABLE INITIALLY DEFERRED; '))
            logger.info('the well_query_index table has been created')
        except Exception, e:
            logger.info((
                'Exception: %r on trying to create the well_query_index table,'
                'note that this is normal if the table already exists '
                '(PostgreSQL <9.1 has no "CREATE TABLE IF NOT EXISTS"'), e)

    def _create_well_data_column_positive_index_table(self, conn):
        try:
            conn.execute(text(
                'select * from "well_data_column_positive_index" limit 1; '))
            logger.debug('The well_data_column_positive_index table exists')
            return
        except Exception as e:
            logger.info('creating the well_data_column_positive_index table')
        
        try:
            conn.execute(text(
                'CREATE TABLE well_data_column_positive_index ('
                ' "well_id" text NOT NULL, '
                ' "data_column_id" integer NOT NULL, ' 
                ' "screen_id" integer NOT NULL ' 
                ');'
            ))
            conn.execute(text(
                'CREATE INDEX wdc_screen_id '
                'on well_data_column_positive_index(screen_id);'))

            # Note: foreign keys are not needed and complicate delete
            # 'CREATE TABLE well_data_column_positive_index ('
            # ' "well_id" text NOT NULL REFERENCES "well" ("well_id") '
            # '    DEFERRABLE INITIALLY DEFERRED,'
            # ' "data_column_id" integer NOT NULL ' 
            # ' REFERENCES "data_column" ("data_column_id") '
            # '    DEFERRABLE INITIALLY DEFERRED, '
            # ' "screen_id" integer NOT NULL ' 
            # ' REFERENCES "screen" ("screen_id") '
            # '    DEFERRABLE INITIALLY DEFERRED'
            
            logger.info('the well_data_column_positive_index table created')
        except Exception, e:
            logger.info((
                'Exception: %r on trying to create the '
                'well_data_column_positive_index table,'
                'note that this is normal if the table already exists '
                '(PostgreSQL <9.1 has no "CREATE TABLE IF NOT EXISTS"'), e)
    
    def _create_screen_overlap_table(self, conn):
        try:
            conn.execute(text(
                'select * from "screen_overlap" limit 1; '))
            logger.debug('The screen_overlap table exists')
            return
        except Exception as e:
            logger.info('creating the screen_overlap table')
        
        try:
            conn.execute(text(
                'CREATE TABLE screen_overlap ('
                ' "screen_id" integer NOT NULL, '
                ' "overlap_screen_id" integer NOT NULL '
                ');'
            ))
            
            # Note: foreign keys are not needed and complicate delete
            # 'CREATE TABLE screen_overlap ('
            # ' "screen_id" integer NOT NULL '
            # 'REFERENCES "screen" ("screen_id") '
            # '    DEFERRABLE INITIALLY DEFERRED,'
            # ' "overlap_screen_id" integer NOT NULL '
            # 'REFERENCES "screen" ("screen_id") '
            # '    DEFERRABLE INITIALLY DEFERRED '

            conn.execute(text(
                'CREATE INDEX screen_overlap_screen_id '
                'on screen_overlap(screen_id);'))
            conn.execute(text(
                'CREATE INDEX screen_overlap_overlap_screen_id '
                'on screen_overlap(overlap_screen_id);'))
            logger.info('the screen_overlap table created')
        except Exception, e:
            logger.info((
                'Exception: %r on trying to create the '
                'screen_overlap table,'
                'note that this is normal if the table already exists '
                '(PostgreSQL <9.1 has no "CREATE TABLE IF NOT EXISTS"'), e)

    @classmethod
    @transaction.atomic
    def get_create_well_data_column_positive_index(cls):
        '''
        If the well_data_column_positive_index has been cleared, recreate.
        - recreate the entire index each time
        '''
        bridge = get_tables()
        _aw = bridge['assay_well']
        _sr = bridge['screen_result']
        _s = bridge['screen']
        _dc = bridge['data_column']
        _wdc = cls.get_table_def('well_data_column_positive_index')

        # Recreate the well_data_column_positive_index:
        # Note: because this is lazy recreated, we are creating the whole 
        # index each time; could just recreate for the specific screen
        count_stmt = select([func.count()]).select_from(_wdc)
        count = 0
        with get_engine().begin() as conn:
            count = int(conn.execute(count_stmt).scalar())        
        
            if count == 0:
                
                logger.info(
                    'well_data_column_positive_index count: %r, recreating', count)
            
                base_stmt = join(
                    _aw, _dc, _aw.c.screen_result_id == _dc.c.screen_result_id)
                base_stmt = base_stmt.join(
                    _sr, _sr.c.screen_result_id==_aw.c.screen_result_id)
                base_stmt = base_stmt.join(_s, _sr.c.screen_id==_s.c.screen_id)
                base_stmt = select([
                    _aw.c.well_id,
                    _dc.c.data_column_id,
                    _sr.c.screen_id
                    ]).select_from(base_stmt)            
                base_stmt = base_stmt.where(_aw.c.is_positive)
                base_stmt = base_stmt.where(_s.c.study_type==None)
                base_stmt = base_stmt.where(
                    _dc.c.data_type.in_([
                        'boolean_positive_indicator',
                        'partition_positive_indicator', 
                        'confirmed_positive_indicator']))
                base_stmt = base_stmt.order_by(
                    _dc.c.data_column_id, _aw.c.well_id)
                insert_statement = (
                    insert(_wdc)
                        .from_select(['well_id', 'data_column_id','screen_id'], 
                            base_stmt))
                logger.info('execute mutual pos insert statement...')
                logger.debug(
                    'mutual pos insert statement: %r',
                    str(insert_statement.compile(
                        dialect=postgresql.dialect(),
                        compile_kwargs={"literal_binds": True})))
                get_engine().execute(insert_statement)
                logger.info('mutual pos insert statement, executed.')
        return _wdc
    
    @classmethod
    @transaction.atomic
    def get_create_screen_overlap_indexes(cls):
        _wdc = cls.get_create_well_data_column_positive_index()
        _screen_overlap = cls.get_table_def('screen_overlap')
        wdc1 = _wdc.alias('wdc1')
        wdc2 = _wdc.alias('wdc2')
        
        count_stmt = select([func.count()]).select_from(_screen_overlap)
        count = 0
        with get_engine().begin() as conn:
            count = int(conn.execute(count_stmt).scalar())        
        
        if count == 0:
            logger.info('screen_overlap count: %r, recreating', count)
        
            base_stmt = (
                select([wdc1.c.screen_id, wdc2.c.screen_id.label('overlap_screen_id')])
                .select_from(wdc1)
                .select_from(wdc2)
                .where(wdc1.c.screen_id!=wdc2.c.screen_id)
                .where(wdc1.c.well_id==wdc2.c.well_id)
                .group_by(wdc1.c.screen_id,wdc2.c.screen_id))
            insert_statement = (
                insert(_screen_overlap)
                    .from_select(['screen_id','overlap_screen_id'], base_stmt))
            
            if logger.isEnabledFor(logging.DEBUG) or DEBUG_SCREENRESULT is True:
                logger.info(
                    'screen_overlap insert statement: %r',
                    str(insert_statement.compile(
                        dialect=postgresql.dialect(),
                        compile_kwargs={"literal_binds": True})))
                
            result = get_engine().execute(insert_statement)
            logger.info('screen_overlap insert statement, executed: %d.' % result.rowcount)
        return _screen_overlap
    
    def get_apilog_resource(self):
        if self.apilog_resource is None:
            self.apilog_resource = ApiLogResource()
        return self.apilog_resource
    
    def get_resource_resource(self):
        if self.resource_resource is None:
            self.resource_resource = ResourceResource()
        return self.resource_resource
    
    def get_field_resource(self):
        if self.field_resource is None:
            self.field_resource = FieldResource()
        return self.field_resource
    
    def get_request_user(self,request):
        
        return ScreensaverUser.objects.get(username=request.user.username)
    
    def build_schema(self, user=None, **kwargs):
        logger.debug('build schema for: %r', self._meta.resource_name)
        return self.get_resource_resource()._get_resource_schema(
            self._meta.resource_name, user=user, **kwargs)
    
class PlateLocationResource(DbApiResource):        
    
    class Meta:
        queryset = PlateLocation.objects.all()
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'platelocation'
        authorization = UserGroupAuthorization(resource_name)
        serializer = LimsSerializer()
        always_return_data = True 
        
    def __init__(self, **kwargs):
        super(PlateLocationResource, self).__init__(**kwargs)
        
        self.lcp_resource = None
        
    def get_librarycopyplate_resource(self):
        if not self.lcp_resource:
            self.lcp_resource = LibraryCopyPlateResource()
        return self.lcp_resource
    
    def prepend_urls(self):

        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)/(?P<room>[\w \-<>]+)"
                r"/(?P<freezer>[\w \-<>]+)"
                r"/(?P<shelf>[\w \-<>]+)"
                r"/(?P<bin>[\w <>]+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
        ]

    @read_authorization
    def get_detail(self, request, **kwargs):
        
        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True

        kwargs['room__eq'] = kwargs.pop('room', None)
        kwargs['freezer__eq'] = kwargs.pop('freezer', None)
        kwargs['shelf__eq'] = kwargs.pop('shelf', None)
        kwargs['bin__eq'] = kwargs.pop('bin', None)
        
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, schema=None, **kwargs):

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
            
        if schema is None:
            raise Exception('schema not initialized')
        is_for_detail = kwargs.pop('is_for_detail', False)
        
        try:
            
            # general setup
          
            manual_field_includes = set(param_hash.get('includes', []))
            manual_field_includes.add('plate_location_id')
            
            (filter_expression, filter_hash, readable_filter_hash) = \
                SqlAlchemyResource.build_sqlalchemy_filters(
                    schema, param_hash=param_hash)
            filename = self._get_filename(
                readable_filter_hash, schema, is_for_detail)
            filter_expression = \
                self._meta.authorization.filter(request.user,filter_expression)
                 
            order_params = param_hash.get('order_by', [])
            field_hash = self.get_visible_fields(
                schema['fields'], filter_hash.keys(), manual_field_includes,
                param_hash.get('visibilities'),
                exact_fields=set(param_hash.get('exact_fields', [])),
                order_params=order_params)
            order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
                order_params, field_hash)
             
            rowproxy_generator = None
            if use_vocab is True:
                rowproxy_generator = \
                    DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            rowproxy_generator = \
                self._meta.authorization.get_row_property_generator(
                    request.user, field_hash, rowproxy_generator)
 
            # specific setup 
 
            _p = self.bridge['plate']
            _pl = self.bridge['plate_location']
            _c = self.bridge['copy']
            _l = self.bridge['library']
            
            plate_location_counts = (
                select([_pl.c.plate_location_id, func.count().label('plate_count')])
                    .select_from(
                        _pl.join(_p,_pl.c.plate_location_id
                            ==_p.c.plate_location_id))
                    .group_by(_pl.c.plate_location_id )
                ).cte('plate_counts')
            
            custom_columns = {
                'plate_count': literal_column('plate_counts.plate_count'),
                'libraries': (
                    select([func.array_to_string(
                        func.array_agg(distinct(_l.c.short_name)), 
                        LIST_DELIMITER_SQL_ARRAY)])
                        .select_from(
                            _l.join(_c,_l.c.library_id==_c.c.library_id)
                                .join(_p,_p.c.copy_id==_c.c.copy_id))
                        .where(_p.c.plate_location_id
                            ==literal_column('plate_location.plate_location_id')))
            }

            base_query_tables = ['plate', 'copy', 'plate_location', 'library']

            columns = self.build_sqlalchemy_columns(
                field_hash.values(), base_query_tables=base_query_tables,
                custom_columns=custom_columns)
            # build the query statement
            
            j = _pl.join(
                    plate_location_counts, 
                    _pl.c.plate_location_id
                            ==plate_location_counts.c.plate_location_id,
                    isouter=True)
            stmt = select(columns.values()).select_from(j)

            if 'library_copy' in param_hash:
                param = param_hash.pop('library_copy')
                param = urllib.unquote(param).decode('utf-8')
                logger.info('param library_copy: %r', param)
                if '/' not in param or len(param.split('/')) !=2:
                    raise BadRequestError({
                        'library_copy': 'must be of the form '
                            '"{library_short_name}/{copy_name}"' })
                param = param.split('/')
                try:
                    library_copy = Copy.objects.get(
                        library__short_name=param[0],
                        name=param[1])
                    stmt = stmt.where(_pl.c.plate_location_id.in_(
                        select([_pl.c.plate_location_id])
                            .select_from(_pl.join(
                                _p,_pl.c.plate_location_id==_p.c.plate_location_id)
                                .join(_c,_p.c.copy_id==_c.c.copy_id))
                            .where(_c.c.copy_id==library_copy.copy_id)))
                except:
                    raise Http404('Copy not found: %r' % (param))
            # general setup
             
            (stmt, count_stmt) = self.wrap_statement(
                stmt, order_clauses, filter_expression)
 
            if not order_clauses:
                stmt = stmt.order_by("room","freezer","shelf","bin")

            # logger.info(
            #     'stmt: %s',
            #     str(stmt.compile(
            #         dialect=postgresql.dialect(),
            #         compile_kwargs={"literal_binds": True})))

            # create a generator to wrap the cursor and expand copy_plates ranges
            def create_copy_plate_ranges_gen(generator):
                bridge = self.bridge
                _library = self.bridge['library']
                _lcp = self.bridge['plate']
                _cp = self.bridge['copy']
                lcp_query = (
                    select([ 
                        _library.c.short_name,
                        _cp.c.name,
                        _lcp.c.plate_number
                     ])
                    .select_from(
                        _lcp.join(_cp, _cp.c.copy_id == _lcp.c.copy_id)
                            .join(
                                _library,
                                _library.c.library_id == _cp.c.library_id))
                    .where(_lcp.c.plate_location_id == text(':plate_location_id'))
                    .group_by(
                        _library.c.short_name, _cp.c.name, _lcp.c.plate_number)
                    .order_by(
                        _library.c.short_name, _cp.c.name, _lcp.c.plate_number))
                logger.debug('lcp_query: %r', str(lcp_query.compile()))
                

                def copy_plate_ranges_generator(cursor):
                    if generator:
                        cursor = generator(cursor)
                    class Row:
                        def __init__(self, row):
                            self.row = row
                            self.entries = []
                            plate_location_id = row['plate_location_id']
                            query = conn.execute(
                                lcp_query, plate_location_id=plate_location_id)
                            copy = None
                            start_plate = None
                            end_plate = None
                            def create_entry(library, copy, start_plate, end_plate):
                                if start_plate != end_plate:
                                    return '%s:%s:%s-%s' % (
                                        library, copy, start_plate, end_plate)
                                else:
                                    return '%s:%s:%s' % (
                                        library, copy, start_plate)
                                
                            for x in query:
                                if not copy:
                                    copy = x[1]
                                    library = x[0]
                                if not start_plate:
                                    start_plate = end_plate = x[2]
                                if (x[0] != library 
                                    or x[1] != copy 
                                    or x[2] > end_plate + 1):
                                    # start a new range, save old range
                                    self.entries.append(create_entry(
                                        library, copy, start_plate, end_plate))
                                    start_plate = end_plate = x[2]
                                    copy = x[1]
                                    library = x[0]
                                else:
                                    end_plate = x[2]
                            if copy: 
                                self.entries.append(create_entry(
                                    library, copy, start_plate, end_plate))
                                    
                        def has_key(self, key):
                            if key == 'copy_plate_ranges': 
                                return True
                            return self.row.has_key(key)
                        def keys(self):
                            return self.row.keys()
                        def __getitem__(self, key):
                            if key == 'copy_plate_ranges':
                                return self.entries
                            else:
                                return self.row[key]
                    conn = get_engine().connect()
                    # FIXME: use with get_engine().connect() as conn:
                    try:
                        for row in cursor:
                            yield Row(row)
                    finally:
                        conn.close()
                        
                return copy_plate_ranges_generator
            
            if 'copy_plate_ranges' in field_hash:
                
                rowproxy_generator = create_copy_plate_ranges_gen(rowproxy_generator)

            title_function = None
            if use_titles is True:
                def title_function(key):
                    return field_hash[key]['title']
            if is_data_interchange:
                title_function = DbApiResource.datainterchange_title_function(
                    field_hash,schema['id_attribute'])

            return self.stream_response_from_statement(
                request, stmt, count_stmt, filename,
                field_hash=field_hash, param_hash=param_hash,
                is_for_detail=is_for_detail,
                rowproxy_generator=rowproxy_generator,
                title_function=title_function, meta=kwargs.get('meta', None))

        except Exception, e:
            logger.exception('on get list')
            raise e   

    def put_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_detail')

    @write_authorization
    @un_cache  
    @transaction.atomic      
    def patch_detail(self, request, **kwargs):
        '''
        Override to generate informational summary for callee
        '''
        
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        deserialized = kwargs.pop('data', None)
        # allow for internal data to be passed
        deserialize_meta = None
        if deserialized is None:
            deserialized, deserialize_meta = self.deserialize(
                request, format=kwargs.get('format', None))

        logger.debug('patch detail %s, %s', deserialized,kwargs)

        # cache state, for logging
        # Look for id's kwargs, to limit the potential candidates for logging
#         id_attribute = schema['id_attribute']
        kwargs_for_log = self.get_id(
            deserialized, schema=schema, validate=True,**kwargs)

        original_data = None
        if kwargs_for_log:
            try:
                original_data = \
                    self._get_detail_response_internal(**kwargs_for_log)
            except Exception, e: 
                logger.exception('exception when querying for existing obj: %s', 
                    kwargs_for_log)
        try:
            log = kwargs.get('parent_log', None)
            if not log:
                log = self.make_log(request)
                log.save()
                kwargs['parent_log'] = log
            patch_response = self.patch_obj(request, deserialized, **kwargs)
            patched_plate_logs = patch_response[API_RESULT_OBJ]
        except ValidationError as e:
            logger.exception('Validation error: %r', e)
            raise e

        # get new state, for logging
        new_data = self._get_detail_response_internal(**kwargs_for_log)
        update_log = self.log_patch(request, original_data,new_data, schema=schema, **kwargs)
        if update_log:
            update_log.save()
        # patch_count = len(patched_plate_logs)
        update_count = len([x for x in patched_plate_logs if x.diffs ])
        #unchanged_count = patch_count - update_count
        action = update_log.api_action if update_log else 'Unchanged'
        if action == API_ACTION.CREATE:
            action += ': ' + update_log.key
        meta = { 
            SCHEMA.API_MSG_RESULT: { 
                'Plate location': action,
                # Note remove submitted/unchanged count, inaccurate because
                # all plates for the location are "submitted"
                # SCHEMA.API_MSG_SUBMIT_COUNT : patch_count, 
                SCHEMA.API_MSG_UPDATED: update_count, 
                # SCHEMA.API_MSG_UNCHANGED: unchanged_count, 
                SCHEMA.API_MSG_COMMENTS: log.comment
            }
        }
        if deserialize_meta:
            meta.update(deserialize_meta)
        return self.build_response(
            request,  {API_RESULT_META: meta }, response_class=HttpResponse, 
            **kwargs)
            
    @write_authorization
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):

        logger.info('patch platelocation')
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        id_kwargs = \
            self.get_id(deserialized, schema=schema, validate=True, **kwargs)

        create = False
        try:
            plate_location = PlateLocation.objects.get(**id_kwargs)
        except ObjectDoesNotExist:
            logger.info('plate location does not exist, creating: %r',
                id_kwargs)
            create=True
            plate_location = PlateLocation.objects.create(**id_kwargs)

        initializer_dict = self.parse(deserialized, schema=schema, create=create)
        errors = self.validate(initializer_dict, schema=schema,patch=not create)
        if errors:
            raise ValidationError(errors)
        
        copy_plate_ranges = deserialized.get(
            'copy_plate_ranges', [])
        all_plates = self._find_plates(
            schema['fields']['copy_plate_ranges']['regex'], copy_plate_ranges)
        
        # get the old plate locations, for logging
        plate_log_hash = {}
        lookup = ['room','freezer','shelf','bin']
        # combine all_plates (new) with plate_set.all() (previous)
        for plate in set(all_plates) | set(plate_location.plate_set.all()):
            plate_dict = {
                'library_short_name': plate.copy.library.short_name,
                'copy_name': plate.copy.name,
                'plate_number': str(plate.plate_number)
            }
            plate_log_hash[plate.plate_id] = [plate_dict, plate_dict.copy()]
            
            if plate.plate_location:
                plate_dict.update({
                    k:getattr(plate.plate_location,k, None)
                        for k in lookup })
        
        plate_location.plate_set = all_plates
        plate_location.save()
        
        for plate in set(all_plates) | set(plate_location.plate_set.all()):
            plate_dict = {
                'library_short_name': plate.copy.library.short_name,
                'copy_name': plate.copy.name,
                'plate_number': str(plate.plate_number)
            }
            if plate.plate_location:
                plate_dict.update({
                    k:getattr(plate.plate_location,k, None)
                        for k in lookup })
            plate_log_hash[plate.plate_id] = \
                [plate_log_hash[plate.plate_id][0],plate_dict]

        logger.info(
            'log copyplate patches for platelocation, '
            'len: %d...', len(plate_log_hash.items()))
        
        plate_logs = []
        lcp_schema = \
            self.get_librarycopyplate_resource()\
                .build_schema(user=request.user)
        for prev_dict,new_dict in plate_log_hash.values():
            log = self.get_librarycopyplate_resource().log_patch( 
                request,prev_dict,new_dict,
                **{ 'parent_log': kwargs.get('parent_log', None),
                    'full_create_log': False,
                    'log_empty_diffs': False,
                    'schema': lcp_schema } )
            if log: 
                plate_logs.append(log)
        ApiLog.bulk_create(plate_logs)
        logger.info('plate logs saved %r', len(plate_logs))
        
        return { API_RESULT_OBJ: plate_logs }
            
    # FIXME: replace with LibraryCopyPlateResource.find_plates (21070505)
    def _find_plates(self, regex_string, copy_plate_ranges):
        logger.info('find copy_plate_ranges: %r', copy_plate_ranges)
        
        if not regex_string:
            return []
        
        # parse library_plate_ranges
        # E.G. Regex: /(([^:]+):)?(\w+):(\d+)-(\d+)/
        matcher = re.compile(regex_string)
        
        # Expand the copy_plate_ranges
        new_copy_plate_ranges = []
        for copy_plate_range in copy_plate_ranges:
            match = matcher.match(copy_plate_range)
            if not match:
                raise ValidationError(
                    key='copy_plate_ranges',
                    msg=('%r does not match pattern: %s' 
                        % (copy_plate_range, regex_string)))
                break
            else:
                start_plate = int(match.group(4))
                if match.group(6):
                    end_plate = int(match.group(6))
                else:
                    end_plate = start_plate
                new_copy_plate_ranges.append({
                    'library_short_name': match.group(2),
                    'copy_name': match.group(3),
                    'start_plate': start_plate,
                    'end_plate': end_plate,
                     })
        copy_plate_ranges = new_copy_plate_ranges
        
        # validate/find the plate ranges
        logger.info(
            'get the referenced plates for: %r', copy_plate_ranges)
        all_plates = set()
        plates_changed_location = {}
        for _data in copy_plate_ranges:
            try:
                copy_name = _data['copy_name']
                library_short_name = _data['library_short_name']
                copy = Copy.objects.get(
                    name=copy_name,
                    library__short_name=library_short_name)
            except ObjectDoesNotExist:
                raise ValidationError(
                    key='copy_plate_ranges',
                    msg='{copy} does not exist: {val}'.format(
                        copy=copy_name,
                        val=str(_data)))
            try:
                start_plate = _data['start_plate']
                end_plate = _data['end_plate']
                plate_range = ( 
                    Plate.objects.all().filter(
                        copy=copy,
                        plate_number__range=[start_plate, end_plate])
                        .order_by('plate_number')
                    )
                if not plate_range:
                    raise ValidationError(
                        key='copy_plate_ranges',
                        msg=('plate range not found: {copy_name}{start_plate}-{end_plate}'
                            ).format(**_data))
                min = max = plate_range[0].plate_number
                for plate in plate_range:
                    if plate.plate_number < min:
                        min = plate.plate_number
                    if plate.plate_number > max:
                        max = plate.plate_number
                    all_plates.add(plate)
                if min != start_plate:
                    raise ValidationError(
                        key='copy_plate_ranges',
                        msg=('plate range start not found: {copy_name}:{start_plate}-{end_plate}'
                            ).format(**_data))
                if max != end_plate:
                    raise ValidationError(
                        key='copy_plate_ranges',
                        msg=('plate range end not found: {copy_name}:{start_plate}-{end_plate}'
                            ).format(**_data))
            except ObjectDoesNotExist:
                raise ValidationError(
                    key='copy_plate_ranges',
                    msg=('plate range not found: {copy_name}:{start_plate}-{end_plate}'
                        ).format(**_data))
        
        logger.debug('all plate keys: %r', [
            x for x in sorted(all_plates,key=lambda x: x.plate_number)])
        return all_plates

class LibraryCopyPlateResource(DbApiResource):

    class Meta:
        queryset = Plate.objects.all() 
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'librarycopyplate'
        authorization = UserGroupAuthorization(resource_name)
        
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        always_return_data = True 

    def __init__(self, **kwargs):
        
        super(LibraryCopyPlateResource, self).__init__(**kwargs)
        self.plate_location_resource = None
        self.screen_resource = None
        self.library_screening_resource = None
        self.cpr_resource = None
        
    def get_cpr_resource(self):
        if self.cpr_resource is None:
            self.cpr_resource = CherryPickRequestResource()
        return self.cpr_resource
        
    def get_screen_resource(self):
        if not self.screen_resource:
            self.screen_resource = ScreenResource()
        return self.screen_resource
    
    def get_library_screening_resource(self):
        if not self.library_screening_resource:
            self.library_screening_resource = LibraryScreeningResource()
        return self.library_screening_resource
        
    def get_platelocation_resource(self):
        
        if not self.plate_location_resource:
            self.plate_location_resource = PlateLocationResource()
        return self.plate_location_resource
        
        
    def prepend_urls(self):

        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)/batch_edit%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('batch_edit'), name="api_lcp_batch_edit"),
            url(r"^(?P<resource_name>%s)/%s/(?P<%s>[\d]+)%s$" 
                % (self._meta.resource_name, SCHEMA.URI_PATH_COMPLEX_SEARCH, 
                    SCHEMA.API_PARAM_COMPLEX_SEARCH_ID,TRAILING_SLASH),
                self.wrap_view('search'), name="api_search"),
            url(r"^(?P<resource_name>%s)/(?P<library_short_name>[\w.\-\+: ]+)"
                r"/(?P<copy_name>[\w.\-\+: ]+)"
                r"/(?P<plate_number>[\d]+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)"
                r"/(?P<copy_name>[\w.\-\+: ]+)"
                r"/(?P<plate_number>[\d]+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
        ]

    @staticmethod
    def parse_plate_copy_search(plate_search_data):
        '''
        Using raw plate copy search data, separated by lines,
        create a plate search data structure:
            {   
                plates, plate_ranges, copies, 
                plate_numbers_expected, 
                plate_numbers_in_order_listed,
                plate_copy_keys_expected
            }
        '''
        DEBUG_PLATE_COPY_SEARCH = False or logger.isEnabledFor(logging.DEBUG)
        
        if DEBUG_PLATE_COPY_SEARCH:
            logger.info('raw plate_search_data: %r', plate_search_data)
        # Use unquote to decode form data from a post
        if isinstance(plate_search_data, basestring):
            plate_search_data = urllib.unquote(plate_search_data)
        elif isinstance(plate_search_data, (list,tuple)):
            plate_search_data = [urllib.unquote(x) for x in plate_search_data]
        else:
            logger.warn('plate_search_data must be either a string or list of strings')
            return plate_search_data
        
        if DEBUG_PLATE_COPY_SEARCH:
            logger.info('plate_search_data: %r', plate_search_data)
        parsed_searches = []
        
        # Process the patterns by line
        parsed_lines = plate_search_data
        if isinstance(parsed_lines, basestring):
            parsed_lines = re.split(
                lims_utils.PLATE_SEARCH_LINE_SPLITTING_PATTERN,parsed_lines)
            if DEBUG_PLATE_COPY_SEARCH:
                logger.info('parsed_lines: %r', parsed_lines)
        
        for _line in parsed_lines:
            _line = _line.strip()
            if not _line:
                continue

            parts = lims_utils.QUOTED_WORD_SPLITTING_PATTERN.findall(_line)
            if DEBUG_PLATE_COPY_SEARCH:
                logger.info('parse plate copy search: line parts: %r', parts)
            
            parsed_search = defaultdict(list)
            
            plate_numbers_in_order_listed = list()
            for part in parts:
                # unquote
                part = re.sub(r'["\']+','',part)

                if PLATE_PATTERN.match(part):
                    plate_number = int(part)
                    parsed_search['plate'].append(plate_number)
                    plate_numbers_in_order_listed.append(plate_number)
                elif PLATE_RANGE_PATTERN.match(part):
                    match = PLATE_RANGE_PATTERN.match(part)
                    plate_range = [
                        int(match.group(1)), int(match.group(2))]
                    parsed_search['plate_range'].append(sorted(plate_range))
                    if plate_range[1] < plate_range[0]:
                        if DEBUG_PLATE_COPY_SEARCH:
                            logger.info('extend reversed range: %r',plate_range)
                        plate_numbers_in_order_listed.extend(
                            range(plate_range[0],plate_range[1]-1,-1))
                    else:
                        plate_numbers_in_order_listed.extend(
                            range(plate_range[0],plate_range[1]+1))
                else:
                    # Must be a copy
                    if not COPY_NAME_PATTERN.match(part):
                        raise ValidationError(
                            key='plate_copy_search',
                            msg='unrecognized pattern: %r' % part)
                    if DEBUG_PLATE_COPY_SEARCH:
                        logger.info('recognized copy: %r', part)
                    parsed_search['copy'].append(part)

            parsed_search['plate_numbers_in_order_listed'] = \
                plate_numbers_in_order_listed
            
            if parsed_search['copy']:
                plate_copy_keys_expected = set()
                for plate in parsed_search['plate']:
                    for copy in parsed_search['copy']:
                        plate_copy_keys_expected.add('%s/%s' % (copy,plate))
                for plate_range in parsed_search['plate_range']:
                    for plate in range(plate_range[0],plate_range[1]+1):
                        for copy in parsed_search['copy']:
                            plate_copy_keys_expected.add('%s/%s' % (copy,plate))
                parsed_search['plate_copy_keys_expected'] = \
                    list(plate_copy_keys_expected)
            else:
                plate_numbers_expected = set(parsed_search['plate'])
                plate_numbers_expected.update(*[
                    range(plate_range[0],plate_range[1]+1) 
                    for plate_range in parsed_search['plate_range']])
                parsed_search['plate_numbers_expected'] = \
                    sorted(plate_numbers_expected)
            if DEBUG_PLATE_COPY_SEARCH:
                logger.info('parsed: %r', parsed_search)
            parsed_searches.append(parsed_search)
        
        if not parsed_searches:
            raise ValidationError(
                key='plate_copy_search',
                msg='no input recognized')
        
        logger.debug('parsed searches: %r', parsed_searches)
        return parsed_searches
        
    @classmethod
    def find_plates(cls, parsed_searches):
        ''' 
        @param plate_search_data parsed_plate_search data structure 
                @see parse_plate_copy_search
        @return 
            set() of LibraryCopyPlate objects matching the
                plate_search_data (raw user search text)
            array of plates and/or plate_copy_keys expected but not found
        '''
        DEBUG_FIND_PLATES = False or logger.isEnabledFor(logging.DEBUG)
        errors = set()
        plates = set()
        
        if DEBUG_FIND_PLATES:
            logger.info('find_plates: %r', parsed_searches);

        if not parsed_searches:
            return ([], errors)

        if isinstance(parsed_searches, dict):
            parsed_searches = [parsed_searches,]
        
        for parsed_search in parsed_searches:
            if DEBUG_FIND_PLATES:
                logger.info('find plates for %r', parsed_search)
            plate_query = Plate.objects.all()
            
            plate_qs = []
            if parsed_search['plate']:
                plate_qs.append(Q(plate_number__in=parsed_search['plate']))
            if parsed_search['plate_range']:
                for plate_range in parsed_search['plate_range']:
                    plate_qs.append(Q(plate_number__range=plate_range))
            if plate_qs:
                plate_query = plate_query.filter(
                    reduce(lambda x,y: x|y,plate_qs))
            
            if parsed_search['copy']:
                plate_query = plate_query.filter(
                    copy__name__in=parsed_search['copy'])

            # check for not found:
            if parsed_search['copy']:
                plate_copy_keys_found = set([
                    '%s/%s' % (plate.copy.name, plate.plate_number)
                    for plate in plate_query.all()])
                not_found = (set(parsed_search['plate_copy_keys_expected'])
                    - plate_copy_keys_found)
                if DEBUG_FIND_PLATES:
                    logger.info('plate-copies found: %r, expected: %r', 
                        sorted(plate_copy_keys_found), 
                        sorted(parsed_search['plate_copy_keys_expected']) )
                if not_found:    
                    errors.add('plate/copy not found:' + ', '.join(not_found))
            else:
                plate_numbers_found = set([plate.plate_number 
                    for plate in plate_query.all() ])
                
                not_found = (set(parsed_search['plate_numbers_expected'])
                    - plate_numbers_found)
                if DEBUG_FIND_PLATES:
                    logger.info('plates found: %r, expected: %r', 
                        sorted(plate_numbers_found), 
                        sorted(parsed_search['plate_numbers_expected']) )
                if not_found:
                    errors.add('plates not found: ' + ', '.join(map(str, not_found))) #(plate_number))
            plates.update(plate_query.all())
        
        if not plates:
            errors.add('No plates found')
        if DEBUG_FIND_PLATES:
            logger.info('plates found: %d, errors: %r', len(plates), errors)
        return (plates, [x for x in sorted(errors)])

    @read_authorization
    def get_detail(self, request, **kwargs):
        
        library_short_name = kwargs.get('library_short_name', None)
        if not library_short_name:
            logger.info('no library_short_name provided')
        copy_name = kwargs.get('copy_name', None)
        if not copy_name:
            raise Http404('must provide a copy_name parameter')
        plate_number = kwargs.get('plate_number', None)
        if not copy_name:
            raise Http404('must provide a plate_number parameter')
        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    @classmethod
    def get_librarycopyplate_cte(cls, 
        filter_hash=None, library_id=None, copy_id=None, plate_ids=None):
        
        bridge = get_tables()
        _l = bridge['library']
        _c = bridge['copy']
        _p = bridge['plate']
        
        j = _p
        j = j.join(_c, _p.c.copy_id == _c.c.copy_id)
        j = j.join(_l, _c.c.library_id == _l.c.library_id)
        plate_table = (
            select([
                _p.c.plate_id,
                _p.c.plate_number,
                _p.c.copy_id,
                _c.c.library_id,
                _c.c.name.label('copy_name'),
                _concat(
                    _l.c.short_name, '/', _c.c.name, '/', 
                    cast(_p.c.plate_number, sqlalchemy.sql.sqltypes.Text)
                ).label('key'),
                ])
            .select_from(j))
        if library_id is not None:
            plate_table = plate_table.where(_c.c.library_id==library_id)
        if copy_id is not None:
            plate_table = plate_table.where(_c.c.copy_id==copy_id)
        if plate_ids is not None:
            plate_table = plate_table.where(_p.c.plate_id.in_(plate_ids))
        if filter_hash:
            extra_filters = {k:v for k,v in filter_hash.items()
                if k in ['plate_number', 'copy_name']}
            if extra_filters:
                logger.debug('extra filters: %r', extra_filters)
                plate_table = (
                    select([literal_column(x) for x in plate_table.columns.keys()])
                    .select_from(Alias(plate_table))
                )
                # always OR at the subquery level
                plate_table = plate_table.where(or_(*(extra_filters.values())))

        return plate_table
        
    @classmethod
    def get_plate_copywell_statistics_cte(cls, 
        filter_hash=None, library_id=None, copy_id=None, plate_ids=None):  
        bridge = get_tables()
        _w = bridge['well']
        _l = bridge['library']
        _p = bridge['plate']
        _cw = bridge['copy_well']
        _c = bridge['copy']
        
        j = _cw
        if copy_id is not None:
            j = j.join(_c, _cw.c.copy_id==_c.c.copy_id)
        cw_vols = (
            select([
                _cw.c.plate_id,
                func.count().label('count'),
                func.sum(_cw.c.volume).label('cum_well_vol'),
                func.min(_cw.c.volume).label('min_well_vol'),
                func.max(_cw.c.volume).label('max_well_vol'),
                func.min(_cw.c.mg_ml_concentration).label('min_well_mg_ml'),
                func.max(_cw.c.mg_ml_concentration).label('max_well_mg_ml'),
                func.min(_cw.c.molar_concentration).label('min_well_molar'),
                func.max(_cw.c.molar_concentration).label('max_well_molar'),
                ])
            .select_from(j)
            .group_by(_cw.c.plate_id))
        if library_id is not None:
            cw_vols = cw_vols.where(_c.c.library_id==library_id)
        if copy_id is not None:
            cw_vols = cw_vols.where(_c.c.copy_id==copy_id)
        # FIXME: adding plate_ids is not performant here
        # if plate_ids is not None:
        #     cw_vols = cw_vols.where(_p.c.plate_id.in_(plate_ids))
        cw_vols = cw_vols.cte('copy_well_volumes')
        
        well_c = (
            select([
                _w.c.plate_number,
                func.min(_w.c.molar_concentration).label('min_well_molar'),
                func.max(_w.c.molar_concentration).label('max_well_molar'),
                func.min(_w.c.mg_ml_concentration).label('min_well_mg_ml'),
                func.max(_w.c.mg_ml_concentration).label('max_well_mg_ml'),
                ])
            .select_from(_w)
            .group_by(_w.c.plate_number))
        if library_id is not None:
            well_c = well_c.where(_w.c.library_id==library_id)
        if copy_id is not None:
            well_c = well_c.where(_w.c.library_id==
                ( select([distinct(_c.c.library_id)])
                    .where(_c.c.copy_id==copy_id).as_scalar()))
        well_c = well_c.cte('well_concentrations')

        # NOTE: this method of calculating plate volumes requires that plates
        # are never screened as "library_screenings" after being screened as
        # cherry pick screenings: (copy well statistics are set on the first
        # cherry pick screening, and are not reset on subsequent library_screenings)
        j2 = _p.outerjoin(cw_vols,_p.c.plate_id==cw_vols.c.plate_id)
        j2 = j2.join(well_c, _p.c.plate_number==well_c.c.plate_number)
        j2 = j2.join(_c, _p.c.copy_id==_c.c.copy_id)
        query = (
            select([
                _p.c.plate_id,
                _p.c.plate_number,
                _p.c.copy_id,
                _c.c.name.label('copy_name'),
                case([
                    (_p.c.experimental_well_count > 0,
                    ( ( func.coalesce(_p.c.remaining_well_volume,_p.c.well_volume) 
                        * (_p.c.experimental_well_count-cw_vols.c.count) )
                        + cw_vols.c.cum_well_vol ) / _p.c.experimental_well_count)],
                    else_=None).label('avg_well_remaining_volume'),
                func.coalesce(_p.c.remaining_well_volume,_p.c.well_volume)
                    .label('remaining_well_volume'),
                func.coalesce(cw_vols.c.min_well_vol,
                    _p.c.remaining_well_volume,).label('min_well_remaining_volume'),
                case([
                    (_p.c.experimental_well_count == cw_vols.c.count,
                    func.coalesce(cw_vols.c.min_well_vol,
                        _p.c.remaining_well_volume))],
                    else_=func.coalesce(_p.c.remaining_well_volume,_p.c.well_volume)
                    ).label('max_well_remaining_volume'),
                func.coalesce(cw_vols.c.min_well_mg_ml,
                    _p.c.mg_ml_concentration,
                    well_c.c.min_well_mg_ml
                    ).label('min_mg_ml_concentration'),
                func.coalesce(cw_vols.c.max_well_mg_ml,
                    _p.c.mg_ml_concentration,
                    well_c.c.max_well_mg_ml
                    ).label('max_mg_ml_concentration'),
                func.coalesce(cw_vols.c.min_well_molar,
                    _p.c.molar_concentration,
                    well_c.c.min_well_molar
                    ).label('min_molar_concentration'),
                func.coalesce(cw_vols.c.max_well_molar,
                    _p.c.molar_concentration,
                    well_c.c.max_well_molar
                    ).label('max_molar_concentration'),
                ])
            .select_from(j2)
            )
        if library_id is not None:
            query = query.where(_c.c.library_id==library_id)
        if copy_id is not None:
            query = query.where(_p.c.copy_id==copy_id)
        if plate_ids is not None:
            query = query.where(_p.c.plate_id.in_(plate_ids))

        if filter_hash:
            # Create a subquery and filter on the expressions that match the fields
            extra_filters = [v for k,v in filter_hash.items()
                if k in ['plate_number', 'copy_name']]
            if extra_filters:
                query = (
                    select([literal_column(x) for x in query.columns.keys()])
                    .select_from(Alias(query))
                )
                # Always OR at the subquery level
                query = query.where(or_(*extra_filters))
        
        return query
    
    @classmethod
    def get_plate_screening_statistics_cte(cls, 
        filter_hash=None, library_id=None, copy_id=None, plate_ids=None):
        bridge = get_tables()
        _p = bridge['plate']
        _a = bridge['activity']
        _ls = bridge['library_screening']
        _ap = bridge['assay_plate']
        _c = bridge['copy']

        j = ( _ap.join(_ls,_ls.c.activity_id==_ap.c.library_screening_id)
                .join(_a, _a.c.activity_id==_ls.c.activity_id)
                .join(_p, _ap.c.plate_id==_p.c.plate_id)
                .join(_c, _p.c.copy_id==_c.c.copy_id))
        
        query = (
            select([
                _p.c.copy_id,
                _c.c.name.label('copy_name'),
                _ap.c.plate_id,
                _ap.c.plate_number,
                func.min(_a.c.date_of_activity).label('first_date_screened'),
                func.max(_a.c.date_of_activity).label('last_date_screened')
                ])
            .select_from(j)
            .group_by(_ap.c.plate_id, _ap.c.plate_number, _p.c.copy_id,
                _c.c.name ) )
        if library_id is not None:
            query = query.where(_c.c.library_id == library_id)
        if copy_id is not None:
            query = query.where(_p.c.copy_id==copy_id)
        if plate_ids is not None:
            query = query.where(_p.c.plate_id.in_(plate_ids))
        if filter_hash:
            extra_filters = [v for k,v in filter_hash.items()
                if k in ['plate_number', 'copy_name']]
            if extra_filters:
                query = (
                    select([literal_column(x) for x in query.columns.keys()])
                    .select_from(Alias(query))
                )
                # Always OR at the subquery level
                query = query.where(or_(*extra_filters))
        ## TODO: this does not include cherry pick screenings, redo when 
        # cherry pick screenings are reworked:
        # rework: so that source plates are linked to screening activity
        
        return query
        
    def build_screened_plate_response(
        self, request, screen_facility_id=None, library_screening_id=None,
         **kwargs):    
        
        if screen_facility_id is None and library_screening_id is None:
            msg = 'must provide a "screen_facility_id" or "library_screening_id"'
            raise BadRequestError({
                'screen_facility_id': msg, 'library_screening_id': msg})
        if screen_facility_id is not None and library_screening_id is not None:
            msg = 'must provide a "screen_facility_id" or "library_screening_id"'
            raise BadRequestError({
                'screen_facility_id': msg, 'library_screening_id': msg})
        if screen_facility_id is not None:
            if self.get_screen_resource()._meta.authorization\
                .has_screen_read_authorization(
                    request.user, screen_facility_id) is False:
                raise PermissionDenied
        if library_screening_id is not None:
            if self.get_library_screening_resource()._meta.authorization\
                .has_activity_read_authorization(
                    request.user, library_screening_id) is False:
                raise PermissionDenied
            
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False

        manual_field_includes = set(param_hash.get('includes', []))
        logger.info('manual_field_includes: %r', manual_field_includes)
        
        if screen_facility_id is not None:
            filename = 'plates_for_screen_%s' % screen_facility_id
        else:
            filename = 'plates_for_screening_%s' % library_screening_id
            
        # construct a limited plate view here 
        # start from the librarycopyplate schema, but limit to plate only fields
        schema = self.build_schema(user=request.user)
        
        fields_to_show = [
            'library_short_name', 'library_screening_status', 
            'library_comment_array','plate_number','comment_array', 
            'screening_count','assay_plate_count','copies_screened', 
            'last_date_screened','first_date_screened']
        
        new_fields = {}
        for k,v in schema['fields'].items():
            if k in fields_to_show:
                new_fields[k] = v
                v['visibility'] = ['l']
        # Add a "copies_screened" field
        new_fields['copies_screened'] = schema['fields']['copy_name']
        new_fields['copies_screened']['key'] = 'copies_screened'
        new_fields['copies_screened']['title'] = 'Copies Screened'
        new_fields['copies_screened']['data_type'] = 'list'
        
        # Add a "copy_comments" field
        if 'copy_comments' in schema['fields']:
            # copy_comments may be restricted by auth
            new_fields['copy_comments'] = schema['fields']['copy_comments']
            new_fields['copy_comments']['data_type'] = 'list'
            new_fields['copy_comments']['visibility'] = ['l']

        schema['fields'] = new_fields
        
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filter_expression = \
            self._meta.authorization.filter(request.user,filter_expression)
        visibilities = ['l','d']
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            visibilities,
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
         
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            # use "use_vocab" as a proxy to also adjust siunits for viewing
            rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
                field_hash, rowproxy_generator)
        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)

        # specific setup 
        _p = self.bridge['plate']
        _c = self.bridge['copy']
        _l = self.bridge['library']
        _ls = self.bridge['library_screening']
        _a = self.bridge['activity']
        _ap = self.bridge['assay_plate']
        _screen = self.bridge['screen']
        _apilog = self.bridge['reports_apilog']
        _assay_plates_query = (select([
            _ls.c.activity_id,
            _a.c.date_of_activity,
            _ap.c.plate_id,
            _c.c.copy_id,
            _c.c.name.label('copy_name'),
            _l.c.short_name,
            _c.c.library_id,
            _ap.c.plate_number,
            _ap.c.assay_plate_id,
            _concat(
                _l.c.short_name, '/', _c.c.name, '/', 
                cast(_p.c.plate_number, sqlalchemy.sql.sqltypes.Text)
            ).label('plate_key'),
            ])
            .select_from(
                _ap.join(_ls,_ap.c.library_screening_id==_ls.c.activity_id)
                   .join(_a,_ls.c.activity_id==_a.c.activity_id)
                   .join(_p, _ap.c.plate_id==_p.c.plate_id)
                   .join(_c, _c.c.copy_id==_p.c.copy_id)
                   .join(_l, _c.c.library_id==_l.c.library_id)
                   .join(_screen,_a.c.screen_id==_screen.c.screen_id)
                )
            .where(_ap.c.replicate_ordinal==0))
        
        if screen_facility_id is not None:
            _assay_plates_query = _assay_plates_query.where(
                _screen.c.facility_id==screen_facility_id)
        if library_screening_id is not None:
            _assay_plates_query = _assay_plates_query.where(
                _ls.c.activity_id==library_screening_id)
        _assay_plates = _assay_plates_query.cte('assay_plates')
        _assay_plates_inner = _assay_plates_query.cte('assay_plates_inner')
        _plate_comment_apilogs = ApiLogResource.get_resource_comment_subquery(
            self._meta.resource_name)
        _library_comment_apilogs = \
            ApiLogResource.get_resource_comment_subquery('library')
        
        # Get the library and plate keys to prefilter the logs
        with get_engine().connect() as conn:
            library_names = [x[0] for x in 
                conn.execute(select([distinct(_assay_plates.c.short_name)])
                    .select_from(_assay_plates))]
            plate_keys = [x[0] for x in 
                conn.execute(select([
                        distinct(_assay_plates.c.plate_key)])
                    .select_from(_assay_plates)) ]
            _plate_comment_apilogs = _plate_comment_apilogs.where(
                _apilog.c.key.in_(plate_keys))
            _library_comment_apilogs = _library_comment_apilogs.where(
                _apilog.c.key.in_(library_names))
        _library_comment_apilogs = \
            _library_comment_apilogs.cte('_library_comment_apilogs')
        _plate_comment_apilogs = _plate_comment_apilogs.cte('_comment_apilogs')

        stmt = ( 
            select([
                _assay_plates.c.short_name.label('library_short_name'),
                _l.c.library_name.label('library_name'),
                _l.c.screening_status.label('library_screening_status'),
                ( select([
                    func.array_to_string(
                        func.array_agg(literal_column('name')), 
                        LIST_DELIMITER_SQL_ARRAY),
                    ])
                    .select_from(
                        select([distinct(_c.c.name)])
                        .select_from(_c.join(
                            _assay_plates_inner,_c.c.copy_id
                                ==_assay_plates_inner.c.copy_id))
                        .where(_assay_plates_inner.c.plate_number
                            ==text('assay_plates.plate_number'))
                        .order_by(_c.c.name).alias('inner_copies'))
                    ).label('copies_screened'),
                ( select([
                    func.array_to_string(
                        func.array_agg(literal_column('comments')), 
                        LIST_DELIMITER_SQL_ARRAY),
                    ])
                    .select_from(
                        select([_concat_with_sep(
                            (_c.c.name,_c.c.comments),': ').label('comments')])
                        .select_from(_c.join(
                            _assay_plates_inner,_c.c.copy_id
                                ==_assay_plates_inner.c.copy_id))
                        .where(_assay_plates_inner.c.plate_number
                            ==text('assay_plates.plate_number'))
                        .where(func.trim(_c.c.comments)!='')
                        .group_by(_c.c.name, _c.c.comments)
                        .order_by(_c.c.name).alias('inner_copy_comments'))
                    ).label('copy_comments'),
                ( select([func.min(_assay_plates_inner.c.date_of_activity)])
                    .select_from(_assay_plates_inner)
                    .where(_assay_plates_inner.c.plate_number
                        ==text('assay_plates.plate_number'))
                    ).label('first_date_screened'),
                ( select([func.max(_assay_plates_inner.c.date_of_activity)])
                    .select_from(_assay_plates_inner)
                    .where(_assay_plates_inner.c.plate_number
                        ==text('assay_plates.plate_number'))
                    ).label('last_date_screened'),
                _assay_plates.c.plate_number,
                func.count(_assay_plates.c.assay_plate_id).label('assay_plate_count'),
                func.count(distinct(_assay_plates.c.activity_id))
                    .label('screening_count'),
                (
                    select([func.array_to_string(
                        func.array_agg(
                            _concat(                            
                                cast(_library_comment_apilogs.c.name,
                                    sqlalchemy.sql.sqltypes.Text),
                                LIST_DELIMITER_SUB_ARRAY,
                                cast(_library_comment_apilogs.c.date_time,
                                    sqlalchemy.sql.sqltypes.Text),
                                LIST_DELIMITER_SUB_ARRAY,
                                _library_comment_apilogs.c.comment)
                        ), 
                        LIST_DELIMITER_SQL_ARRAY) ])
                    .select_from(_library_comment_apilogs)
                    .where(_library_comment_apilogs.c.key
                        ==_assay_plates.c.short_name)
                    ).label('library_comment_array'),
                (
                    select([func.array_to_string(
                        func.array_agg(
                            _concat(                            
                                cast(_plate_comment_apilogs.c.name,
                                    sqlalchemy.sql.sqltypes.Text),
                                LIST_DELIMITER_SUB_ARRAY,
                                cast(_plate_comment_apilogs.c.date_time,
                                    sqlalchemy.sql.sqltypes.Text),
                                LIST_DELIMITER_SUB_ARRAY,
                                _plate_comment_apilogs.c.comment)
                        ), 
                        LIST_DELIMITER_SQL_ARRAY) ])
                    .select_from(_plate_comment_apilogs)
                    .where(_plate_comment_apilogs.c.key.ilike(
                        _concat(
                            _assay_plates.c.short_name,
                            '/%/',
                            cast(_assay_plates.c.plate_number, 
                                sqlalchemy.sql.sqltypes.Text))))
                    ).label('comment_array'),
            ])
            .select_from(
                _assay_plates.join(
                    _l,_assay_plates.c.library_id==_l.c.library_id))
            .group_by(
                _assay_plates.c.short_name,
                _l.c.library_name,
                _l.c.screening_status,
                _assay_plates.c.plate_number) )

        # general setup
        
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        
        if not order_clauses:
            stmt = stmt.order_by("plate_number")
        
        # compiled_stmt = str(stmt.compile(
        # dialect=postgresql.dialect(),
        # compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)
        
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
        
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash, param_hash=param_hash,
            is_for_detail=False,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None))
        
    def build_list_response(self, request, schema=None, **kwargs):

        meta = kwargs.get('meta', {})

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False

        if schema is None:
            raise Exception('schema not initialized')
        
        manual_field_includes = set(param_hash.get('includes', []))

        # FIXME: plates_screened has been replaced by 
        # librarycopyplate.build_screened_plate_response?
        for_screen_facility_id = param_hash.pop('for_screen_facility_id', None)
        if for_screen_facility_id is not None:
            authorized = self.get_screen_resource()._meta.authorization\
                .has_screen_read_authorization(
                    request.user, for_screen_facility_id)
            if authorized is False:
                raise PermissionDenied
                
        library_screening_id = param_hash.pop('library_screening_id', None)
        if library_screening_id is not None:
            authorized = self.get_library_screening_resource()._meta\
                .authorization.has_activity_read_authorization(
                    request.user, library_screening_id)
            if authorized is False:
                raise PermissionDenied

        cherry_pick_request_id = param_hash.pop('cherry_pick_request_id', None)
        if cherry_pick_request_id is not None:
            authorized = self.get_cpr_resource()._meta.authorization\
                .has_cherry_pick_read_authorization(
                    request.user, cherry_pick_request_id)
            if authorized is False:
                raise PermissionDenied
            param_hash['cpr'] = cherry_pick_request_id
        
        is_for_detail = kwargs.pop('is_for_detail', False)
        
        library_id = None
        library_short_name = param_hash.pop('library_short_name',
            param_hash.get('library_short_name__eq', None))
        if not library_short_name:
            logger.info('no library_short_name provided')
        else:
            param_hash['library_short_name__eq'] = library_short_name
            library_id = Library.objects.get(short_name=library_short_name).library_id
        
        copy_id = None
        copy_name = param_hash.pop('copy_name',
            param_hash.get('copy_name', None))
        if copy_name and library_id:
            param_hash['copy_name__eq'] = copy_name
            copy_id = Copy.objects.get(library_id=library_id, name=copy_name).copy_id
        plate_number = param_hash.pop('plate_number',
            param_hash.get('plate_number', None))
        if plate_number:
            param_hash['plate_number__eq'] = plate_number
        plate_ids = param_hash.pop('plate_ids', None)
        
        if plate_ids is not None:
            if isinstance(plate_ids,basestring):
                plate_ids = [int(x) for x in plate_ids.split(',')]
        
        raw_plate_search_data = param_hash.pop(SCHEMA.API_PARAM_SEARCH, None)
        if len(filter(lambda x: x is not None, 
            [cherry_pick_request_id, for_screen_facility_id, 
                library_screening_id, plate_ids, raw_plate_search_data]))>1:
            raise BadRequestError(
                key='filters', 
                msg='Mutually exclusive params: %r'
                    % ['cherry_pick_request_id', 'for_screen_facility_id', 
                    'library_screening_id', 'plate_ids',SCHEMA.API_PARAM_SEARCH])
            
        # If querying in the context of a library, copy, or plate, construct a
        # log key to use for the comment subquery.
        log_key = '/'.join(str(x) if x is not None else '%' 
            for x in [library_short_name,copy_name,plate_number])
        if log_key == '%/%/%':
            log_key = None

        # Use cherry_pick_request_id, screen_id, library_screening_id, or 
        # search data to pre-filter for plate_ids
        # TODO: grab keys as well for log query performance
        
        if cherry_pick_request_id is not None:
            _lcp = self.bridge['lab_cherry_pick']
            _well = self.bridge['well']
            _p = self.bridge['plate']
            cpr_plates = (
                select([distinct(_p.c.plate_id).label('plate_id')])
                .select_from(
                    _lcp.join(_well,_lcp.c.source_well_id==_well.c.well_id)
                        .join(_p,and_(
                            _p.c.plate_number==_well.c.plate_number,
                            _p.c.copy_id==_lcp.c.copy_id)))
                .where(_lcp.c.cherry_pick_request_id==cherry_pick_request_id)
                #.where(_lcp.c.selected==True)
            )
            with get_engine().connect() as conn:
                plate_ids = [x[0] for x in 
                    conn.execute(cpr_plates)]
        
        # FIXME: plates_screened has been replaced by 
        # librarycopyplate.build_screened_plate_response?
        if for_screen_facility_id:
            manual_field_includes.add('assay_plate_count')
            # plates screened
            with get_engine().connect() as conn:
                plate_ids = [x[0] for x in 
                    conn.execute(
                        self.get_screen_librarycopyplate_subquery(
                            for_screen_facility_id))]
        if library_screening_id is not None:
            manual_field_includes.add('assay_plate_count')
            with get_engine().connect() as conn:
                plate_ids = [x[0] for x in 
                    conn.execute(
                        self.get_libraryscreening_plate_subquery(
                            library_screening_id))]

        # Parse plate search OR-clauses:
        # POST data: line delimited plate-range text entered by the user
        # Embedded plate_search url params: array of plate-ranges
        plate_search_errors = set()
        if raw_plate_search_data:
            logger.info('plate raw_search_data: %r', raw_plate_search_data)
            plate_search_data = self.parse_plate_copy_search(raw_plate_search_data)
            logger.info('parsed plate search data: %r', plate_search_data)
        
            if plate_search_data is not None:
                (plates, errors) = self.find_plates(plate_search_data)
                plate_search_errors.update(errors)
                plate_ids = [p.plate_id for p in plates]
                logger.info('found plates %r for search: %r', 
                    plate_ids, plate_search_data)
            
        # general setup
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filename = self._get_filename(
            readable_filter_hash, schema, is_for_detail)
        filter_expression = \
            self._meta.authorization.filter(request.user,filter_expression)
        
        order_params = set(param_hash.get('order_by', []))
            
        if 'location' in order_params or '-location' in order_params:
            temp = []
            for v in order_params:
                if v == 'location':
                    temp.extend(['room','freezer','shelf','bin'])
                elif v == '-location':
                    temp.extend(['-room','-freezer','-shelf','-bin'])
                else:
                    temp.append(v)
            order_params = temp
        
        logger.info('filter hash keys: %r', filter_hash.keys())
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
         
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            # use "use_vocab" as a proxy to also adjust siunits for viewing
            rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
                field_hash, rowproxy_generator)
        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)
 
        # specific setup 
 
        _apilog = self.bridge['reports_apilog']
        _diff = self.bridge['reports_logdiff']
        _p = self.bridge['plate']
        _well = self.bridge['well']
        _pl = self.bridge['plate_location']
        _c = self.bridge['copy']
        _l = self.bridge['library']
        _ls = self.bridge['library_screening']
        _a = self.bridge['activity']
        _ap = self.bridge['assay_plate']
        _screen = self.bridge['screen']
        _plate_cte = (
            self.get_librarycopyplate_cte(
                filter_hash=filter_hash,
                library_id=library_id, copy_id=copy_id, plate_ids=plate_ids)
                    .cte('plate_cte'))
        _plate_statistics = (
            self.get_plate_copywell_statistics_cte(
                filter_hash=filter_hash,
                library_id=library_id, copy_id=copy_id, plate_ids=plate_ids)
                    .cte('plate_statistics'))
        _plate_screening_statistics = (
            self.get_plate_screening_statistics_cte(
                filter_hash=filter_hash,
                library_id=library_id, copy_id=copy_id, plate_ids=plate_ids)
                    .cte('plate_screening_statistics'))
            
        # Status Date/performedBy: Use window function for performance:
        #   SELECT DISTINCT ON (reports_apilog.key)
        #   last_value(date_time) OVER wnd as date_time,
        #   last_value(reports_apilog.id) OVER wnd as id,
        #   reports_apilog.key
        #  FROM reports_apilog
        #  where ref_resource_name = 'librarycopyplate'
        #  WINDOW wnd AS (
        #    PARTITION BY reports_apilog.id ORDER BY date_time
        #    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING )
        w_params = { 
            'order_by': _apilog.c.date_time,
            'partition_by': _apilog.c.id }
        status_apilogs = (
            select([
                func.last_value(_apilog.c.date_time).over(**w_params).label('date_time'),
                func.last_value(_apilog.c.id).over(**w_params).label('id'),
                _apilog.c.key
                ])
            .select_from(_apilog.join(_diff, _apilog.c.id==_diff.c.log_id))
            .where(_apilog.c.ref_resource_name==self._meta.resource_name)
            .where(_diff.c.field_key=='status')
            .distinct(_apilog.c.key)
            )
        if log_key:
            status_apilogs = status_apilogs.where(_apilog.c.key.ilike(log_key))
        status_updated_apilogs = status_apilogs.cte('updated_apilogs')
        _user_cte = ScreensaverUserResource.get_user_cte().cte('user_cte')
        _status_apilogs = (
            select([
                _apilog.c.date_time,
                _apilog.c.key,
                _user_cte.c.username,
                _user_cte.c.name,                    
                ])
            .select_from(_apilog.join(
                status_updated_apilogs,
                    _apilog.c.id==status_updated_apilogs.c.id)
                .join(_user_cte, _apilog.c.username==_user_cte.c.username))
            )
        _comment_apilogs = ApiLogResource.get_resource_comment_subquery(
            self._meta.resource_name)
        if log_key:
            _status_apilogs = _status_apilogs.where(_apilog.c.key.ilike(log_key))
            _comment_apilogs = _comment_apilogs.where(_apilog.c.key.ilike(log_key))
        _status_apilogs = _status_apilogs.cte('_status_apilogs')
        _comment_apilogs = _comment_apilogs.cte('_comment_apilogs')

        _library_comment_apilogs = \
            ApiLogResource.get_resource_comment_subquery('library')
        if library_short_name is not None:
            _library_comment_apilogs = _library_comment_apilogs.where(
                _apilog.c.key==library_short_name)
        _library_comment_apilogs = \
            _library_comment_apilogs.cte('_library_comment_apilogs')

        custom_columns = {
            'assay_plate_count': (
                select([func.count(None)])
                    .select_from(_ap)
                    .where(_ap.c.plate_id==_plate_cte.c.plate_id)),
            'librarycopyplate_id': (
                _concat(_l.c.short_name,'/',_c.c.name,'/', 
                    cast(_p.c.plate_number, sqlalchemy.sql.sqltypes.Text))),
            'copyplate_id': (
                _concat(_c.c.name,'/',
                    cast(_p.c.plate_number, sqlalchemy.sql.sqltypes.Text))),
            'remaining_well_volume': _plate_statistics.c.remaining_well_volume,
            'avg_remaining_volume': _plate_statistics.c.avg_well_remaining_volume,
            'min_remaining_volume': _plate_statistics.c.min_well_remaining_volume,
            'max_remaining_volume': _plate_statistics.c.max_well_remaining_volume,
            'min_molar_concentration': _plate_statistics.c.min_molar_concentration,
            'max_molar_concentration': _plate_statistics.c.max_molar_concentration,
            'min_mg_ml_concentration': _plate_statistics.c.min_mg_ml_concentration,
            'max_mg_ml_concentration': _plate_statistics.c.max_mg_ml_concentration,
            'first_date_screened': _plate_screening_statistics.c.first_date_screened,
            'last_date_screened': _plate_screening_statistics.c.last_date_screened,
            'status_date': _status_apilogs.c.date_time,
            'status_performed_by': _status_apilogs.c.name,
            'status_performed_by_username': _status_apilogs.c.username,
            'experimental_copy_well_count': (
                select([func.count(None)])
                .select_from(_well)
                .where(_well.c.plate_number==_p.c.plate_number)
                .where(_well.c.library_well_type==WELL_TYPE.EXPERIMENTAL)),

            'comment_array': (
                select([func.array_to_string(
                    func.array_agg(
                        _concat(                            
                            cast(_comment_apilogs.c.name,
                                sqlalchemy.sql.sqltypes.Text),
                            LIST_DELIMITER_SUB_ARRAY,
                            cast(_comment_apilogs.c.date_time,
                                sqlalchemy.sql.sqltypes.Text),
                            LIST_DELIMITER_SUB_ARRAY,
                            _comment_apilogs.c.comment)
                    ), 
                    LIST_DELIMITER_SQL_ARRAY) ])
                .select_from(_comment_apilogs)
                .where(_comment_apilogs.c.key==_plate_cte.c.key)
                ),
            'library_comment_array': (
                select([func.array_to_string(
                    func.array_agg(
                        _concat(                            
                            cast(_library_comment_apilogs.c.name,
                                sqlalchemy.sql.sqltypes.Text),
                            LIST_DELIMITER_SUB_ARRAY,
                            cast(_library_comment_apilogs.c.date_time,
                                sqlalchemy.sql.sqltypes.Text),
                            LIST_DELIMITER_SUB_ARRAY,
                            _library_comment_apilogs.c.comment)
                    ), 
                    LIST_DELIMITER_SQL_ARRAY) ])
                .select_from(_library_comment_apilogs)
                .where(_library_comment_apilogs.c.key==_l.c.short_name)
                ),
        }

        base_query_tables = ['plate', 'copy', 'plate_location', 'library']

        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)
        # build the query statement

        j = join(_plate_cte, _p, _p.c.plate_id == _plate_cte.c.plate_id)
        j = j.join(_c, _p.c.copy_id == _c.c.copy_id)
        j = j.join(
            _pl, _p.c.plate_location_id == _pl.c.plate_location_id,
            isouter=True)
        j = j.join(_l, _c.c.library_id == _l.c.library_id)

        if set(['status_date','status_performed_by',
            'status_performed_by_username']) | set(field_hash.keys()):
            j = j.outerjoin(_status_apilogs,_plate_cte.c.key==_status_apilogs.c.key)
        if set(['avg_remaining_volume','min_remaining_volume',
            'max_remaining_volume']) | set(field_hash.keys()):
            j = j.outerjoin(_plate_statistics,_p.c.plate_id==_plate_statistics.c.plate_id)     
        if (set(['first_date_screened', 'last_date_screened'])
            | set(field_hash.keys()) ):
            j = j.outerjoin(_plate_screening_statistics,
                _p.c.plate_id==_plate_screening_statistics.c.plate_id)

        # FIXME: replaced by build_screened_plate_response? 20170912
        if for_screen_facility_id is not None:
            custom_columns['screening_count'] = (
                select([distinct(_ls.c.activity_id)])
                    .select_from(_ap.join(
                        _screen, _ap.c.screen_id==_screen.c.screen_id))
                    .where(_ap.c.plate_id==_plate_cte.c.plate_id)
                    .where(_screen.c.facility_id==for_screen_facility_id)
            )
            custom_columns['assay_plate_count'] = (
                select([func.count(None)])
                    .select_from(_ap.join(
                        _screen, _ap.c.screen_id==_screen.c.screen_id))
                    .where(_ap.c.plate_id==_plate_cte.c.plate_id)
                    .where(_screen.c.facility_id==for_screen_facility_id)
            )
            custom_columns['first_date_screened'] = (
                select([func.min(_a.c.date_of_activity)])
                    .select_from(
                        _ap.join(_screen, _ap.c.screen_id==_screen.c.screen_id)
                           .join(_a, _ap.c.library_screening_id==_a.c.activity_id)
                        )
                    .where(_ap.c.plate_id==_plate_cte.c.plate_id)
                    .where(_screen.c.facility_id==for_screen_facility_id)
                )
            custom_columns['last_date_screened'] = (
                select([func.max(_a.c.date_of_activity)])
                    .select_from(
                        _ap.join(_screen, _ap.c.screen_id==_screen.c.screen_id)
                           .join(_a, _ap.c.library_screening_id==_a.c.activity_id)
                        )
                    .where(_ap.c.plate_id==_plate_cte.c.plate_id)
                    .where(_screen.c.facility_id==for_screen_facility_id)
                )
                
        if library_screening_id is not None:
            custom_columns['assay_plate_count'] = (
                select([func.count(None)])
                    .select_from(_ap)
                    .where(_ap.c.plate_id==_plate_cte.c.plate_id)
                    .where(_ap.c.library_screening_id==library_screening_id)
            ),
                
        stmt = select(columns.values()).select_from(j)
        
        if plate_ids is not None:
            stmt = stmt.where(_p.c.plate_id.in_(plate_ids))
        if library_id is not None:
            stmt = stmt.where(_l.c.library_id==library_id)
        if copy_id is not None:
            stmt = stmt.where(_p.c.copy_id==copy_id)

        # general setup
             
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
 
        if not order_clauses:
            stmt = stmt.order_by("plate_number", "copy_name")

        # compiled_stmt = str(stmt.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)
         
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
            
        if plate_search_errors:
            meta[SCHEMA.API_MSG_WARNING] = ', '.join(sorted(plate_search_errors))
        
        logger.info('stream LibraryCopyPlate...')
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash, param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=meta)

    @classmethod
    def get_screen_librarycopyplate_subquery(cls, for_screen_facility_id):
        
        bridge = get_tables()
        _screen = bridge['screen']
        _assay_plate = bridge['assay_plate']
        _plate = bridge['plate']
        
        j = _plate
        j = j.join(_assay_plate, _plate.c.plate_id == _assay_plate.c.plate_id)
        j = j.join(_screen, _assay_plate.c.screen_id == _screen.c.screen_id)
        
        screen_lcps = (
            select([
                distinct(_plate.c.plate_id).label('plate_id')])
            .select_from(j)
            .where(_screen.c.facility_id == for_screen_facility_id)
            .where(_assay_plate.c.replicate_ordinal == 0))
        return screen_lcps

    @classmethod
    def get_libraryscreening_plate_subquery(cls, library_screening_id):
        
        bridge = get_tables()
        _assay_plate = bridge['assay_plate']
        _plate = bridge['plate']
        
        j = _plate
        j = j.join(_assay_plate, _plate.c.plate_id == _assay_plate.c.plate_id)
        
        _query = (
            select([
                distinct(_plate.c.plate_id).label('plate_id')])
            .select_from(j)
            .where(_assay_plate.c.library_screening_id == library_screening_id)
            .where(_assay_plate.c.replicate_ordinal == 0))
        return _query

    @write_authorization
    @un_cache        
    @transaction.atomic    
    def batch_edit(self, request, **kwargs):
        '''
        Batch edit is a POST operation:
        
        librarycopyplate batch_edit uses a POST form to send both
        the search data (3 types of search filter: SCHEMA.API_PARAM_NESTED_SEARCH,  
        SCHEMA.API_PARAM_SEARCH, and ordinary 'GET' search params),
        as well as the update data (in the form of "plate_info" and "plate_location")
        (Instead of sending all plates to be PATCHED);
        '''
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        convert_request_method_to_put(request)
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        logger.info('batch_edit plates...')
        if DEBUG_PLATE_EDIT:
            logger.info('param_hash: %r', param_hash)
        
        plate_info_data = param_hash.pop('plate_info', None)
        if plate_info_data: 
            plate_info_data = json.loads(plate_info_data)
        if DEBUG_PLATE_EDIT:
            logger.info('plate_info: %r', plate_info_data)
        
        location_data = param_hash.pop('plate_location', None)
        if location_data:
            location_data = json.loads(location_data)
        if DEBUG_PLATE_EDIT:
            logger.info('plate_location: %r', location_data)
        if plate_info_data is None and location_data is None:
            msg = '"data" must contain one of %r' % ['plate_info', 'plate_location']
            raise ValidationError({
                'plate_info': msg, 'plate_location': msg })
        
        # Use the rest of the POST data for the plate search
        plate_kwargs = param_hash.copy()

        nested_search_data = param_hash.pop(SCHEMA.API_PARAM_NESTED_SEARCH, None)
        if nested_search_data:
            if DEBUG_PLATE_EDIT:
                logger.info('nested_search_data: %r', nested_search_data)
            plate_kwargs[SCHEMA.API_PARAM_NESTED_SEARCH] = json.loads(nested_search_data)
        
        if DEBUG_PLATE_EDIT:
            logger.info('batch_edit: plate_kwargs: %r', plate_kwargs)
        
        meta = { SCHEMA.API_MSG_RESULT: {} }
        if plate_info_data:
            logger.info('batch_edit: Get the original plate data...')
            plate_kwargs['includes'] = ['plate_id']
            original_data = self._get_list_response_internal(**plate_kwargs)
            if not original_data:
                raise Http404
            logger.info('got the plate objects for search data: %d', len(original_data))
            if DEBUG_PLATE_EDIT:
                logger.info('plates %r', ['{copy_name}/{plate_number}'.format(**lcp)
                    for lcp in original_data])
            
            plate_ids = [x['plate_id'] for x in original_data]

            query = Plate.objects.all().filter(plate_id__in=plate_ids)
            for key,value in plate_info_data.items():
                fi = schema['fields'].get(key,None)
                if fi is None or 'u' not in fi.get('editability',[]):
                    raise ValidationError(
                        key=key, msg='is not editable')
                query.update(**{ key: value })
                if key=='status':
                    if value == SCHEMA.VOCAB.plate.status.AVAILABLE:
                        query.update(**{ 'date_plated': _now()})
                        # query.update(**{ 'date_retired': None})
                    elif value in SCHEMA.VOCAB.plate.status.retired_statuses:
                        query.update(**{ 'date_retired': _now()})
                    
            logger.info('batch update complete, get new state for logging...')
            
            new_data = self._get_list_response_internal(**plate_kwargs)
            
            parent_log = self.make_log(request)
            # NOTE: "batch" log is keyed for the resource name only
            parent_log.key = self._meta.resource_name
            parent_log.uri = self._meta.resource_name
            parent_log.save()
            plate_kwargs['parent_log'] = parent_log
            logs = self.log_patches(request, original_data, new_data,**plate_kwargs)
            patch_count = len(logs)
            update_count = len([x for x in logs if x.diffs ])
            unchanged_count = patch_count - update_count
            meta[SCHEMA.API_MSG_RESULT]['plate'] = { 
                SCHEMA.API_MSG_SUBMIT_COUNT : patch_count, 
                SCHEMA.API_MSG_UPDATED: update_count, 
                SCHEMA.API_MSG_UNCHANGED: unchanged_count, 
                SCHEMA.API_MSG_COMMENTS: parent_log.comment
            }
            
        if location_data:
            plate_location_fields = ['room', 'freezer', 'shelf', 'bin']
            if (not location_data 
                    or not set(plate_location_fields) | set(location_data.keys())):
                raise ValidationError(
                    key='data', msg='must contain plate location fields')
            logger.info('find or create the location: %r', location_data)
            original_location_data = (
                self.get_platelocation_resource()
                    ._get_detail_response_internal(**location_data))
            if not original_location_data:
                logger.info('plate location not found: %r', location_data)
            else:
                location_data['copy_plate_ranges'] = \
                    original_location_data.get('copy_plate_ranges',None)
    
            logger.info('batch_edit location: Get the original plate data...')
            original_data = self._get_list_response_internal(**plate_kwargs)
            if not original_data:
                raise Http404
            logger.info('got the plate objects for search data: %d',
                len(original_data))
            if DEBUG_PLATE_EDIT:
                logger.info('plates for batch edit %r', 
                    ['{copy_name}/{plate_number}'.format(**lcp)
                        for lcp in original_data])
            
            library_copy_ranges = defaultdict(list)
            for plate_data in original_data:
                library_copy = '{library_short_name}:{copy_name}'.format(**plate_data)
                library_copy_ranges[library_copy].append(int(plate_data['plate_number']))
            copy_plate_ranges = []
            for library_copy,plate_range in library_copy_ranges.items():
                plate_range = sorted(plate_range)
                copy_plate_ranges.append(
                    '%s:%s-%s' 
                    % (library_copy, 
                       str(plate_range[0]),
                       str(plate_range[-1]) ))
    
            logger.info(
                'patch original location: %r, with copy_ranges: %r', 
                location_data, copy_plate_ranges) 
            location_copy_ranges = \
                location_data.get('copy_plate_ranges', [])
            location_copy_ranges.extend(copy_plate_ranges)
            location_data['copy_plate_ranges'] = location_copy_ranges
            
            # NOTE: "batch" log is keyed for the resource name only
            parent_log = self.make_log(request)
            parent_log.key = self._meta.resource_name
            parent_log.uri = self._meta.resource_name
            parent_log.save()
            kwargs['parent_log'] = parent_log
            
            # Consider: create PATCH logs for each of the plates patched as well
            
            response = self.get_platelocation_resource().patch_detail(
                request, 
                data=location_data,
                **kwargs)
            
            # Modify result meta to make specific for LibraryCopyPlate
            plate_location_name = '-'.join(
                [str(location_data[k]) for k in plate_location_fields])
            _data = self.get_serializer().deserialize(
                LimsSerializer.get_content(response), JSON_MIMETYPE)
            results = _data[API_RESULT_META][SCHEMA.API_MSG_RESULT]
            meta[SCHEMA.API_MSG_RESULT]['plate_location'] = { 
                'Plate Location Result: %s' % plate_location_name: results,
                'Plate Copy Ranges patched': ','.join(copy_plate_ranges),
                'Plate Copy Count': len(original_data)
            }
        logger.info('results plate location patch: %r', meta)
        return self.build_response(
            request, {API_RESULT_META: meta }, response_class=HttpResponse, 
            **kwargs)
    
    def validate(self, _dict, patch=False, schema=None):
        errors = DbApiResource.validate(self, _dict, patch=patch, schema=schema)
        
        if set(['status','is_active']) | set(_dict.keys()):
            
            if _dict.get('is_active',False) is True:
                if _dict.get('status', None) != SCHEMA.VOCAB.plate.status.AVAILABLE:
                    errors['is_active'] = 'Requires status == "available"'
        
        return errors
    
    @write_authorization
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):
        
        logger.debug('patch obj, deserialized: %r', deserialized)
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        fields = schema['fields']

        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
        logger.debug('id_kwargs: %r', id_kwargs)
        required_kwargs_check = ['copy_name', 'plate_number']
        if ( not id_kwargs 
                or not set(required_kwargs_check) & set(id_kwargs.keys())):
            raise ValidationError({
                k:'required' for k in required_kwargs_check if k not in id_kwargs })
        try:
            plate = Plate.objects.get(
                copy__name=id_kwargs['copy_name'],
                plate_number=id_kwargs['plate_number'])
        except ObjectDoesNotExist:
            logger.info('plate does not exist: %r',
                id_kwargs)
            raise
        
        initializer_dict = self.parse(deserialized, schema=schema, create=False)
        errors = self.validate(initializer_dict, schema=schema,patch=True)
        if errors:
            raise ValidationError(errors)
        for key, val in initializer_dict.items():
            if hasattr(plate, key):
                setattr(plate, key, val)
            if key=='status':
                if val == SCHEMA.VOCAB.plate.status.AVAILABLE:
                    plate.date_plated = _now()
                    # plate.date_retired = None
                elif val in SCHEMA.VOCAB.plate.status.retired_statuses:
                    plate.date_retired = _now() 
        if initializer_dict:
            
            # New 20170407 - is_active check
            if plate.status != SCHEMA.VOCAB.plate.status.AVAILABLE:
                plate.is_active = False
            
            plate.save()
            
        # Process plate location fields separately
        plate_location_fields = ['room', 'freezer', 'shelf', 'bin']
        location_data = {}
        update_plate_location = False
        for k in plate_location_fields:
            location_data[k] = deserialized.get(k,None)
            if location_data[k]:
                update_plate_location = True
        if update_plate_location:
            original_location_data = (
                self.get_platelocation_resource()
                    ._get_detail_response_internal(**location_data))
            try:
                plate_location = PlateLocation.objects.get(**location_data)
                logger.info('plate location found: %r', plate_location)
                
            except ObjectDoesNotExist:
                logger.info('plate location not found: %r', location_data)
                logger.info(
                    'plate location does not exist, creating: %r',
                    location_data)
                plate_location = PlateLocation.objects.create(**location_data)
                plate_location.save()
                
            if plate.plate_location != plate_location:
                logger.info('update plate location: %r, to %r', plate,  plate_location)
                plate.plate_location = plate_location
                plate.save()
            
            # FIXME: when using patch/post list, this will create a log for
            # each plate location update in the list, need to move this out to
            # the patch-post_list methods.
            new_location_data = (
                self.get_platelocation_resource()
                    ._get_detail_response_internal(**location_data))
            log = self.get_platelocation_resource().log_patch(
                request, original_location_data, new_location_data,**kwargs)
            if log:
                log.save()
                logger.info('log created; %r', log)
                    
        return { API_RESULT_OBJ: plate }

class UserAgreementResource(DbApiResource):
    
    VOCAB_USER_AGREEMENT_SM = SCHEMA.VOCAB.user_agreement.type.SM
    VOCAB_USER_AGREEMENT_RNAI = SCHEMA.VOCAB.user_agreement.type.RNAI
    
    VOCAB_FILE_TYPE_SMUA = SCHEMA.VOCAB.user_agreement.file_type.SMUA
    VOCAB_FILE_TYPE_RNAI = SCHEMA.VOCAB.user_agreement.file_type.RNAI
    
    class Meta:

        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'useragreement'
        authorization = UserGroupAuthorization(resource_name)
        serializer = LimsSerializer()

    def __init__(self, **kwargs):
        
        self.screensaveruser_resource = None
        self.attached_file_resource = None
        super(UserAgreementResource, self).__init__(**kwargs)

    def get_su_resource(self):
        if self.screensaveruser_resource is None:
            self.screensaveruser_resource = ScreensaverUserResource()
        return self.screensaveruser_resource
    
    def get_attached_file_resource(self):
        if self.attached_file_resource is None:
            self.attached_file_resource = AttachedFileResource()
        return self.attached_file_resource
    
    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url((r"^(?P<resource_name>%s)/" 
                 r"(?P<screensaver_user_id>([\d]+))/"
                 r"(?P<type>(\w+))%s$")
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
        ] 

    @read_authorization
    def get_detail(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)

    @read_authorization
    def get_list(self, request, **kwargs):
        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        kwargs['file_type__in'] = [self.VOCAB_FILE_TYPE_RNAI, self.VOCAB_FILE_TYPE_SMUA]
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, **kwargs):
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        is_for_detail = kwargs.pop('is_for_detail', False)
        
        manual_field_includes = set(param_hash.get('includes', []))
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filename = self._get_filename(
            readable_filter_hash, schema, is_for_detail)
        filter_expression = \
            self._meta.authorization.filter(request.user,filter_expression)
              
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
         
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)
        
        # specific setup
        _su = self.bridge['screensaver_user']
        _user_cte = ScreensaverUserResource.get_user_cte().cte('ua_user')
        _lab_head = ScreensaverUserResource.get_lab_head_cte('ualh').cte('ulab_head')
        _user_agreement = self.bridge['user_agreement']
        _lab_head_ua = _user_agreement.alias('lab_head_ua')
        _af = self.bridge['attached_file']
        _lhaf = _af.alias('lhaf')
        _vocab = self.bridge['reports_vocabulary']
        
        username = param_hash.pop('username', None)
        screensaver_user_id = param_hash.pop('screensaver_user_id', None)
        
        # Create an "all_users" table that contains one entry for each 
        # agreement type for each user in the system.
        agreement_type_table = (
            select([
                _vocab.c.key.label('type'),
                _vocab.c.ordinal,
                _vocab.c.is_retired.label('is_retired'),
            ])
            .select_from(_vocab)
            .where(_vocab.c.scope == 'useragreement.type')
            .where(func.coalesce(_vocab.c.is_retired,False) != True)
            .order_by(_vocab.c.ordinal))
        types = []
        queries = []
        with get_engine().connect() as conn:
            result = conn.execute(agreement_type_table)
            types =  [x[0] for x in result ]
        for type in types:
            query = (
                select([
                    _su.c.screensaver_user_id,
                    _su.c.username,
                    _su.c.lab_head_id,
                    literal_column("'%s'"%type).label('type')])
                .select_from(_su))
            if username:
                query = query.where(
                    _su.c.username == username)
            if screensaver_user_id:
                query = query.where(
                    _su.c.screensaver_user_id == screensaver_user_id)
            queries.append(query)
        all_users = union_all(*queries).cte('all_users')
      
        # Build a table of entered agreements
        j = _su
        j = j.join(
            _user_agreement, _user_agreement.c.screensaver_user_id 
                == _su.c.screensaver_user_id, isouter=True)
        j = j.join(
            _af, _user_agreement.c.file_id 
                == _af.c.attached_file_id, isouter=True)
        entered_agreements = select([
            _su.c.screensaver_user_id,
            _user_agreement.c.data_sharing_level,
            _user_agreement.c.type,
            _user_agreement.c.date_active,
            _user_agreement.c.date_expired,
            _user_agreement.c.date_notified,
            _af.c.attached_file_id.label('file_id'),
            _af.c.filename,
            ]).select_from(j).cte('entered_agreements')
        _user_entered_agreements = entered_agreements.alias('user_entered_agreements')        
        _lh_entered_agreements = entered_agreements.alias('lh_entered_agreements')        
        
        custom_columns = {
            'screensaver_user_id': all_users.c.screensaver_user_id,
            'username': _user_cte.c.username,
            'user_name': _user_cte.c.name,
            'user_first_name': _user_cte.c.first_name,
            'user_last_name': _user_cte.c.last_name,
            'user_email': _user_cte.c.email,
            'classification': _user_cte.c.classification,
            'type' : all_users.c.type,
            'data_sharing_level': _user_entered_agreements.c.data_sharing_level,
            'status': case([
                ( _user_entered_agreements.c.date_active != None, 
                  case([
                      (_user_entered_agreements.c.date_expired != None, 'expired')],
                      else_='active'))],
                else_='inactive'),  
            'date_active': _user_entered_agreements.c.date_active,
            'date_notified': _user_entered_agreements.c.date_notified,
            'date_expired': _user_entered_agreements.c.date_expired,
            'file_id': _user_entered_agreements.c.file_id,
            'filename': _user_entered_agreements.c.filename,
            'lab_head_id': _lh_entered_agreements.c.screensaver_user_id,
            'lab_head_name': _lab_head.c.name,
            'lab_affiliation_category': _lab_head.c.lab_affiliation_category,
            'lab_affiliation_name': _lab_head.c.lab_affiliation_name,
            'lab_head_data_sharing_level': _lh_entered_agreements.c.data_sharing_level,
            'lab_head_user_agreement_status': 
                case([
                ( _lh_entered_agreements.c.screensaver_user_id == None, None),
                ( _lh_entered_agreements.c.date_active != None, 
                    case([
                    (_lh_entered_agreements.c.date_expired != None, 'expired')],
                    else_='active'))],
                else_='inactive'),  
            'lab_head_file_id': _lh_entered_agreements.c.file_id,
            'lab_head_filename': _lh_entered_agreements.c.filename,
            }
        
        base_query_tables = [] 
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)
         
        # Build the final query by joining all_users to entered agreements
        j = all_users
        j = j.join(_user_cte, all_users.c.screensaver_user_id
            ==_user_cte.c.screensaver_user_id)
        j = j.join(_user_entered_agreements, 
            and_( all_users.c.screensaver_user_id
                ==_user_entered_agreements.c.screensaver_user_id,
                all_users.c.type==_user_entered_agreements.c.type),
            isouter=True)
        j = j.join(
            _lh_entered_agreements,
            and_(
#                 all_users.c.lab_head_id != all_users.c.screensaver_user_id,
                all_users.c.lab_head_id == _lh_entered_agreements.c.screensaver_user_id,
                _lh_entered_agreements.c.type== all_users.c.type),
            isouter=True
            )
        j = j.join(_lab_head,
            all_users.c.lab_head_id == _lab_head.c.screensaver_user_id, isouter=True)
        
        stmt = select(columns.values()).select_from(j)
        stmt = stmt.order_by(
            all_users.c.screensaver_user_id, 
            all_users.c.type)

        # general setup

        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        
        # compiled_stmt = str(stmt.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)
        
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
        
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None),
            use_caching=True)

    @write_authorization
    @un_cache
    @transaction.atomic
    def post_list(self, request, schema=None, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'post_list')

    @write_authorization
    @un_cache        
    @transaction.atomic
    def post_detail(self, request, parent_log=None, **kwargs):
        '''
        API NOTE: see ScreensaverUserResource.dispatch_useragreement_view:
        - performs user maintenance functions related to UA updates.
        
        Modified POST because:
        - Attached File may be included in request
        '''
        schema = kwargs.get('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        fields = schema['fields']
#         id_attribute = schema['id_attribute']
        
        # Perform manual deserialization; MULTIPART content is in POST dict
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        logger.debug('param_hash: %r', 
            {k:v for k,v in param_hash.items() if k != 'schema'})
        deserialized = {}
        for key in fields.keys():
            if param_hash.get(key, None) is not None:
                deserialized[key] = param_hash[key]
                
        screensaver_user_id = param_hash.get('screensaver_user_id')

        # POST is only used to create or reset to active; file is required
        # Note: see PATCH: status=deactivate
        attached_file = request.FILES.get('attached_file', None)
        if attached_file is None:
            status_update = deserialized.get('status', None)
            if status_update is None or status_update not in ('inactive','expire'):
                raise ValidationError(
                    key='attached_file',
                    msg='required to create or reactivate a user agreement')
        # NOT supporting normal deserialization with POST
        # if attached_file is None:
        #     # If no attached file, allow ordinary deserialization
        #     deserialized = self.deserialize(
        #         request, format=kwargs.get('format', None))
        #     logger.info('deserialized: %r', deserialized)

        deserialized['screensaver_user_id'] = screensaver_user_id
        id_kwargs = self.get_id(deserialized,validate=True, **kwargs)
        log = self.make_log(
            request, id_kwargs, schema=schema)
        log.parent_log = parent_log;
        log.save()
        original_data = self._get_detail_response_internal(**id_kwargs)

        # POST is only used to create or reset to active
        # Note: see PATCH: status=deactivate, expire
        try:
            user_agreement = UserAgreement.objects.get(**id_kwargs)
            logger.info('UA exists: resetting: %r', user_agreement)
            user_agreement.date_active=_now()
            user_agreement.date_expired = None
            user_agreement.date_notified = None
            user_agreement.save()
        except ObjectDoesNotExist:
            user_agreement = UserAgreement.objects.create(**id_kwargs)
            if 'date_active' not in deserialized:
                deserialized['date_active'] = _now().date().strftime("%Y-%m-%d")
            logger.info('UA DNE, creating: %r', user_agreement)
        
        # Perform the PATCH
        patch_result = self.patch_obj(request, deserialized, log=log, **param_hash)
        user_agreement = patch_result[API_RESULT_OBJ]
        user_agreement.save()

        # === Attached File ===
        attached_type = None
        if user_agreement.type == self.VOCAB_USER_AGREEMENT_SM:
            attached_type = self.VOCAB_FILE_TYPE_SMUA
        elif user_agreement.type == self.VOCAB_USER_AGREEMENT_RNAI:
            attached_type = self.VOCAB_FILE_TYPE_RNAI
        else:
            raise ValidationError(
                key='type', 
                msg='must be one of %r' % [
                    self.VOCAB_USER_AGREEMENT_SM, self.VOCAB_USER_AGREEMENT_RNAI])
        kwargs_for_attachedfile = { 'screensaver_user_id': screensaver_user_id }
        kwargs_for_attachedfile['type'] = attached_type
        kwargs_for_attachedfile['filename'] = deserialized.get('filename')
        
        logger.info('POST user agreement attached file: %r', kwargs_for_attachedfile)
        attached_file_resource = self.get_attached_file_resource()
        attached_file_response = \
            attached_file_resource.post_detail(request, **kwargs_for_attachedfile)
        logger.info('UserAgreement attached file result: %r', attached_file_response)
        if attached_file_response.status_code == 201:
            af_data = attached_file_resource._meta.serializer.deserialize(
                LimsSerializer.get_content(attached_file_response), JSON_MIMETYPE)
            logger.info('attached_file: %r', af_data)
            user_agreement.file_id = af_data['attached_file_id']
        else:
            error_resp = attached_file_resource._meta.serializer.deserialize(
                LimsSerializer.get_content(attached_file_response), JSON_MIMETYPE)
            logger.error('attached file error: %r, %r', 
                attached_file_response.status_code, error_resp)
            raise Exception('attached file resource error: %r', error_resp)
        user_agreement.save()
        
        new_data = self._get_detail_response_internal(**id_kwargs)
        
        self.log_patch(request, original_data, new_data, log, 
            schema=schema, parent_log=parent_log, full_create_log=True)
        log.save()
        
        data = { API_RESULT_DATA: [new_data]}
        if 'test_only' in param_hash:
            logger.info('test_only flag: %r', param_hash.get('test_only'))    
            raise InformationError(
                'successful post, "test_only" flag is set, rollback...')
        
        return self.build_response(request, data)
    
    @write_authorization
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):

        logger.info('patch user agreement item: %r', deserialized)
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        fields = schema['fields']
#         id_attribute = schema['id_attribute']
        id_kwargs = self.get_id(deserialized,validate=True,schema=schema,**kwargs)
        screensaver_user_id = id_kwargs['screensaver_user_id']
        screensaver_user = ScreensaverUser.objects.get(
            screensaver_user_id=screensaver_user_id)

        # PATCH may not be used to create a user agreement; see post_detail
        user_agreement = UserAgreement.objects.get(**id_kwargs)

        initializer_dict = self.parse(deserialized, schema=schema, create=True)
        if not initializer_dict:
            raise Exception('Empty patch')
        
        errors = self.validate(initializer_dict, schema=schema)
        if errors:
            raise ValidationError(errors)
        
        new_status = initializer_dict.pop('status', None)
        new_date_notified = initializer_dict.pop('date_notified', None)
        if new_status is not None:
            if initializer_dict:
                logger.error(
                    'Other actions not allowed if expiring or deactivating: %r', 
                    initializer_dict)
                raise ValidationError(
                    key='status', 
                    msg='other values may not be set if expiring or deactivating')
            if user_agreement.date_active is None:
                raise ValidationError(
                    key='status', msg='Agreement is not active')
        else:
            if user_agreement.date_expired is not None:
                msg = 'Expired agreements may not be patched'
                raise ValidationError({
                    k:msg for k in initializer_dict.keys() })
            if new_date_notified is not None:
                user_agreement.date_notified = new_date_notified
                    
        for key, val in initializer_dict.items():
            if hasattr(user_agreement, key):
                setattr(user_agreement, key, val)

        if user_agreement.date_active is None:
            raise ValidationError(
                key='date_active', msg='required')
        
        if user_agreement.data_sharing_level is None:
            raise ValidationError(
                key='data_sharing_level', msg='required')
        
        meta = {}
        if new_status is not None:
            if new_status == 'expired':
                if user_agreement.date_expired is not None:
                    raise ValidationError(
                        key='status', 
                        msg='Agreement may not be expired more than once')
                user_agreement.date_expired = _now()
                logger.info('UserAgreement %r expired', user_agreement)
            elif new_status == 'inactive':
                if user_agreement.date_expired is not None:
                    raise ValidationError(
                        key='status', 
                        msg='Agreement is already expired and may not be deactivated')
                user_agreement.date_active = None
                user_agreement.date_notified = None
                # 20181011 - leave the dsl on the user for information purposes
                # user_agreement.data_sharing_level = None
                user_agreement.file = None
                logger.info('UserAgreement %r deactivated', user_agreement)
            else:
                raise ValidationError(
                    key='status', msg='may only be set to "expired" or "inactive"')
        elif new_date_notified is not None:
            pass
        else:
            pass
            # No new status
            # TODO: if DSL does not match user screens, display warning?
            
        user_agreement.save()
        logger.info('UserAgreement %r patched', user_agreement)
        
        return { 
            API_RESULT_OBJ: user_agreement,
            API_RESULT_META: meta }

class ScreenAuthorization(UserGroupAuthorization):
    
    VOCAB_USER_AGREEMENT_SM = UserAgreementResource.VOCAB_USER_AGREEMENT_SM
    VOCAB_USER_AGREEMENT_RNAI = UserAgreementResource.VOCAB_USER_AGREEMENT_RNAI
    
    '''
    Implements the data viewing restrictions defined in the ICCB-L User 
    Agreement documents (Small Molecule and RNAi).
    
    Restriction mechanisms:
    1. Schema level: search and sort capability are removed for fields with 
    a Field.data_sharing_level of 1 or 2 (for restricted users).
    2. Query level: restricted users may only see Screens that they are 
    authorized to view per User Agreement viewing rules.
    3. Cursor Generator: restricted users may only see screen 
    properties having a Field.data_sharing_level matching the users access 
    level for the specific Screen.
    '''

    def __init__(self, *args, **kwargs):
        UserGroupAuthorization.__init__(self, *args, **kwargs)
        self._screen_overlap_table = None
        
    def _is_resource_authorized(
        self, user, permission_type, **kwargs):
        '''
        Override UserGroupAuthorization to give users permission for restricted
        read access to screens they are authorized to view.
        '''
        
        authorized = super(ScreenAuthorization, self)._is_resource_authorized(
            user, permission_type, **kwargs)
        if authorized is True:
            return True
        else:
            screensaver_user = ScreensaverUser.objects.get(username=user.username)
            authorized_screens = self.get_read_authorized_screens(screensaver_user)
            
            if authorized_screens:
                logger.info('_is_resource_authorized: %r - True', user)
                logger.debug(
                    'user: %r, authorized_screens: %r', 
                    screensaver_user.username, 
                    [x.facility_id for x in authorized_screens])
                return True
        
        return False

   
    def get_screen_overlapping_table(self):
        '''
        Create a current screen-overlap table from the database:
        - Fields: ['facility_id', 'data_sharing_level','screen_type',
        'overlapping_positive_screens']
        '''
        logger.info('get_screen_overlapping_table...')
        bridge = get_tables()
        _screen = bridge['screen']
        _overlap_screen = _screen.alias('overlap_screen')
        _screen_overlap = DbApiResource.get_create_screen_overlap_indexes()
        
        stmt = (
            select([
                _screen.c.facility_id,
                _screen.c.data_sharing_level,
                _screen.c.screen_type,
                func.array_agg(_overlap_screen.c.facility_id)
            ])
            .select_from(
                _screen
                    .join(
                        _screen_overlap, 
                        _screen_overlap.c.screen_id==_screen.c.screen_id, isouter=True)
                    .join(
                        _overlap_screen, 
                        _overlap_screen.c.screen_id
                            ==_screen_overlap.c.overlap_screen_id, isouter=True))
            .group_by(_screen.c.facility_id, _screen.c.data_sharing_level, 
                _screen.c.screen_type )
        )
        
        with get_engine().begin() as conn:
            _dict = {}
            rows = conn.execute(stmt).fetchall()
            for row in rows:
                _dict[row[0]] = dict(zip(
                    ['facility_id', 'data_sharing_level','screen_type',
                        'overlapping_positive_screens'],
                    row[0:]))
        return _dict
    
    def get_user_screens(self, screensaver_user):
        '''
        Return the screens that the user is a member of.
        '''
        if DEBUG_SCREEN_ACCESS:
            logger.info('get_user_screens %r', screensaver_user)
        my_screens = Screen.objects.all().filter(
            Q(lead_screener=screensaver_user)
            | Q(lab_head=screensaver_user)
            | Q(collaborators=screensaver_user))
        if DEBUG_SCREEN_ACCESS:
            logger.info('user: %r, screens: %r', 
                screensaver_user.username, [s.facility_id for s in my_screens])
        return set(my_screens)
    
    def get_user_data_sharing_level(self, screensaver_user, user_agreement_type):
        logger.info('get user data sharing agreements for %r, %r', 
            screensaver_user, user_agreement_type)
        active_agreements = \
            screensaver_user.useragreement_set.all()\
            .filter(type=user_agreement_type)\
            .filter(date_active__isnull=False)\
            .filter(date_expired__isnull=True)
        current_dsl = 0
        if active_agreements.exists():
            current_dsl = active_agreements[0].data_sharing_level
        else:
            logger.info('no active %r user agreements for user: %r', 
                        user_agreement_type, screensaver_user)
        logger.debug('user dsl: %r, %r, %r', 
            screensaver_user, user_agreement_type, current_dsl)
        return current_dsl
    
    def has_sm_data_deposited(self, screensaver_user):
        '''
        True if the user has qualifying data deposited
        - as defined in the User Agreement: level 1 or 2 data, depending on the 
        data_sharing_level for the user.
        '''
        current_dsl = self.get_user_data_sharing_level(
            screensaver_user, self.VOCAB_USER_AGREEMENT_SM)
        my_screens = self.get_user_screens(screensaver_user)
        for screen in my_screens:
            if hasattr(screen, 'screenresult'):
                if ( screen.screen_type == SCREEN_TYPE.SMALL_MOLECULE
                     and screen.data_sharing_level < DSL.PRIVATE ):
                    if screen.data_sharing_level == current_dsl:
                        logger.info(
                            'has_sm_data_deposited %r: True, dsl: %r', 
                            screensaver_user.screensaver_user_id, current_dsl)
                        return True
        if DEBUG_SCREEN_ACCESS:
            logger.info('has_sm_data_deposited %r: %r: %r', 
                screensaver_user.screensaver_user_id, current_dsl, False)
        return False
    
    def has_rna_data_deposited(self, screensaver_user):
        '''
        True if the user has qualifying data deposited
        - as defined in the User Agreement: level 1 or 2 data, depending on the 
        data_sharing_level for the user.
        '''
        current_dsl = self.get_user_data_sharing_level(
            screensaver_user, self.VOCAB_USER_AGREEMENT_RNAI)
        logger.info('current_dsl: %r, %r', screensaver_user, current_dsl)
        my_screens = self.get_user_screens(screensaver_user)
        for screen in my_screens:
            if hasattr(screen, 'screenresult'):
                if ( screen.screen_type == SCREEN_TYPE.RNAI
                     and screen.data_sharing_level < DSL.PRIVATE ):
                    if screen.data_sharing_level == current_dsl:
                        logger.info(
                            'has_rnai_data_deposited %r: True, dsl: %r', 
                            screensaver_user.screensaver_user_id, current_dsl)
                        return True
        if DEBUG_SCREEN_ACCESS:
            logger.info('has_rnai_data_deposited %r: %r: %r', 
                screensaver_user.screensaver_user_id, current_dsl, False)
        return False
    
    def get_read_authorized_screens(self, screensaver_user):
        '''
        Returns the screens that the user has permission to view general details
        for.
        - either unrestricted "read" access or restricted access as defined in
        the User Agreement rules for data visibility. 
        '''
        
        if DEBUG_SCREEN_ACCESS:
            logger.info('get_read_authorized_screens %r', screensaver_user)
        authorized_screens = set()
        user_screens = self.get_user_screens(screensaver_user)
        authorized_screens.update(user_screens)
        
        # 20180627: TODO: should allow access only to screen type for user?
        has_rnai = any([x.screen_type == SCREEN_TYPE.RNAI
                        for x in user_screens])
        has_sm = any([x.screen_type == SCREEN_TYPE.SMALL_MOLECULE
                        for x in user_screens])
        if DEBUG_SCREEN_ACCESS:
            logger.info('user: %r, has_rnai: %r, has_sm: %r', 
                        screensaver_user.username, has_rnai, has_sm)
        if has_sm:
            public_screens = Screen.objects.all()\
                .filter(data_sharing_level=DSL.SHARED)\
                .filter(screen_type=SCREEN_TYPE.SMALL_MOLECULE)
            authorized_screens.update(public_screens)
        if has_rnai:
            public_screens = Screen.objects.all()\
                .filter(data_sharing_level=DSL.SHARED)\
                .filter(screen_type=SCREEN_TYPE.RNAI)
            authorized_screens.update(public_screens)

        has_sm_data_deposited = self.has_sm_data_deposited(screensaver_user)
        has_rna_data_deposited = self.has_rna_data_deposited(screensaver_user)
        
        if has_sm_data_deposited:
            visible_screens = (
                Screen.objects.all().filter(screen_type=SCREEN_TYPE.SMALL_MOLECULE)
                    .filter(data_sharing_level__lt=DSL.PRIVATE))
            logger.info('update with visible small_molecule screens: %r', 
                        [x.facility_id for x in visible_screens])
            authorized_screens.update(visible_screens)
        if has_rna_data_deposited:
            visible_screens = (
                Screen.objects.all().filter(screen_type=SCREEN_TYPE.RNAI)
                    .filter(data_sharing_level__lt=DSL.PRIVATE))
            logger.info('update with visible rna screens: %r', 
                        [x.facility_id for x in visible_screens])
            authorized_screens.update(visible_screens)
        if DEBUG_SCREEN_ACCESS:
            logger.info(
                'user: %r, visible screens (dsl <3): %r', 
                screensaver_user.screensaver_user_id, 
                [x.facility_id for x in authorized_screens])
        return authorized_screens
    
    def has_screen_read_authorization(self, user, screen_facility_id):
        '''
        True if the user has permission to view general details for the resource;
        - User-Screen Access Level = 0, 1, 2, 3
        - either unrestricted "read" access or restricted access as defined in
        the User Agreement rules for data visibility. 
        '''
        logger.info('has_screen_read_authorization: %r, %r', user, screen_facility_id)

        is_restricted = self.is_restricted_view(user)
        if is_restricted is not True:
            return True
        
        screensaver_user = ScreensaverUser.objects.get(username=user.username)
        authorized_screens = self.get_read_authorized_screens(screensaver_user)
        result = screen_facility_id in set([screen.facility_id for screen in authorized_screens])
        logger.info('has_screen_read_authorization: %r, %r: %r', 
            user, screen_facility_id, result)
        return result
    
    def get_user_effective_data_sharing_level(self, screensaver_user, screen_type):
        effective_dsl = 0
        if screen_type==SCREEN_TYPE.RNAI \
            and self.has_rna_data_deposited(screensaver_user) is True:
            effective_dsl = self.get_user_data_sharing_level(
                screensaver_user, self.VOCAB_USER_AGREEMENT_RNAI)
        if screen_type==SCREEN_TYPE.SMALL_MOLECULE \
            and self.has_sm_data_deposited(screensaver_user) is True:
            effective_dsl =  self.get_user_data_sharing_level(
                screensaver_user, self.VOCAB_USER_AGREEMENT_SM)
        if DEBUG_SCREEN_ACCESS:
            logger.info('effective dsl: %r, %r, %r', screensaver_user, screen_type, effective_dsl)
        return effective_dsl
    
    def get_screen_access_level_table(self, username):
        '''
        Create a current user-screen access level table:
        - Fields: 
            'facility_id', 'data_sharing_level','screen_type',
            overlapping_positive_screens: 
                Calculated in two passess, filtered to only show screens that the 
                user has user_access_level_granted >= 2 from the unfiltered set
                of overlapping_positive_screens
            user_access_level_granted:
            Effective User-Screen Access Level - fields visible:
            
            ACCESS_LEVEL.LIMITED_ONLY 
            0 - Field level 0 only; no overlapping data
            ACCESS_LEVEL.OVERLAPPING_ONLY
            1 - (overlapping pos screens) Field level 0,1; no
                overlapping data (unless viewing from own
                screen results, not visible here)
            ACCESS_LEVEL.MUTUALLY_SHARED
            2 - (mutual shared screens) Field level 0,1,2, Screen Results, 
                Positives Summary; 
                overlapping_positive_screens: filtered to show users 
                user_access_level_granted 2,3 screens
            ACCESS_LEVEL.ALL
            3 - (own screens) Field level 0,1,2,3, 
                Screen Results, CPRs, Activities, Visits; 
                overlapping_positive_screens: filtered to show users 
                user_access_level_granted 2,3 screens
        '''
        
        screen_overlapping_table = self.get_screen_overlapping_table()
        screensaver_user = ScreensaverUser.objects.get(username=username)
        my_screen_facility_ids = set([
            screen.facility_id for screen 
                in self.get_user_screens(screensaver_user)])
        
        if DEBUG_SCREEN_ACCESS:
            logger.info('my screens: %r, %r', username, my_screen_facility_ids)

        user_effective_sm_dsl = self.get_user_effective_data_sharing_level(
            screensaver_user, SCREEN_TYPE.SMALL_MOLECULE)
        user_effective_rna_dsl = self.get_user_effective_data_sharing_level(
            screensaver_user, SCREEN_TYPE.RNAI)
        if DEBUG_SCREEN_ACCESS:
            logger.info('my dsls: %r, %r, %r', username, 
                user_effective_sm_dsl,user_effective_rna_dsl)
        
        my_qualified_sm_facility_ids = set([
            facility_id for facility_id in my_screen_facility_ids if
                screen_overlapping_table[facility_id]['data_sharing_level']
                    == user_effective_sm_dsl and
                screen_overlapping_table[facility_id]['screen_type'] == SCREEN_TYPE.SMALL_MOLECULE            
            ])
        my_qualified_rnai_facility_ids = set([
            facility_id for facility_id in my_screen_facility_ids if
                screen_overlapping_table[facility_id]['data_sharing_level']
                    == user_effective_rna_dsl and 
                screen_overlapping_table[facility_id]['screen_type'] == SCREEN_TYPE.RNAI            
            ])

        
        if DEBUG_SCREEN_ACCESS:
            logger.info(
                'user: %s, '
                'user_effective_rna_dsl: %r, ' 
                'user_effective_sm_dsl: %r',
                username, user_effective_rna_dsl, user_effective_sm_dsl)
            
        def effective_access_level_function(
                facility_id=None, data_sharing_level=None, 
                screen_type=None, overlapping_positive_screens=None ):

            if screen_type == SCREEN_TYPE.RNAI:
                user_effective_dsl = user_effective_rna_dsl
                my_qualified_facility_ids = my_qualified_rnai_facility_ids
            elif screen_type == SCREEN_TYPE.SMALL_MOLECULE:
                user_effective_dsl = user_effective_sm_dsl
                my_qualified_facility_ids = my_qualified_sm_facility_ids
            else:
                raise ProgrammingError('unknown screen_type: %r', screen_type)

            effective_access_level = None
            if facility_id in my_screen_facility_ids:
                effective_access_level = ACCESS_LEVEL.ALL # 3
            
            elif data_sharing_level == DSL.SHARED: # 0:
                effective_access_level = ACCESS_LEVEL.MUTUALLY_SHARED # 2
            
            elif data_sharing_level == DSL.PRIVATE: # 3:
                # Note: level 3 screens other than own should not appear
                effective_access_level = None
            
            elif user_effective_dsl == DSL.MUTUAL \
                    and data_sharing_level == DSL.MUTUAL:
                effective_access_level = ACCESS_LEVEL.MUTUALLY_SHARED #2
            
            elif user_effective_dsl in [DSL.MUTUAL, DSL.MUTUAL_POSITIVES] \
                    and data_sharing_level in [DSL.MUTUAL, DSL.MUTUAL_POSITIVES]:

                if  my_qualified_facility_ids & set(overlapping_positive_screens):
                    effective_access_level = ACCESS_LEVEL.OVERLAPPING_ONLY # 1
                else:
                    effective_access_level = ACCESS_LEVEL.LIMITED_ONLY # 0
                    
            return effective_access_level
        
        # pass one: calculate the user_access_level_granted
        for facility_id,_dict in screen_overlapping_table.items():
            _dict['user_access_level_granted'] = \
                effective_access_level_function(**_dict)
        
        screens_by_user_access_level = defaultdict(set)
        for facility_id, screen in screen_overlapping_table.items():
            ual = screen['user_access_level_granted']
            screens_by_user_access_level[ual].add(facility_id)
        
        if DEBUG_SCREEN_ACCESS: 
            logger.info('screens_by_user_access_level: %r ',
                screens_by_user_access_level)
            logger.info('screen access levels for "%s" %r',
                username, [(k,len(v)) for k,v 
                    in screens_by_user_access_level.items()])

        # pass two: calculate overlapping screens visible
        overlapping_screens_visible_2 = (
            screens_by_user_access_level[2]
            | screens_by_user_access_level[3])
        overlapping_screens_visible_3 = (
            screens_by_user_access_level[1]
            | screens_by_user_access_level[2]
            | screens_by_user_access_level[3])
        for facility_id,_dict in screen_overlapping_table.items():
            if _dict['user_access_level_granted'] == ACCESS_LEVEL.MUTUALLY_SHARED: #2:
                overlapping_positive_screens = \
                    set(_dict['overlapping_positive_screens'])
                overlapping_positive_screens &= overlapping_screens_visible_2
                _dict['overlapping_positive_screens'] = \
                    sorted(overlapping_positive_screens)
            elif _dict['user_access_level_granted'] == ACCESS_LEVEL.ALL: #3:
                overlapping_positive_screens = \
                    set(_dict['overlapping_positive_screens'])
                overlapping_positive_screens &= overlapping_screens_visible_3
                _dict['overlapping_positive_screens'] = \
                    sorted(overlapping_positive_screens)
            else:
                _dict['overlapping_positive_screens'] = None
        
        return screen_overlapping_table
    
    def filter(self, user, filter_expression):
        if self.is_restricted_view(user):
            if DEBUG_AUTHORIZATION:
                logger.info('create_authorized_screen_filter for %r', user)
            screensaver_user = ScreensaverUser.objects.get(username=user.username)
            authorized_screens = \
                self.get_read_authorized_screens(screensaver_user)
            # FIXME: should use screen_id, but it is not visible at the top level?
            auth_filter = column('facility_id').in_(
                [screen.facility_id for screen in authorized_screens])
            if filter_expression is not None:
                filter_expression = and_(filter_expression, auth_filter)
            else:
                filter_expression = auth_filter
 
        return filter_expression

    def filter_in_sql(self, user, stmt, screen_table):
        return stmt
        # if self.is_restricted_view(user):
        #     logger.info('create_authorized_screen_filter')
        #     screensaver_user = ScreensaverUser.objects.get(username=user.username)
        #     authorized_screens = \
        #         self.get_read_authorized_screens(screensaver_user)
        #     stmt = stmt.where(
        #         screen_table.c.screen_id.in_(
        #             [screen.screen_id for screen in authorized_screens]))
        # return stmt
    
    def get_row_property_generator(self, user, fields, extant_generator):
        '''
        Filter result properties based on authorization rules
        '''
        return self.get_access_level_property_generator(
           user, fields, extant_generator)
    
    def get_access_level_property_generator(self, user, fields, extant_generator):
        '''
        Override UserGroupAuthorization to filter based on User Agreement data
        sharing rules.
        '''
        
        is_restricted = self.is_restricted_view(user)
        if is_restricted is not True:
            return extant_generator
        else:
            logger.info('get_access_level_property_generator: %r for user: %r', 
                self.resource_name, user)
            
            screen_access_dict = self.get_screen_access_level_table(user.username)
            fields_by_level = self.get_fields_by_level(fields)
            logger.debug('fields by level: %r', fields_by_level)
            class Row:
                def __init__(self, row):
                    logger.debug(
                        'filter screen row: %r', 
                        [(key, row[key]) for key in row.keys()])
                    self.row = row
                    self.facility_id = facility_id = row['facility_id']
                    
                    effective_access_level = None
                    screen_data = screen_access_dict.get(facility_id, None)
                    if screen_data:
                        effective_access_level = \
                            screen_data.get('user_access_level_granted',None)
                    if DEBUG_SCREEN_ACCESS:
                        logger.info('screen: %r, effective_access_level: %r', 
                            facility_id, effective_access_level)
                    self.effective_access_level = effective_access_level
                    self.allowed_fields = set()
                    if effective_access_level is not None:
                        for level in range(0,effective_access_level+1):
                            self.allowed_fields.update(fields_by_level[level])
                            if DEBUG_SCREEN_ACCESS:
                                logger.info(
                                    'allow level: %r: %r', 
                                    level, fields_by_level[level])
                    else: 
                        logger.warn('user: %r effective_access_level is None, screen: %r',
                            user.username, facility_id)
                    if DEBUG_SCREEN_ACCESS:
                        logger.info('allowed fields: %r', self.allowed_fields)
                def has_key(self, key):
                    if key == 'user_access_level_granted':
                        return True
                    return self.row.has_key(key)
                def keys(self):
                    return self.row.keys()
                def __getitem__(self, key):
                    logger.debug(
                        'key: %r, allowed: %r', key, key in self.allowed_fields)
                    if key == 'user_access_level_granted':
                        return self.effective_access_level
                    if self.row[key] is None:
                        return None
                    else:
                        if key in self.allowed_fields:
                            if key == 'has_screen_result':
                                original_val = self.row[key]
                                val = original_val
                                if self.effective_access_level >= ACCESS_LEVEL.MUTUALLY_SHARED: #2:
                                    # Change not_shared to available for sharing users
                                    if val == SCREEN_AVAILABILITY.NOT_SHARED: #2: # "not shared"
                                        val = SCREEN_AVAILABILITY.AVAILABLE # 1 # "available"
                                else:
                                    if val == SCREEN_AVAILABILITY.AVAILABLE: #1: # available
                                        val = SCREEN_AVAILABILITY.NOT_SHARED # 2 # not shared
                                return val
                            elif key == 'overlapping_positive_screens':
                                if self.effective_access_level >= ACCESS_LEVEL.MUTUALLY_SHARED: # 2:
                                    reference_screen = \
                                        screen_access_dict[self.facility_id]
                                    return reference_screen['overlapping_positive_screens']
                                # restricted users may not view
                                return None
                            else:
                                logger.debug('allow %r: %r', key, self.row[key])
                                return self.row[key]
                        else:
                            logger.debug(
                                '%r filter field: %r for restricted user: %r',
                                self.facility_id, key, user.username)
                            return None

            def screen_property_generator(cursor):
                if extant_generator is not None:
                    cursor = extant_generator(cursor)
                for row in cursor:
                    yield Row(row)
            
            return screen_property_generator
        

class ScreenResultAuthorization(ScreenAuthorization):
    
    def _is_resource_authorized(
        self, user, permission_type, screen_facility_id=None, **kwargs):
        '''
        Override UserGroupAuthorization to determine if the user is allowed 
        to view the screen results (level 2 or 3 access)
        '''
        authorized = super(ScreenAuthorization, self)._is_resource_authorized(
            user, permission_type, **kwargs)
        if authorized is True:
            return True
        else:
            if not screen_facility_id:
                raise MissingParam('screen_facility_id')
            try:
                screen = Screen.objects.get(facility_id=screen_facility_id)

                screensaver_user = ScreensaverUser.objects.get(username=user.username)
                users_own_screens = self.get_user_screens(screensaver_user)
                user_effective_dsl = self.get_user_effective_data_sharing_level(
                    screensaver_user, screen.screen_type)
                can_view = False
                if screen in users_own_screens:
                    can_view = True
                elif user_effective_dsl in [0,1,2,3]:
                    if screen.data_sharing_level == DSL.SHARED: # 0:
                        can_view = True
                    elif screen.data_sharing_level == DSL.MUTUAL: # 1:
                        if  user_effective_dsl == DSL.MUTUAL: # 1:
                            can_view = True
                if DEBUG_SCREEN_ACCESS:
                    logger.info(
                        'user: %s, effective dsl: %r, screen: %r, type: %r, dsl: %r',
                        user.username, user_effective_dsl, screen_facility_id, 
                        screen.screen_type, screen.data_sharing_level)
                if can_view is not True:
                    raise PermissionDenied
                if not hasattr(screen, 'screenresult'):
                    raise Http404('No screen result for: %r'%screen_facility_id)
                return True
            
            except ObjectDoesNotExist:
                raise Http404
        return False

    def get_row_property_generator(self, user, fields, extant_generator):
        '''
        Filter result properties based on authorization rules
        '''
        return self.get_access_level_property_generator(
           user, fields, extant_generator)
    
    def get_access_level_property_generator(self, user, fields, extant_generator):
        
        is_restricted = self.is_restricted_view(user)
        if is_restricted is not True:
            return extant_generator
        else:
            
            access_level_1_fields = [ field['key'] 
                for field in fields.values() 
                    if field.get('user_access_level_granted') == 1]
            datacolumn_fields = [field for field in fields.values() 
                if field.get('is_datacolumn',False) is True ]
            access_level_1_cp = [field['key']
                for field in datacolumn_fields
                    if field['vocabulary_scope_ref'] 
                        == 'resultvalue.confirmed_positive_indicator']
            access_level_1_pp = [field['key']
                for field in datacolumn_fields
                    if field['vocabulary_scope_ref'] 
                        == 'resultvalue.partitioned_positive']
            access_level_1_boolean_positive = [field['key']
                for field in datacolumn_fields
                    if field['data_type'] == 'boolean']
            if DEBUG_AUTHORIZATION:
                logger.info('access level 1 confirmed positive: %r',
                    access_level_1_cp )
                logger.info('access level 1 partitioned positive: %r',
                    access_level_1_pp )
                logger.info('access level 1 boolean positive: %r',
                    access_level_1_boolean_positive )
            
            class Row:
                def __init__(self, row):
                    self.row = row
                    self.well_id = row['well_id']
                    self.is_positive = False
                    if row.has_key('is_positive'):
                        self.is_positive = row['is_positive']

                def has_key(self, key):
                    return self.row.has_key(key)
                def keys(self):
                    return self.row.keys()
                def __getitem__(self, key):
                    if self.row[key] is None:
                        return None
                    
                    elif key in access_level_1_fields:
                        value = self.row[key]
                        final_value = None
                        if self.is_positive is True:
                            if key in access_level_1_boolean_positive:
                                if value is True:
                                    final_value = value
                            elif key in access_level_1_cp:
                                if int(value) == 3:
                                    final_value = value
                            elif key in access_level_1_pp:
                                if int(value) != 0:
                                    final_value = value
                            else:
                                logger.error(
                                    'unknown level 1 result value field: %r:%r: %r',
                                    self.well_id, key, value)
                        logger.info('level 1: %r:%r val: %r, is_positive: %r, final: %r',
                            self.well_id, key, value, self.is_positive, final_value)
                        return final_value
                    else:
                        # NOTE: all users viewing screen results already have 
                        # granted access level 2 or 3 for this screen result
                        return self.row[key]

            def result_value_generator(cursor):
                if extant_generator is not None:
                    cursor = extant_generator(cursor)
                for row in cursor:
                    yield Row(row)
            
            return result_value_generator


class ScreenResultResource(DbApiResource):

    class Meta:
    
        queryset = ScreenResult.objects.all()  # .order_by('facility_id')
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'screenresult'
        authorization = ScreenResultAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = ScreenResultSerializer()
        object_class = dict
        max_limit = 10000
        
    def __init__(self, **kwargs):

        self.scope = 'fields.screenresult'
        super(ScreenResultResource, self).__init__(**kwargs)
        
        self.reagent_resource = None
        self.screen_resource = None
        self.datacolumn_resource = None
        
    def get_datacolumn_resource(self):
        if self.datacolumn_resource is None:
            self.datacolumn_resource = DataColumnResource()
        return self.datacolumn_resource

    def get_screen_resource(self):
        if not self.screen_resource:
            self.screen_resource = ScreenResource()
        return self.screen_resource
    
    @transaction.atomic()        
    def clear_cache(self, request, **kwargs):
        
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        all=bool(param_hash.get('all',False))
        by_date=param_hash.get('by_date',None)
        by_uri=param_hash.get('by_uri',None)
        by_size=bool(param_hash.get('by_size', False))
        
        logger.info(
            'clear_cache called: all: %r, by_date: %r, by_uri: %r, by_size: %r',
            all, by_date, by_uri, by_size)
        
        if all is True or any([by_date,by_size,by_uri]) is False:
            # Manually clear the screen caches
            DbApiResource.clear_cache(self, request, **kwargs)
            caches['screen_cache'].clear()

        max_indexes_to_cache = getattr(
            settings, 'MAX_WELL_INDEXES_TO_CACHE', 3e+08)
        logger.debug('max_indexes_to_cache %s' % max_indexes_to_cache)

        _wellQueryIndex = self.get_table_def('well_query_index')

        try:
            query = CachedQuery.objects.filter(uri__contains='/screenresult/')
            if query.exists():
                logger.debug(
                    'clear_cache: screenresult queries to consider %s',
                    [(x.id, x.uri) for x in query])
            ids = set()
            if by_size:
                query = query.order_by('-datetime')
                cumulative_count = 0

                for q in query:
                    if q.count:
                        cumulative_count += q.count
                    if cumulative_count >= max_indexes_to_cache:
                        last_id_to_save = q.id
                        logger.info(
                            'cumulative_count: %d, last_id_to_save: %d',
                            cumulative_count, last_id_to_save)
                        query = query.filter(id__lte=last_id_to_save)
                        ids.update([q.id for q in query])
                        break
                
            if by_date:  # TODO: test
                query = query.filter(datetime__lte=by_date)
                ids.update([q.id for q in query])
            if by_uri: 
                query = query.filter(uri__exact=by_uri)
                ids.update([q.id for q in query])
                
            if ids or all:
                logger.info('clear cachedQueries: ids: %r, all: %r', ids, all)
                if all:
                    stmt = delete(_wellQueryIndex)
                    get_engine().execute(stmt)
                    logger.info('cleared all cached wellQueryIndexes')
                    CachedQuery.objects.all().delete()
                    # # TODO: delete the well_data_column_positive_indexes
                    # # related to this uri only
                    # stmt = delete(_well_data_column_positive_index)
                    # conn.execute(stmt)
                    # logger.info(
                    #     'cleared all cached well_data_column_positive_indexes')
                    # TODO: delete the screen_overlap references to this URI only
                else:
                    stmt = delete(_wellQueryIndex).where(
                        _wellQueryIndex.c.query_id.in_(ids))
                    get_engine().execute(stmt)
                    logger.info('cleared cached wellQueryIndexes: %r', ids)
                    CachedQuery.objects.filter(id__in=ids).delete()

                    # stmt = delete(_well_data_column_positive_index)
                    # conn.execute(stmt)
                    # logger.info(
                    #     'cleared all cached well_data_column_positive_indexes')
            else:
                logger.info('no CachedQuery values to be cleared')

        except Exception, e:
            logger.exception('on screenresult clear cache')
            raise e  

        # NOTE: for now, clear the wdcpi when a new screen_result is loaded:
        # (when "all" or "by_uri" is set)
        # TODO: incrementally clear the wdcpi for each set of ids, or all
        if all is True or by_uri is not None:
            logger.info('all: %r, by_uri: %r', all, by_uri)
            get_engine().execute(delete(self.get_table_def('well_data_column_positive_index')))
            logger.info(
                'cleared all cached well_data_column_positive_indexes')
            get_engine().execute(delete(self.get_table_def('screen_overlap')))
            logger.info(
                'cleared all cached screen_overlap entries')
        else:
            logger.info('do not clear the well_data_column_positive_index table...')
        
        # Manually clear the screen caches - removed 201802
        # DbApiResource.clear_cache(self)
        # caches['screen'].clear()
        # self.get_screen_resource().clear_cache()
            
    def prepend_urls(self):

        return [
            url(r"^(?P<resource_name>%s)/"
                r"(?P<screen_facility_id>\w+)/" 
                r"(?P<well_id>\d{1,5}\:[a-zA-Z]{1,2}\d{1,2})%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)/"
                r"(?P<screen_facility_id>\w+)/"
                r"schema%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)/"
                r"(?P<screen_facility_id>\w+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_list'), name="api_dispatch_list"),
        ]

    @read_authorization
    def get_detail(self, request, **kwargs):

        facility_id = kwargs.get('screen_facility_id', None)
        if not facility_id:
            raise MissingParam('screen_facility_id')

        well_id = kwargs.get('well_id', None)
        if not well_id:
            raise MissingParam('well_id')

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):
        logger.info('get_list: %r', kwargs)
        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)
        
    def _build_result_value_column(self, field_information):
        '''
        Each result value will be added to the query as a subquery select:
        (SELECT <value_field> 
         FROM result_value
         WHERE result_value.data_column_id=<id> 
         AND rv.well_id=assay_well.well_id limit 1) as <data_column_value_alias>
        '''
        logger.debug('build result value for field: %r', field_information)
        key = field_information['key']
        data_column_id = field_information['data_column_id']
        data_column_type = field_information.get('data_type') 
        # TODO: column to select: use controlled vocabulary
        column_to_select = None
        if(data_column_type in ['numeric', 'decimal', 'integer']):  
            column_to_select = 'numeric_value'
        else:
            column_to_select = 'value'
        logger.debug('_build_result_value_column: %r, %r, %r, %r', 
            key, column_to_select, data_column_id, column_to_select)
        
        _rv = self.bridge['result_value']
        rv_select = select([column(column_to_select)]).select_from(_rv)
        rv_select = rv_select.where(
            _rv.c.data_column_id == field_information['data_column_id'])
        rv_select = rv_select.where(_rv.c.well_id == text('assay_well.well_id'))
        # FIXME: limit to rv's to 1 - due to the duplicated result value issue
        rv_select = rv_select.limit(1)  
        rv_select = rv_select.label(key)
        return rv_select

    # 20180220 - not used after rv query optimizations
    # def _build_result_value_column_for_base_select(self, field_information):
    #     '''
    #     join the result_value table as an alias
    #     '''
    #     logger.debug('build result value for field: %r', field_information)
    #     key = field_information['key']
    #     data_column_id = field_information['data_column_id']
    #     data_column_type = field_information.get('data_type') 
    #     # TODO: column to select: use controlled vocabulary
    #     column_to_select = None
    #     if(data_column_type in ['numeric', 'decimal', 'integer']):  
    #         column_to_select = 'numeric_value'
    #     else:
    #         column_to_select = 'value'
    #     logger.debug('_build_result_value_column: %r, %r, %r, %r', 
    #         key, column_to_select, data_column_id, column_to_select)
    #      
    #     _rv = self.bridge['result_value']
    #     _rv = _rv.alias('rv_')
    #      
    #      
    #     rv_select = select([column(column_to_select)]).select_from(_rv)
    #     rv_select = rv_select.where(
    #         _rv.c.data_column_id == field_information['data_column_id'])
    #     rv_select = rv_select.where(_rv.c.well_id == text('assay_well.well_id'))
    #     # FIXME: limit to rv's to 1 - due to the duplicated result value issue
    #     rv_select = rv_select.limit(1)  
    #     rv_select = rv_select.label(key)
    #     return rv_select

    def _build_result_value_cte(self, field_information):
        '''
        Not used - an alternate method of constructing the result values as 
        subqueries. Analyze indicates that this requires more memory and 
        scales poorly.
        '''
        _rv = self.bridge['result_value']
        field_name = field_information['key']
        data_column_type = field_information.get('data_type') 
        column_to_select = None
        if(data_column_type in ['numeric', 'decimal', 'integer']):  
            column_to_select = 'numeric_value'
        else:
            # FIXME: boolean result value must be cast to BOOLEAN for filtering sql
            column_to_select = 'value'
        
        rv_select = (
            select([_rv.c.well_id, column(column_to_select).label('value')])
            .select_from(_rv)
            .where(
            _rv.c.data_column_id == field_information['data_column_id']))
        rv_select = rv_select.cte(field_information['key'] + '_cte')
        return rv_select

    def create_exclusions_cte(self, screenresult):    
        _rv = self.bridge['result_value']
        _dc = self.bridge['data_column']
        # Create the exclusions subquery: well_id:colnames_excluded
        # Note: this is used to enable filtering and for the UI
        # However, for export types, is overriden by the is_excluded_gen
        # because the generator creates the full col name
        excl_join = _rv.join(_dc, _rv.c.data_column_id==_dc.c.data_column_id)
        excluded_cols_select = (
            select([
                _rv.c.well_id,
                func.array_to_string(
                    func.array_agg(func.lower(_dc.c.name)), 
                    LIST_DELIMITER_SQL_ARRAY).label('exclude')
                ])
            .select_from(excl_join)
            .where(_dc.c.screen_result_id == screenresult.screen_result_id)
            .where(_rv.c.is_exclude)
            .group_by(_rv.c.well_id)
            .order_by(_rv.c.well_id)
            )
        return excluded_cols_select.cte('exclusions')
    
    @transaction.atomic
    def create_cached_well_query(self, 
        base_stmt, param_hash, screen_facility_id, username):
        '''
        Get or create a CachedQuery and well_query_index entries
        
        Note: begin() in this context is not properly transactional -
        (sqlalchemy is not aware of Django connections or transactions).
        However, the transaction.atomic wrapper will properly roll back.
        '''
        MIN_WELLS_TO_CLEAR_INDEXES = getattr(
            settings, 'MIN_WELLS_TO_CLEAR_INDEXES', 3e5)
        
        _wellQueryIndex = self.get_table_def('well_query_index')
        
        # 1.a insert the base statement well ids into the indexing table
        m = hashlib.md5()
        compiled_stmt = str(base_stmt.compile(
            dialect=postgresql.dialect(),
            compile_kwargs={"literal_binds": True}))
        
        if DEBUG_SCREENRESULT or True: 
            logger.info('base_stmt: %r', compiled_stmt)
        
        m.update(compiled_stmt)
        key = m.hexdigest()
        if DEBUG_SCREENRESULT: 
            logger.info('cached query key: %r', key)

        (cachedQuery, create_new_well_index_cache) = \
            CachedQuery.objects.all().get_or_create(key=key)
        if DEBUG_SCREENRESULT: 
            logger.info('create_new_well_index_cache: %r', 
                create_new_well_index_cache)
        
        with get_engine().begin() as conn:
            
            logger.info('cached query: %r', cachedQuery)
            if not cachedQuery.count:
                logger.info('count not created, recreate the cache: %r', cachedQuery)
                create_new_well_index_cache = True
                stmt = delete(_wellQueryIndex)\
                    .where(_wellQueryIndex.c.query_id==cachedQuery.id)
                conn.execute(stmt)
            
            if create_new_well_index_cache:
                cachedQuery.sql = compiled_stmt
                cachedQuery.username = username
                cachedQuery.uri = \
                    '/screenresult/%s' % screen_facility_id
                cachedQuery.params = json.dumps(
                    { k:v for k,v in param_hash.items() if k!='schema'})
                cachedQuery.save()

                base_stmt = base_stmt.alias('base_stmt')
                
                insert_statement = insert(_wellQueryIndex).\
                    from_select(['well_id', 'query_id'],
                        select([
                            literal_column("well_id"),
                            literal_column(
                                str(cachedQuery.id)).label('query_id')
                            ]).select_from(base_stmt))
                if DEBUG_SCREENRESULT:
                    logger.info('insert stmt: %s',
                        str(insert_statement.compile(
                            dialect=postgresql.dialect(),
                            compile_kwargs={"literal_binds": True})))
                 
                logger.info('execute screenresult well index insert...')
                result = conn.execute(insert_statement)
                logger.info('screenresult well index complete: %r', result.rowcount)
                 
                cachedQuery.count = result.rowcount
                
                # cachedQuery.count = self.copy_from(base_stmt, cachedQuery.id)
                
                if cachedQuery.count == 0:
                    cachedQuery.delete()
                    logger.warn('Query generates no results: %r', compiled_stmt)
                else:
                    cachedQuery.save()

                # clear out older cached query wells
                self.clear_cache(None, by_size=True)
            else:
                logger.info('using cached well_query: %r', cachedQuery)
        logger.info('create_cached_well_query returns: %r', cachedQuery)
        return cachedQuery
        
    def copy_from(self, _stmt, query_id):
        ''' Use copy from to create the well query index
        - Note: performance testing indicates no benifit for this.'''
         
        count = 0
 
        with get_engine().begin() as conn:
         
            with SpooledTemporaryFile(max_size=MAX_SPOOLFILE_SIZE) as f:
                fieldnames = ['well_id','query_id']
                writer = unicodecsv.DictWriter(
                    f, fieldnames=fieldnames, delimiter=str(','),
                    lineterminator="\n")
                 
                final_stmt = \
                    select([
                        literal_column("well_id"),
                        literal_column(
                        str(query_id)).label('query_id')
                    ]).select_from(_stmt)
                logger.info('execute wqx query...')
                result = conn.execute(final_stmt)
                logger.info('executed %d, write temp file...', result.rowcount)
                for row in result:
                    wq_initializer = {
                        'well_id': row[0],
                        'query_id': row[1] }
                    writer.writerow(wq_initializer)
                    count += 1
                logger.info('wrote temp file: %d....', count)
                f.seek(0)
             
                logger.info(
                    'use copy_from to create %d well indexes...', count)
                get_engine().raw_connection().cursor().copy_from(
                    f, 'well_query_index', sep=str(','), 
                    columns=fieldnames, null=PSYCOPG_NULL)
                logger.info('well query indexes created.')
                 
        return count

    def get_query(
            self, username, screenresult, param_hash, schema, limit, offset):

        logger.info('build screenresult query')
        
        # Switch result_value query strategy for performance testing:
        # NOTE: 20180221 - orchestra testing shows no perf benefit from 
        # left outer join; however, when filtering, left outer join can yield 
        # significant benefit 
        RV_JOIN_TYPE_LEFT_OUTER = 'lo'
        RV_JOIN_TYPE_NESTED_SELECT = 'ns'
        RV_JOIN_TYPE = RV_JOIN_TYPE_NESTED_SELECT
        
        manual_field_includes = set(param_hash.get('includes', []))
        if screenresult.screen.study_type is None:
            manual_field_includes.add('assay_well_control_type')
            
        # general setup
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filename = self._get_filename(
            readable_filter_hash, schema, 
            **{'screen': screenresult.screen.facility_id})
                              
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params )
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
    
        # specific setup 
        
        _aw = self.bridge['assay_well']
        _w = self.bridge['well']
        _sr = self.bridge['screen_result']
        _s = self.bridge['screen']
        _rv = self.bridge['result_value']
        _dc = self.bridge['data_column']
        _reagent = self.bridge['reagent']
        _library = self.bridge['library']
        excluded_cols_select = self.create_exclusions_cte(screenresult)
        _wellQueryIndex = self.get_table_def('well_query_index')
                    
        # Strategy: 
        # 1. create the base clause, which will build a stored index in 
        # the table: well_query_index;
        # the base query uses only columns: well_id, +filters, +orderings
        base_custom_columns = {
            'well_id': literal_column('assay_well.well_id'),
            }

        # The base_fields are always included in the query: 
        # - always well_id
        # - any fields that are used in sort or filtering
        base_fields = [ fi for fi in field_hash.values() 
            if ( # anything filtered or ordered
                 fi['key'] == 'well_id'
                 or fi['key'] in order_params
                 or '-%s' % fi['key'] in order_params
                 or fi['key'] in filter_hash)]
        logger.info('base fields: %r', [
            (fi['key'], fi['scope']) for fi in base_fields])

        # If filtering on result_value columns, the left joins perform better
        if [fi for fi in base_fields 
                if fi.get('is_datacolumn', None)
                    and fi['key'] in filter_hash]:
            RV_JOIN_TYPE = RV_JOIN_TYPE_LEFT_OUTER
        
        base_clause = _aw
        base_query_tables = ['assay_well',]
        
        # Include join tables as needed for the base clause
        filter_excluded = True
        if ('exclude' not in filter_hash 
                and 'exclude' not in order_params
                and '-exclude' not in order_params):
            filter_excluded = False
        if filter_excluded is True:
            logger.info('filter excluded...')
            base_clause = base_clause.join(
                excluded_cols_select, 
                _aw.c.well_id==excluded_cols_select.c.well_id,isouter=True)
            base_custom_columns['exclude'] = literal_column('exclusions.exclude')
            base_query_tables.append('exclusions')
        
        if any([ ( fi.get('table',None) in ('well','reagent') 
                    or fi['scope'] in [
                        'fields.reagent','fields.smallmoleculereagent',
                        'fields.silencingreagent'] ) for fi in base_fields]):
            logger.info('include well tables')
            base_query_tables.extend(['well','reagent'])
            base_clause = base_clause.join(_w, _aw.c.well_id == _w.c.well_id)
            base_clause = base_clause.join(
                _reagent,_w.c.well_id==_reagent.c.well_id, isouter=True)
        if any([fi.get('table',None) 
                in ('screen','screen_result') for fi in base_fields]):
            logger.info('include screen tables')
            base_query_tables.extend(['screen','screen_result'])
            base_clause = base_clause.join(
                _sr, _aw.c.screen_result_id == _sr.c.screen_result_id)
            base_clause = base_clause.join(_s, _sr.c.screen_id == _s.c.screen_id)
        if any([fi.get('table',None) in ('library',) for fi in base_fields]):
            logger.info('include library table...')
            # Store the library table as a cte - the postgres query planner 
            # is not able to use the well-to-library fk to perform an
            # efficient index scan and instead uses a hash join
            
            # Slight performance increase using plate_number to join
            # base_query_tables.extend(['library_cte',])
            # library_cte = (
            #     select([text('well.well_id, library.*')])
            #     .select_from(
            #         _library.join(_w, _w.c.library_id==text('library.library_id'))
            #         .join(_aw, _w.c.well_id==_aw.c.well_id))
            #     .where(_aw.c.screen_result_id==screenresult.screen_result_id)).cte('library_cte')
            # base_clause = base_clause.join(
            #     library_cte,_aw.c.well_id==text('library_cte.well_id'))

            library_cte = (
                select([text('a.plate_number, library.*')])
                .select_from(_library)
                .select_from(
                    ( select([_w.c.plate_number, _w.c.library_id])
                        .select_from(
                            _w.join(_aw, _w.c.well_id==_aw.c.well_id))
                        .where(_aw.c.screen_result_id==screenresult.screen_result_id)
                        .distinct()
                        .alias('a')))
                .where(text('a.library_id')==_library.c.library_id)
                .cte('library_cte'))
            base_clause = base_clause.join(
                library_cte,_aw.c.plate_number==text('library_cte.plate_number'))
            
            for fi in base_fields:
                if fi.get('table') == 'library':
                    base_custom_columns[fi['key']] = \
                        literal_column('library_cte.%s' % fi['key'])
        
        # # Include Result Value tables
        if RV_JOIN_TYPE == RV_JOIN_TYPE_NESTED_SELECT:        
            # Using nested selects 
            for fi in [fi for fi in base_fields 
                    if fi.get('is_datacolumn', None)]:
                rv_select = self._build_result_value_column(fi)
                base_custom_columns[fi['key']] = rv_select
        elif RV_JOIN_TYPE == RV_JOIN_TYPE_LEFT_OUTER:
            # 20180215
            # For using rv_joins - all rows must be represented      
            for fi in [fi for fi in base_fields 
                    if fi.get('is_datacolumn', None)]:
                key = fi['key']
                result_value_alias = _rv.alias('rv_%s' % key)
                data_column_id = fi['data_column_id']
                data_column_type = fi.get('data_type') 
                if data_column_type in ['numeric', 'decimal', 'integer']:
                    base_custom_columns[key] = \
                        literal_column('rv_%s.%s' % (key, 'numeric_value') )
                elif data_column_type == 'boolean':
                    base_custom_columns[key] = \
                        literal_column('CAST( rv_%s.%s as BOOLEAN)' 
                            % (key, 'value'))
                else:
                    base_custom_columns[key] = \
                        literal_column('rv_%s.%s' % (key, 'value') )
                        
                base_clause = base_clause.join(result_value_alias,
                    and_(_aw.c.well_id==result_value_alias.c.well_id,
                         result_value_alias.c.data_column_id==data_column_id),
                    isouter=True)
        else: 
            raise ProgrammingError('unknown join type: %r', RV_JOIN_TYPE)
        # USING CTEs (shown to be slower)           
        # for fi in [fi for fi in base_fields 
        #         if fi.get('is_datacolumn', None)]:
        #     rv_select = self._build_result_value_cte(fi)
        #     base_clause = base_clause.join(
        #         rv_select, _aw.c.well_id==rv_select.c.well_id)
        #     rv_selector = (
        #         select([rv_select.c.value])
        #         .select_from(rv_select)
        #         .where(rv_select.c.well_id==text('assay_well.well_id'))
        #         )
        #     rv_selector.label(fi['key'])
        #     base_custom_columns[fi['key']] = rv_selector            

        
        base_columns = self.build_sqlalchemy_columns(
            base_fields, base_query_tables=base_query_tables,
            custom_columns=base_custom_columns) 
        logger.debug('base columns: %r', base_columns)
        # remove is_positive if not needed, to help the query planner
        if ('is_positive' not in filter_hash 
                and 'is_positive' in base_columns
                and 'is_positive' not in order_params
                and '-is_positive' not in order_params):
            del base_columns['is_positive']
            
        # Get the reagent column definitions - only if in the base_fields
        reagent_resource = self.get_reagent_resource(screenresult.screen.screen_type)
        base_reagent_tables = ['reagent','library','well']
        sub_columns = reagent_resource.build_sqlalchemy_columns(
            base_fields, base_query_tables=base_reagent_tables)
        for key,col in sub_columns.items():
            if key not in base_columns:
                if DEBUG_SCREENRESULT: 
                    logger.info('adding reagent column: %r...', key)
                base_columns[key] = col

        if not base_columns:
            raise ProgrammingError(
                'no base columns found in sub_columns: %r', sub_columns)
        base_stmt = select(base_columns.values()).select_from(base_clause)
        base_stmt = base_stmt.where(
            _aw.c.screen_result_id == screenresult.screen_result_id)

        (base_stmt, count_stmt) = \
            self.wrap_statement(base_stmt, order_clauses, filter_expression)
        base_stmt = base_stmt.order_by(asc(column('well_id')))  
        
        cachedQuery = self.create_cached_well_query(
            base_stmt, param_hash, screenresult.screen.facility_id, username)            

        # Use the cached well_query_index table to build efficient output query
        logger.info('build screenresult output query...')
        # specific setup 
        base_query_tables = ['assay_well', 'screen']
        # force query to use well_query_index.well_id
        custom_columns = { 
            'well_id': 
                literal_column('well_query_index.well_id'),
            'exclude': literal_column('exclusions.exclude')
        }
        # Use the well_query_index well_ids as the central subquery loop
        _wqx = select([column('id'), column('well_id')]).select_from(_wellQueryIndex)
        _wqx = _wqx.where(_wellQueryIndex.c.query_id == cachedQuery.id)
        _wqx = _wqx.order_by(_wellQueryIndex.c.id)
        if limit > 0:    
            _wqx = _wqx.limit(limit)
        _wqx = _wqx.offset(offset)
        _wqx = _wqx.cte('wqx')
        # Join to well table first to take advantage of well_id foreign key
        # between well and assay_well
        j = join(_wqx, _w, _wqx.c.well_id == _w.c.well_id)
        j = j.join(_aw, _w.c.well_id == _aw.c.well_id )
        j = j.join(_sr, _aw.c.screen_result_id == _sr.c.screen_result_id)
        j = j.join(_s, _sr.c.screen_id == _s.c.screen_id)
        
        # JOIN THE REAGENT COLUMNS
        j = j.join(_reagent,_w.c.well_id==_reagent.c.well_id, isouter=True)
        j = j.join(_library,_w.c.library_id==_library.c.library_id)

        if 'exclude' in field_hash:
            # FIXME: 20171218: create an index for the exclude col on result_value
            # - ultimately needed is a refactor of the result value table:
            # - 1. divide into sm and rnai tables,
            # - 2. refactor (using assay_well?) to index each row in results 
            # (or create an assay_row table)
            j = j.join(excluded_cols_select, 
                excluded_cols_select.c.well_id == _aw.c.well_id, isouter=True)
        if RV_JOIN_TYPE == RV_JOIN_TYPE_NESTED_SELECT:        
            # Using nested selects
            for fi in [
                fi for fi in field_hash.values() 
                    if fi.get('is_datacolumn', None)]:
                logger.debug('building rv column: %r', fi['key'])
                custom_columns[fi['key']] = self._build_result_value_column(fi)
        elif RV_JOIN_TYPE == RV_JOIN_TYPE_LEFT_OUTER:
            # 20180215
            # Using joins - left join rqd      
            for fi in [fi for fi in field_hash.values() 
                    if fi.get('is_datacolumn', None)]:
                key = fi['key']
                result_value_alias = _rv.alias('rv_%s' % key)
                data_column_id = fi['data_column_id']
                data_column_type = fi['data_type'] 
                if data_column_type in ['numeric', 'decimal', 'integer']:
                    custom_columns[key] = \
                        literal_column('rv_%s.%s' % (key, 'numeric_value') )
                elif data_column_type == 'boolean':
                    custom_columns[key] = \
                        literal_column('CAST( rv_%s.%s as BOOLEAN)' 
                            % (key, 'value'))
                else:
                    custom_columns[key] = \
                        literal_column('rv_%s.%s' % (key, 'value') )
                j = j.join(result_value_alias,
                    and_(_aw.c.well_id==result_value_alias.c.well_id,
                         result_value_alias.c.data_column_id==data_column_id),
                    isouter=True)
        else: 
            raise ProgrammingError('unknown join type: %r', RV_JOIN_TYPE)
        
        
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)

        # Get the reagent column definitions
        base_reagent_tables = ['reagent','library','well']
        sub_columns = reagent_resource.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_reagent_tables)
        for key,col in sub_columns.items():
            # 20181013 update so that subclass definition overrides
            # if key not in columns:
            columns[key] = col

        # Force the query to use well_query_index.well_id
        columns['well_id'] = literal_column('wqx.well_id').label('well_id')
        if DEBUG_SCREENRESULT: 
            logger.info('columns: %r', columns.keys())
        
        stmt = select(columns.values()).select_from(j)
        stmt = stmt.where(
            _aw.c.screen_result_id == screenresult.screen_result_id)
        
        stmt = stmt.order_by(_wqx.c.id)

        if DEBUG_SCREENRESULT: 
            logger.info(
                'stmt: %s',
                str(stmt.compile(
                    dialect=postgresql.dialect(),
                    compile_kwargs={"literal_binds": True})))
        logger.info('screenresult query built')
        return (field_hash, columns, stmt, count_stmt, cachedQuery, filename)
    

    def build_list_response(self, request, **kwargs):

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        use_raw_lists = request.GET.get(HTTP_PARAM_RAW_LISTS, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
            use_raw_lists = True
        
        manual_field_includes = set(param_hash.get('includes', []))
        manual_field_includes.add('screen_facility_id')
        limit = param_hash.get('limit', 0)        
        try:
            limit = int(limit)
        except ValueError:
            raise BadRequestError({
                'limit': 'Please provide a positive integer: %r' % limit})
        param_hash['limit'] = limit
        
        offset = param_hash.get('offset', 0)
        try:
            offset = int(offset)
        except ValueError:
            raise BadRequestError({
                'offset': 'Please provide a positive integer: %r' % offset })
        if offset < 0:    
            offset = -offset
        param_hash['offset'] = offset
        
        is_for_detail = kwargs.pop('is_for_detail', False)

        screen_facility_id = param_hash.pop('screen_facility_id', None)
        if not screen_facility_id:
            raise MissingParam('screen_facility_id')
        screenresult = ScreenResult.objects.get(
            screen__facility_id=screen_facility_id)              
            
        show_mutual_positives = bool(param_hash.get('show_mutual_positives', False))
        extra_dc_ids = param_hash.get(API_PARAM_DC_IDS, None)
            
        logger.info('build screen_result schema...')
        schema = self.build_schema(
            screenresult=screenresult,
            show_mutual_positives=show_mutual_positives,
            user=request.user,
            extra_dc_ids=extra_dc_ids)
        logger.info('build screen_result schema - done')

        content_type = self.get_serializer().get_accept_content_type(
            request, format=kwargs.get('format', None))
        
        if content_type == SDF_MIMETYPE:
            manual_field_includes.add('molfile')
        else:
            manual_field_includes.discard('molfile')
        param_hash['includes'] = list(manual_field_includes)

        logger.info('build screenresult query...')
        (field_hash, columns, stmt, count_stmt, cachedQuery,filename) = \
            self.get_query(
                request.user.username,
                screenresult,
                param_hash,
                schema,
                limit,
                offset)
        # Custom screen result serialization
        
        def is_excluded_gen(rows):
            '''
            Collate all of the excluded columns by well_id.
            - This generator creates the full column names; allowing the 
            "exclusions" query to be simpler.
            '''
            excluded_well_to_datacolumn_map = {}
            for key,field in schema['fields'].items():
                if ( field.get('is_datacolumn', False) 
                    and field['scope'] 
                        == 'datacolumn.screenresult-%s' % screen_facility_id):
                    
                    for well_id in (
                        ResultValue.objects.filter(
                                data_column_id=field['data_column_id'])
                            .filter(is_exclude=True)
                            .values_list('well_id', flat=True)):
                        excluded_cols = excluded_well_to_datacolumn_map.get(well_id,[])
                        excluded_cols.append(key)
                        excluded_well_to_datacolumn_map[well_id] = excluded_cols
            for row in rows:
                well_id = row['well_id']
                if well_id in excluded_well_to_datacolumn_map:
                    excluded_cols = excluded_well_to_datacolumn_map[well_id]
                    row['exclude'] = sorted(excluded_cols)
                else:
                    row['exclude'] = None
                yield row                  
        # Perform custom serialization because each output format will be 
        # different.
        list_brackets = LIST_BRACKETS
        if use_raw_lists:
            list_brackets = None
        image_keys = [key for key,field in field_hash.items()
            if field.get('display_type', None) == 'image']
        ordered_keys = sorted(field_hash.keys(), 
            key=lambda x: field_hash[x].get('ordinal',key))
        list_fields = [ key for (key,field) in field_hash.items() 
            if( field.get('json_field_type',None) == 'fields.ListField' 
                or field.get('linked_field_type',None) == 'fields.ListField'
                or field.get('data_type', None) == 'list' ) ]
        value_templates = {key:field['value_template'] 
            for key,field in field_hash.items() 
                if field.get('value_template', None)}

        title_function = None
        if use_titles is True:
            extra_titles = {
                'plate': 'Plate Number',
                'well': 'Well Name',
                'type': 'Assay Well Control Type',
                'exclude': 'Excluded'}
            def title_function(key):
                if key in field_hash:
                    return field_hash[key]['title']
                elif key in extra_titles:
                    return extra_titles[key]
                else:
                    return key
        rowproxy_generator = None

        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)
        
        if ( use_vocab or content_type in [XLS_MIMETYPE,XLSX_MIMETYPE]):
            # NOTE: xls export uses vocab values
            logger.info('use vocab generator...')
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
        else:
            logger.info('do not use vocabularies')

        # Custom serializer for screen results
        with get_engine().connect() as conn:
            if DEBUG_SCREENRESULT or logger.isEnabledFor(logging.DEBUG):
                logger.info(
                    'excute stmt %r...',
                    str(stmt.compile(
                        dialect=postgresql.dialect(),
                        compile_kwargs={"literal_binds": True})))
            logger.info('execute screenresult query...')
            result = conn.execute(stmt)
            logger.info('serialize screenresult response...')
            if rowproxy_generator:
                result = rowproxy_generator(result)

            data = cursor_generator(
                result,ordered_keys,list_fields=list_fields,
                value_templates=value_templates)
            data = closing_iterator_wrapper(data, conn.close)
            logger.info('content_type: %r', content_type)
            if content_type == JSON_MIMETYPE:
                meta = {
                    'limit': limit,
                    'offset': offset,
                    'screen_facility_id': screen_facility_id,
                    'total_count': cachedQuery.count,
                }    
                # Note: is_excluded generator not needed for JSON - already
                # part of the query, and no need to convert to col letters
                data = image_generator(data, image_keys, request) 
                response = StreamingHttpResponse(
                    ChunkIterWrapper(
                        json_generator(data, meta, is_for_detail=is_for_detail)))
                response['Content-Type'] = content_type
                return response
            elif(content_type == XLS_MIMETYPE or
                content_type == XLSX_MIMETYPE): 
                data = is_excluded_gen(data)
                response = get_xls_response(
                    screen_result_importer.create_output_data(
                        screen_facility_id, field_hash, data),
                    filename, request=request, 
                    title_function=title_function, image_keys=image_keys,
                    list_brackets=list_brackets)
                return response
            elif content_type == SDF_MIMETYPE:
                data = image_generator(
                    is_excluded_gen(data),image_keys, request)
                response = StreamingHttpResponse(
                    ChunkIterWrapper(
                        sdf_generator(data, title_function=title_function)),
                    content_type=content_type)
                response['Content-Disposition'] = \
                    'attachment; filename=%s.sdf' % filename
                return response
            elif content_type == CSV_MIMETYPE:
                data = image_generator(
                    is_excluded_gen(data),image_keys, request)
                response = StreamingHttpResponse(
                    ChunkIterWrapper(
                        csv_generator(
                            data,title_function=title_function, 
                            list_brackets=list_brackets)),
                    content_type=content_type)
                response['Content-Disposition'] = \
                    'attachment; filename=%s.csv' % filename
                return response
            else: 
                raise BadRequestError({
                    'content_type': 'not implemented: %r' % content_type })
    
    def get_mutual_positives_columns(self, screen_result_id):
        logger.info('get_mutual_positives_columns...')
        cache_key = '%s_mutual_positive_columns' % screen_result_id
        cached_ids = self.get_cache().get(cache_key)
        
        if not cached_ids:
        
            # Note: cache is cleared when any screen_results referenced by 
            # a datacolumn are re-loaded
            # 
            # SS1 Methodology:
            # select distinct(dc.data_column_id) 
            # from assay_well aw0
            # cross join assay_well aw1
            # inner join screen_result sr on aw1.screen_result_id=sr.screen_result_id
            # inner join data_column dc on sr.screen_result_id=dc.screen_result_id
            # where aw0.well_id=aw1.well_id
            # and aw0.is_positive
            # and aw1.is_positive
            # and ( dc.data_type = 'boolean_positive_indicator' 
            #       or dc.data_type = 'partition_positive_indicator' )
            # and aw0.screen_result_id = 941
            # and aw1.screen_result_id <> 941;
                    
            _aw = self.bridge['assay_well']
            _sr = self.bridge['screen_result']
            _dc = self.bridge['data_column']
            
            # Query to find mutual positive data columns:
            
            # OLD METHOD: USES EXISTS
            # - THIS HANGS POSTGRES (8.4 & 9.2) 
            # SELECT DISTINCT wdc.data_column_id 
            # FROM well_data_column_positive_index AS wdc 
            # JOIN data_column ON wdc.data_column_id = data_column.data_column_id 
            # WHERE data_column.screen_result_id != 1090 
            # AND (EXISTS (
            #     SELECT null FROM well_data_column_positive_index AS wdc1 
            #     JOIN data_column AS dc1 ON wdc1.data_column_id = dc1.data_column_id 
            #     WHERE wdc.well_id = wdc1.well_id AND dc1.screen_result_id = 1090));
            # 
            # NEW METHOD: USES TEMPORARY TABLE
            # with wdc1 as (
            # SELECT distinct(well_id) 
            #     FROM well_data_column_positive_index AS wdc1 
            #     JOIN data_column AS dc1 ON wdc1.data_column_id = dc1.data_column_id 
            #     WHERE dc1.screen_result_id = 1090
            # order by well_id
            # ) 
            # SELECT DISTINCT wdc.data_column_id 
            # FROM well_data_column_positive_index AS wdc 
            # JOIN data_column ON wdc.data_column_id = data_column.data_column_id
            # join wdc1 on wdc1.well_id=wdc.well_id 
            # WHERE data_column.screen_result_id != 1090;
    
            
            _aw = self.bridge['assay_well']
            _sr = self.bridge['screen_result']
            _dc = self.bridge['data_column']
            
            _wdc = self.get_create_well_data_column_positive_index().alias('wdc')
            _wdc1 = self.get_create_well_data_column_positive_index().alias('wdc1')
            
            _dc1 = _dc.alias('dc1')
            
            j2 = _wdc1.join(_dc1, _wdc1.c.data_column_id == _dc1.c.data_column_id)
            stmt2 = select([distinct(_wdc1.c.well_id).label('well_id')]).select_from(j2)
            stmt2 = stmt2.where(_dc1.c.screen_result_id == screen_result_id)
            stmt2.order_by('well_id')
            stmt2 = stmt2.cte('wdc1')
    
            j = _wdc.join(_dc, _wdc.c.data_column_id == _dc.c.data_column_id)
            j = j.join(stmt2, stmt2.c.well_id == _wdc.c.well_id)
            stmt = select([distinct(_wdc.c.data_column_id)]).select_from(j)
            stmt = stmt.where(_dc.c.screen_result_id != screen_result_id)
    
            if logger.isEnabledFor(logging.DEBUG):        
                sql = str(stmt.compile(
                    dialect=postgresql.dialect(),
                    compile_kwargs={"literal_binds": True}))
                logger.info('mutual positives statement: %r',sql)
            logger.info('execute mutual positives column query...')
            with get_engine().connect() as conn:
                result = conn.execute(stmt)
                cached_ids =  [x[0] for x in result ]
                logger.info('done, cols %r', cached_ids)
                self.get_cache().set(cache_key, cached_ids)
        else:
            logger.info('using cached mutual positive columns')
        return cached_ids

    def get_schema(self, request, **kwargs):

        if not 'screen_facility_id' in kwargs:
            raise Http404(
                'The screenresult schema requires a screen facility ID'
                ' in the URI, as in /screenresult/[facility_id]/schema/')
        facility_id = kwargs.pop('screen_facility_id')
        logger.info('get schema: %r', kwargs)
        if not self._meta.authorization._is_resource_authorized(
            request.user, 'read', screen_facility_id=facility_id):
            logger.info('Permission Denied: ScreenResult.schema, %r, %r',
                request.user, facility_id)
            raise PermissionDenied

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        show_all_other_screens = param_hash.get('show_all_other_screens', False)
        show_mutual_positives = bool(param_hash.get('show_mutual_positives', False))
        extra_dc_ids = param_hash.get(API_PARAM_DC_IDS, None)
        try:
            screenresult = ScreenResult.objects.get(
                screen__facility_id=facility_id)
            return self.build_response(
                request, self.build_schema(
                    screenresult, 
                    show_mutual_positives=show_mutual_positives,
                    user=request.user, extra_dc_ids=extra_dc_ids), 
                **kwargs)
        except ObjectDoesNotExist, e:
            raise Http404(
                'no screen result found for facility id: %r' % facility_id)

    def build_schema(
            self, screenresult=None, show_mutual_positives=False,
            user=None, extra_dc_ids=None, **kwargs):
        '''
        '''

        if user is None:
            raise NotImplementedError(
                'User must be provided for ScreenResult schema view')
        if screenresult is None:
            return super(ScreenResultResource, self).build_schema(user=user, **kwargs)
            
        screen_facility_id = screenresult.screen.facility_id
        
        cache_key = 'screenresult_schema_%s_%s_mutual_pos_%s' \
            % (user.username, screen_facility_id, show_mutual_positives)
        if extra_dc_ids:
            cache_key += '_dcs_.'  + '_'.join(extra_dc_ids)
        logger.info('build screenresult schema: %s', cache_key)
        data = self.get_cache().get(cache_key)
        
        def add_well_fields(current_fields):
            well_schema = \
                self.get_reagent_resource(screenresult.screen.screen_type)\
                .build_schema(user)
            newfields = {}
            newfields.update(well_schema['fields'])
            for key,field in newfields.items():
                if not set(VOCAB.field.visibility.hidden_fields) & set(field['visibility']):
                    field['visibility'] = []
            newfields.update(current_fields)
            
            if screenresult.screen.screen_type == SCREEN_TYPE.SMALL_MOLECULE:
                if 'compound_name' in newfields:
                    newfields['compound_name']['visibility'] = ['l','d']
                    newfields['compound_name']['ordinal'] = 12
            elif screenresult.screen.screen_type == SCREEN_TYPE.RNAI:
                if 'facility_entrezgene_id' in newfields:
                    newfields['facility_entrezgene_id']['visibility'] = ['l','d']
                    newfields['facility_entrezgene_id']['ordinal'] = 10
                if 'facility_entrezgene_symbols' in newfields:
                    newfields['facility_entrezgene_symbols']['visibility'] = ['l','d']
                    newfields['facility_entrezgene_symbols']['ordinal'] = 11
            return newfields
            
            
        if data:
            logger.info('cached: %s', cache_key)
        else:
            logger.info('not cached: %s', cache_key)
            data = super(ScreenResultResource, self).build_schema(user=user, **kwargs)
            logger.info('screen_result schema built: %s', cache_key)
            
            newfields = add_well_fields(data['fields'])
            if screenresult.screen.study_type is not None:
                del newfields['assay_well_control_type']
                
            max_ordinal = 0
            for fi in newfields.values():
                if fi.get('ordinal', 0) > max_ordinal:
                    max_ordinal = fi['ordinal']
            
            logger.info('map datacolumn definitions into field information definitions...')
            
            datacolumns = self.get_datacolumn_resource()\
                ._get_list_response_internal(
                    user, includes='*', screen_facility_id=screen_facility_id)
            
            datacolumn_fields = {}
            for dc in datacolumns:
                dc['visibility'] = ['l','d']
                dc['is_datacolumn'] = True
                # NOTE: if user may view screenresults, access level > 1
                # - filtering and ordering are allowed
                dc['filtering'] = True
                dc['ordering'] = True
                datacolumn_fields[dc['key']] = dc
            max_ordinal += len(datacolumn_fields)
            
            newfields.update(datacolumn_fields)
            
            if show_mutual_positives is True or extra_dc_ids is not None:
                # TODO: only need "overlapping_positive_screens" for the current screen
                # (should be handled by datacolumn_resoure.authorization anyway?)
                visible_screens = \
                    self.get_screen_resource()._get_list_response_internal(
                        user=user,
                        includes=[
                            'user_access_level_granted','overlapping_positive_screens',
                            'data_sharing_level','screen_type'])
                visible_screens = { screen['facility_id']:screen 
                    for screen in visible_screens }
                reported_screen = visible_screens[screen_facility_id]
                
                reference_datacolumns = self.get_datacolumn_resource()\
                    ._get_list_response_internal(
                        includes='*',
                        screen_type=reported_screen['screen_type'])
                reference_datacolumns = { dc['data_column_id']:dc 
                    for dc in reference_datacolumns }

                other_datacolumns = []
                if show_mutual_positives:
                    overlapping_screens = reported_screen['overlapping_positive_screens']
                    
                    # TODO: parameterize: Add the reagent count studies
                    if screenresult.screen.screen_type == 'small_molecule':
                        overlapping_screens.append('200001')
                    elif screenresult.screen.screen_type == 'rnai':
                        overlapping_screens.append('200002')
                        
                    temp_datacolumns = self.get_datacolumn_resource()\
                        ._get_list_response_internal(
                            user, includes='*',
                            screen_facility_id__in=overlapping_screens)
                    for dc in temp_datacolumns:
                        reference_dc = reference_datacolumns[dc['data_column_id']]
                        if reference_dc['positives_count'] > 0 \
                            or reference_dc['screen_facility_id'] in ('200001','200002'):
                            # TODO: parameterize: Add the reagent count studies
                            other_datacolumns.append(dc)
                if extra_dc_ids:
                    extra_datacolumns = self.get_datacolumn_resource()\
                        ._get_list_response_internal(
                            user, includes='*',
                            data_column_id__in=extra_dc_ids)                            

                    if self._meta.authorization.is_restricted_view(user):
                        overlapping = reported_screen['overlapping_positive_screens']
                        for dc in extra_datacolumns:
                            if dc['user_access_level_granted'] \
                                    == ACCESS_LEVEL.OVERLAPPING_ONLY: #1:
                                if reported_screen['user_access_level_granted'] \
                                        < ACCESS_LEVEL.ALL: #3:
                                    continue
                                elif (dc['screen_facility_id'] 
                                    not in overlapping):
                                    continue
                                else:
                                    logger.info('allowed level 1 col'
                                        '%r: %r: %r', 
                                        dc['key'],dc['title'],dc['data_column_id'])
                                    other_datacolumns.append(dc)    
                            else:
                                other_datacolumns.append(dc)
                    else:
                        other_datacolumns.extend(extra_datacolumns)
                
                other_datacolumns = { dc['data_column_id']:dc 
                    for dc in other_datacolumns }
                decorated = [
                    (dc['study_type'] != None,
                     dc['screen_facility_id'],dc['ordinal'], dc) 
                        for dc in other_datacolumns.values()]
                decorated.sort(key=itemgetter(0,1,2))
                other_datacolumns = [dc for sort_study,fid,ordinal,dc in decorated]
                                            
                datacolumn_fields = {}
                for i, dc in enumerate(other_datacolumns):
                    dc['visibility'] = ['l','d']
                    dc['is_datacolumn'] = True
                    dc['ordinal'] = max_ordinal + i
                    if dc['user_access_level_granted'] \
                            > ACCESS_LEVEL.OVERLAPPING_ONLY: #1:
                        dc['ordering'] = True
                        dc['filtering'] = True
                    datacolumn_fields[dc['key']] = dc
                    
                max_ordinal += len(datacolumn_fields)
                
                newfields.update(datacolumn_fields)
                
                
            data['fields'] = newfields
                
            logger.info('build screenresult schema done')
            self.get_cache().set(cache_key, data)
            
        return data

    def create_otherscreen_field(self,screen):
        columnName = "screen_%s" % screen.facility_id
        _dict = {}
        _dict['title'] = '%s (%s)' % (screen.facility_id, screen.title) 
        _dict['description'] =  _dict['title']
        _dict['is_screen_column'] = True
        _dict['key'] = columnName
        _dict['screen_facility_id'] = screen.facility_id
        _dict['visibility'] = ['']
        return (columnName, _dict)
        
    @write_authorization
    def put_list(self, request, **kwargs):
        return self.patch_detail(request, **kwargs)
    
    @write_authorization
    def post_list(self, request, **kwargs):
        return self.patch_detail(request, **kwargs)
    
    @write_authorization
    def post_detail(self, request, **kwargs):
        return self.patch_detail(request, **kwargs)
    
    @write_authorization
    def patch_list(self, request, **kwargs):
        return self.patch_detail(request, **kwargs)

    @write_authorization
    @un_cache        
    def delete_list(self, request, **kwargs):
        return self.delete_detail(request, **kwargs)
    
    @write_authorization
    @un_cache
    @transaction.atomic
    def delete_detail(self, request, **kwargs):
        if 'screen_facility_id' not in kwargs:
            raise BadRequestError(key='screen_facility_id', msg = 'required')
        screen_facility_id = kwargs['screen_facility_id']
        # Clear cache: note "all" because mutual positive columns can change
        # self.clear_cache(by_uri='/screenresult/%s' % screen_facility_id)
        self.clear_cache(None, all=True)
        screen = Screen.objects.get(facility_id=screen_facility_id)

        logger.info('delete screen result for %r', screen_facility_id)
        if hasattr(screen, 'screenresult'):
            screen_result = screen.screenresult
            logger.info('screen result: %r exists, deleting extant data',
                screen_result)
            screen_result.datacolumn_set.all().delete()
            screen_result.assaywell_set.all().delete()
            screen_result.screen.assayplate_set\
                .filter(library_screening__isnull=True).delete()
            screen.screenresult.delete()
            logger.info('screen_result deleted')
            screen_log = self.make_log(request, **kwargs)
            screen_log.ref_resource_name = 'screen'
            screen_log.key = screen.facility_id
            screen_log.uri = '/'.join(['screen', screen_log.key])
            screen_log.save()
        else:
            raise BadRequestError( 
                key='screen_facility_id', msg = 'no screen result to delete')

        return HttpResponse(status=204)
            
    @write_authorization
    @background_job
    @un_cache 
    @transaction.atomic       
    def patch_detail(self, request, **kwargs):
        
        if 'screen_facility_id' not in kwargs:
            raise BadRequestError(key='screen_facility_id', msg = 'required')
        screen_facility_id = kwargs['screen_facility_id']
        screen = Screen.objects.get(facility_id=screen_facility_id)

        data, deserialize_meta = self.deserialize(request)
        meta = data[API_RESULT_META]
        columns = data['fields']
        result_values = data[API_RESULT_DATA]
        
        if ('screen_facility_id' in meta 
            and meta['screen_facility_id'] != screen_facility_id):
            logger.warn(
                'screen_facility_id in file %r does not match url: %r'
                % (meta['screen_facility_id'], screen_facility_id))
        
        screen_result_meta = \
            self.create_screen_result(request, screen, columns, result_values)
             
        meta = {
            SCHEMA.API_MSG_RESULT: SCHEMA.API_MSG_SUCCESS
        }
        meta.update(screen_result_meta)
        if deserialize_meta:
            meta.update(deserialize_meta)
        logger.debug('data: %r', meta)
        response = self.build_response(request, { API_RESULT_META: meta }, **kwargs)
        response.status_code = 200
        response['Content-Disposition'] = (
            'attachment; filename=screen_result_loading_success-%s.xlsx' 
            % screen_facility_id )
        return response
    
    def create_screen_result(self, request, screen, columns, result_values, **kwargs):
        '''
        Create screen results for the given screen (internal callers):
        - if results exist, replaces them.
        '''
        
        schema = self.build_schema(user=request.user)
        
        # Clear cache: note "all" because mutual positive columns can change
        # self.clear_cache(by_uri='/screenresult/%s' % screen_facility_id)
        self.clear_cache(request, all=True)
        
#         id_attribute = schema['id_attribute']
        meta = { 'columns': len(columns) }
        
        try:
            adminuser = ScreensaverUser.objects.get(username=request.user.username)
        except ObjectDoesNotExist as e:
            logger.error('admin user: %r does not exist', request.user.username )
            raise
        screen_log = self.make_log(request, **{ 'action': API_ACTION.PATCH })
        screen_log.ref_resource_name = 'screen'
        screen_log.key = screen.facility_id
        screen_log.uri = '/'.join(['screen', screen_log.key])
        screenresult_log = self.make_log(request, **kwargs)
        screenresult_log.ref_resource_name = 'screenresult'
        screenresult_log.key = screen.facility_id
        screenresult_log.uri = '/'.join(['screenresult', screen_log.key])
        screenresult_log.diffs = {}
        
        with transaction.atomic():

            logger.info('Create screen result for %r', screen.facility_id)
            if hasattr(screen, 'screenresult'):
                screen_result = screen.screenresult
                logger.info('screen result: %r exists, deleting extant data',
                    screen_result)
                screen_result.datacolumn_set.all().delete()
                screen_result.assaywell_set.all().delete()
                screen_result.screen.assayplate_set\
                    .filter(library_screening__isnull=True).delete()
                screen_log.diffs = { 
                    'last_data_loading_date': 
                        [screen_result.date_loaded, screen_log.date_time]
                }
                screen_result.date_loaded = screen_log.date_time
                screen_result.created_by = adminuser
                meta[SCHEMA.API_MSG_ACTION] = 'replaced'
            else:
                screen_result = ScreenResult.objects.create(
                    screen=screen,
                    experimental_well_count=0,
                    replicate_count=0,
                    date_loaded=screen_log.date_time,
                    created_by=adminuser)
                screen_log.diffs = {
                    'last_data_loading_date': [None, screen_log.date_time] }
                logger.info('created screen result: %r', screen_result)
                meta[SCHEMA.API_MSG_ACTION] = 'created'
            
            screen_log.save()
            screenresult_log.parent_log = screen_log
            logger.info('created log: %r', screen_log)
            
            logger.info(
                'Create screen result data columns for %r', screen.facility_id)
            sheet_col_to_datacolumn = {}
            derived_from_columns_map = {}
            errors = {}
            for i, (sheet_column, column_info) in enumerate(columns.items()):
                if ( column_info.get('screen_facility_id',None)
                    and column_info['screen_facility_id'] != screen.facility_id ):
                    logger.info('skipping column for screen_facility_id: %r', 
                        column_info.get('screen_facility_id') )
                    continue
                if column_info.get('ordinal',None) is None:
                    column_info['ordinal'] = i
                try:
                    dc = self.get_datacolumn_resource().patch_obj( 
                        request, column_info, screen_result=screen_result, **kwargs)
                    sheet_col_to_datacolumn[sheet_column] = dc
                    derived_from_columns = column_info.get(
                        'derived_from_columns', None)
                    if derived_from_columns:
                        derived_from_columns = [
                            x.strip().upper() 
                            for x in derived_from_columns.split(',')] 
                        derived_from_columns_map[sheet_column] = \
                            derived_from_columns
                except ValidationError, e:
                    errors.update({ sheet_column: e.errors })
            meta['derived_columns'] = len(derived_from_columns_map)
            logger.info(
                'create derived_from_columns: %r',derived_from_columns_map)
            logger.debug(
                'sheet_col_to_datacolumn: %r', sheet_col_to_datacolumn)
            for sheet_column,derived_cols in derived_from_columns_map.items():
                if not set(derived_cols).issubset(columns.keys()):
                    col_errors = errors.get(sheet_column, {})
                    col_errors['derived_from_columns'] = (
                        '%s are not in available columns: %s'
                        % (convert_list_vals(derived_cols, delimiter=','),
                           convert_list_vals(columns.keys(), delimiter=',')))
                    errors[sheet_column] = col_errors
                else:
                    parent_column = sheet_col_to_datacolumn[sheet_column]
                    for colname in derived_cols:
                        parent_column.derived_from_columns.add(
                            sheet_col_to_datacolumn[colname])
                    logger.info(
                        'parent: %d:%s, derived from columns: %s', 
                        parent_column.data_column_id,parent_column.name, 
                        ['%d:%s'%(col.data_column_id,col.name) for col 
                            in parent_column.derived_from_columns.all()])
                    parent_column.save()
                
            if errors:
                logger.warn('errors: %r', errors)
                raise ValidationError( errors={ 'data_columns': errors })

            logger.debug(
                'sheet_col_to_datacolumn: %r', sheet_col_to_datacolumn.keys())
            
            # Create Result Values
            try:
                result_meta = self.create_result_values(
                    screen_result, result_values, sheet_col_to_datacolumn,
                    screenresult_log)
                meta.update(result_meta)
            except ValidationError, e:
                logger.exception('Validation error: %r', e)
                raise e
            # END of Transaction
            
        self.create_data_loading_statistics(screen_result)
    
        # Create indexes
        self.get_create_screen_overlap_indexes()
        
        screenresult_log.diffs.update({ 
            'created_by': [None,adminuser.username],  
            'experimental_well_count': [
                None,screen_result.experimental_well_count],
            'replicate_count': [None,screen_result.replicate_count],
            'channel_count': [None,screen_result.channel_count],
        })
        screenresult_log.save()
        meta['experimental_well_count'] = screen_result.experimental_well_count
        
        
        if screen.study_type is None:
            logger.info('pre-generate the mutual positives index...')
            self.get_mutual_positives_columns(screen_result.screen_result_id)
            logger.info('done - pre-generate the mutual positives index')
        else:
            logger.info('screen is a study, do not pre-generate mutual positives index')
        return meta
    
    def create_result_value(
            self, colname, value, dc, well, initializer_dict, 
            assay_well_initializer):
        
        if DEBUG_RV_CREATE:
            logger.info('create result value: %r: %r', colname, value)
        
        DATA_TYPE = SCHEMA.VOCAB.datacolumn.data_type
    
        well_id = well.well_id
        
        rv_initializer = {}
        rv_initializer.update(initializer_dict)
        if colname in rv_initializer.get('exclude', []):
            rv_initializer['is_exclude'] = True
        if 'exclude' in rv_initializer:
            del rv_initializer['exclude']

        key = '%s-%s' % (well_id, colname)
        logger.debug(
            'create result value: %r:%r, colname: %r, dc: %r %r',
            key, value, colname, dc.name, dc.data_type)
        rv_initializer['data_column_id'] = dc.data_column_id

        # TODO: 20170731: migrate the data_type of the datacolumn:
        # "numeric" has been replaced by "integer" and "decimal";
        # "text" has been replaced by "string"
        if dc.data_type in ['numeric','decimal','integer']:
            if  dc.decimal_places > 0:
                # parse, to validate
                parse_val(value, key, 'float')
                # record the raw val, to save all digits (precision)
                rv_initializer['numeric_value'] = value
            else:
                rv_initializer['numeric_value'] = \
                    parse_val(value, key, 'integer')
        else:
            # Text value
            rv_initializer['value'] = value
            
        # TODO: 20170731: migrate the positive types to a separate 
        # integer only column
        if dc.data_type in DATA_TYPE.positive_types:
            raw_value = value
            if dc.data_type == DATA_TYPE.PARTITIONED_POSITIVE:
                if value not in PARTITION_POSITIVE_MAPPING:
                    raise ValidationError(
                        key=key,
                        msg='%r val: %r must be one of %r'
                            % (dc.data_type, value, 
                                PARTITION_POSITIVE_MAPPING.keys()))
                value = PARTITION_POSITIVE_MAPPING[value]
                rv_initializer['value'] = value
            elif dc.data_type == DATA_TYPE.CONFIRMED_POSITIVE:
                if value not in CONFIRMED_POSITIVE_MAPPING:
                    raise ValidationError(
                        key=key,
                        msg='%r val: %r must be one of %r'
                            % (dc.data_type, value, 
                                CONFIRMED_POSITIVE_MAPPING.keys()))
                value = CONFIRMED_POSITIVE_MAPPING[value]
                rv_initializer['value'] = value
            elif  dc.data_type == DATA_TYPE.BOOLEAN_POSITIVE:
                value = parse_val(value,key,'boolean')
                rv_initializer['value'] = value


            if rv_initializer['is_exclude'] is True:
                logger.warn(
                    ('excluded col, not considered for positives:'
                     'well: %r, col: %r, type: %r, val: %r'),
                    well_id, colname, dc.data_type, value)
            else:
                if dc.data_type == 'partition_positive_indicator':
                    if value == PARTITION_POSITIVE_MAPPING['W']:
                        dc.weak_positives_count += 1
                        dc.positives_count += 1
                        assay_well_initializer['is_positive'] = True
                    elif value == PARTITION_POSITIVE_MAPPING['M']:
                        dc.medium_positives_count += 1
                        dc.positives_count += 1
                        assay_well_initializer['is_positive'] = True
                    elif value == PARTITION_POSITIVE_MAPPING['S']:
                        dc.positives_count += 1
                        dc.strong_positives_count += 1
                        assay_well_initializer['is_positive'] = True
                elif dc.data_type == 'confirmed_positive_indicator':
                    if value == CONFIRMED_POSITIVE_MAPPING['CP']:
                        dc.positives_count += 1
                        assay_well_initializer['is_positive'] = True
                elif dc.data_type == 'boolean_positive_indicator':
                    if value is True:
                        dc.positives_count += 1
                        assay_well_initializer['is_positive'] = True
             
                if well.library_well_type != WELL_TYPE.EXPERIMENTAL\
                    and assay_well_initializer['is_positive'] is True:
                    # TODO: log these to a report file? slowing down the server
                    logger.debug('non experimental well, not considered for positives:'
                        'well: %r, %r, col: %r, type: %r, val: %r'
                        % ( well_id, well.library_well_type, colname, 
                            dc.data_type, value))
                    raise ValidationError(
                        key=key,
                        msg = ('non experimental well, not considered for positives: '
                         'library_well_type: %r, type: %r, val: %r'
                        % ( well.library_well_type, dc.data_type, raw_value)))
                if dc.data_type == 'confirmed_positive_indicator':
                    if assay_well_initializer.get(
                            'confirmed_positive_value',PSYCOPG_NULL) != PSYCOPG_NULL:
                        raise ValidationError(
                            key=key,
                            msg=('only one "confirmed_positive_indicator" is'
                                'allowed per row'))
                    assay_well_initializer['confirmed_positive_value'] = \
                        rv_initializer['value']
        
        if DEBUG_RV_CREATE:
            logger.info('rv_initializer: %r', rv_initializer)
        return rv_initializer
    
    @transaction.atomic
    def create_result_values(
            self, screen_result, result_values, sheet_col_to_datacolumn,
            screenresult_log):
        logger.info(
            'create result values for %r ...', screen_result.screen.facility_id)

        fieldnames = [
            'well_id', 'data_column_id',  # 'result_value_id',
            'value', 'numeric_value', 'is_positive',
            'is_exclude', 'assay_well_control_type',
        ]
        assay_well_fieldnames = [
            'screen_result_id', 'well_id', 'plate_number', 'is_positive',
            'confirmed_positive_value','assay_well_control_type',
        ]
        meta_columns = ['well_id', 'assay_well_control_type', 'exclude']
        meta = {}
        
        with SpooledTemporaryFile(max_size=MAX_SPOOLFILE_SIZE) as result_value_file,\
            SpooledTemporaryFile(max_size=MAX_SPOOLFILE_SIZE) as assay_well_file:
        
            writer = unicodecsv.DictWriter(
                result_value_file, fieldnames=fieldnames, delimiter=str(','),
                lineterminator="\n")
            assay_well_writer = unicodecsv.DictWriter(
                assay_well_file, fieldnames=assay_well_fieldnames, 
                delimiter=str(','), lineterminator="\n")
            
            rows_created = 0
            rvs_to_create = 0
            # plates_max_replicate_loaded = {}
            logger.info('write temp file result values for screen: %r ...',
                screen_result.screen.facility_id)
            errors = {}
            while True:
                try: 
                    # Note: iterating triggers parsing using the generator
                    # Note: for testing, or if data are posted using JSON, and
                    # not as the Screen Result Load file format, result values
                    # may be a list
                    if isinstance(result_values, (list, tuple)):
                        result_values = iter(result_values)
                    
                    result_row = result_values.next()
                    initializer_dict = { 
                        fieldname:PSYCOPG_NULL for fieldname in fieldnames}
                    assay_well_initializer = { 
                        fieldname:PSYCOPG_NULL 
                            for fieldname in assay_well_fieldnames}
                    for meta_field in meta_columns:
                        if meta_field in result_row:
                            initializer_dict[meta_field] = result_row[meta_field]
                    try:
                        well = Well.objects.get(well_id=result_row['well_id']) 
                    except ObjectDoesNotExist, e:
                        logger.info('well not found: %r', result_row['well_id'])
                        raise ValidationError(
                            key='well_id', msg='well not found: %r' % result_row['well_id'])
                    assay_well_initializer.update({
                        'screen_result_id': screen_result.screen_result_id,
                        'well_id': well.well_id,
                        'plate_number': well.plate_number,
                        'is_positive': False })
                    
                    assay_well_control_type = result_row.get('assay_well_control_type', None)
                    if assay_well_control_type:
                        allowed_control_well_types = [
                            WELL_TYPE.EMPTY, WELL_TYPE.DMSO, WELL_TYPE.LIBRARY_CONTROL]
                        if screen_result.screen.screen_type == SCREEN_TYPE.RNAI:
                            allowed_control_well_types.append(WELL_TYPE.RNAI_BUFFER)
                        if well.library_well_type in allowed_control_well_types:
                            assay_well_initializer['assay_well_control_type'] = \
                                assay_well_control_type
                        else:
                            raise ValidationError(
                                key=well.well_id,
                                msg='control wells must be one of %r, found: %r'
                                 % (allowed_control_well_types, well.library_well_type))

                    for colname, val in result_row.items():
                        if DEBUG_RV_CREATE:
                            logger.info('result value to create: %r, %r', colname, val)
                        if colname in meta_columns:
                            continue
                        if val is None:
                            continue
                        if colname not in sheet_col_to_datacolumn:
                            logger.debug('extra col found in the Data sheet: %r', 
                                colname)
                            continue      
                        dc = sheet_col_to_datacolumn[colname]
                        try:
                            rv_initializer = self.create_result_value(
                                colname, val, dc, well, initializer_dict, 
                                assay_well_initializer)
                            
                            # 20170424 - remove data loading replicate tracking - per JAS
                            # if (dc.is_derived is False
                            #     and well.library_well_type == 'experimental'
                            #     and rv_initializer['is_exclude'] is not True):
                            #     max_replicate = \
                            #         plates_max_replicate_loaded.get(well.plate_number, 0)
                            #     if dc.replicate_ordinal > max_replicate:
                            #         plates_max_replicate_loaded[well.plate_number] = \
                            #             dc.replicate_ordinal
                            # else:
                            #     logger.debug(('not counted for replicate: well: %r, '
                            #         'type: %r, initializer: %r'), 
                            #         well.well_id, well.library_well_type, rv_initializer)   
                            writer.writerow(rv_initializer)
                            rvs_to_create += 1
                        except ValidationError,e1:
                            errors.update(e1.errors)
                        
                    assay_well_writer.writerow(assay_well_initializer)
                    rows_created += 1
                    if rows_created % 10000 == 0:
                        logger.info(
                            'wrote %d result rows to temp file', rows_created)

                except ValidationError, e:
                    logger.exception('validation error: %r; errors: %r', e, errors)
                    errors.update(e.errors) 
                except StopIteration, e:
                    break
            
            if errors:
                logger.warn('errors: %r', errors)
                raise ValidationError(errors=errors)
            
            if not rvs_to_create:
                raise ValidationError( errors={ 
                    'result_values': 'no result values were parsed' })

            logger.info('result_values: rows: %d, result_values to create: %d',
                rows_created, rvs_to_create)
            meta['assay_wells'] = rows_created
            meta['result_values'] = rvs_to_create
            
            logger.info(
                'use copy_from to create %d assay_wells...', rows_created)
            assay_well_file.seek(0)
            
            with connection.cursor() as conn:
                conn.copy_from(
                    assay_well_file, 'assay_well', sep=str(','), 
                    columns=assay_well_fieldnames, null=PSYCOPG_NULL)
                logger.info('assay_wells created.')
                
                logger.info(
                    'use copy_from to create %d result_values...', rvs_to_create)
                result_value_file.seek(0)
                conn.copy_from(
                    result_value_file, 'result_value', sep=str(','), 
                    columns=fieldnames, null=PSYCOPG_NULL)
                logger.info('result_values created.')
            screenresult_log.diffs.update({
                'result_values_created': [None,rvs_to_create],
                'assay_wells_loaded': [None, rows_created]
                })
        for dc in sheet_col_to_datacolumn.values():
            dc.save()
            
        # 20170424 - remove replicate tracking for data load - per JAS
        # if plates_max_replicate_loaded:
        #     plates_max_replicate_loaded = sorted(
        #         plates_max_replicate_loaded.values())
        #     logger.info(
        #         'plates_max_replicate_loaded: %r', plates_max_replicate_loaded)
        #     screen_result.screen.min_data_loaded_replicate_count = \
        #         plates_max_replicate_loaded[0]
        #     screen_result.screen.max_data_loaded_replicate_count = \
        #         plates_max_replicate_loaded[-1]
        # else:
        #     screen_result.screen.min_data_loaded_replicate_count = 1
        #     screen_result.screen.max_data_loaded_replicate_count = 1
        # screen_result.screen.save()
        # logger.info('screen_result.screen.max_data_loaded_replicate_count: %r',
        #     screen_result.screen.max_data_loaded_replicate_count)
        
        logger.info('create_result_values - done.')
        return meta
        
        # REMOVED - find or create assay plates for screenresult load wells
        # This is removed because the only reason for creating these "assay_plates"
        # is to find the min/max replicates loaded for assay plates with data loaded.
        # NOTE 1: This stat could be calculated during load time instead - see: 
        # "plates_max_replicate_loaded" tracking hash in screen_result load process
        # NOTE 2: Per discussion, this stat will be dropped in SS2 - 201612, per
        # discussion with JenS   
        # def find_or_create_assay_plates_data_loaded(self,screen_result):
        # 
        # # FIXME: create stats needed without creating assay_plates
        #  
        # # Create assay plates if the don't exist
        # # SS1 strategy:
        # # see ScreenResult.findOrCreateAssayPlatesDataLoaded(plate_number, replicates_loaded)
        # # a. find the max replicate for each plate in the result_values
        # # 1. find most recent library screening for that plate
        # # 2. find the assay plates (all replicates) for only that library screening
        # # 3. if assay plate count < max replicate screened for that plate, then create ap's
        # # 3.a in this case create the assay plates sans copy information
        # # 4. re-run the stat to determine how many assay plates have data loaded:
        # # - ap.replicate_ordinal in distinct(data_column.replicate_ordinal) for dc join assay_wells  
        #  
        # sql = (
        #     'with replicates_plate_loaded as ( '
        #     'select plate_number, max(replicate_ordinal) plate_max '
        #     '  from screen  '
        #     '  join screen_result sr using(screen_id) '
        #     '  join data_column dc using(screen_result_id)'
        #     '  join result_value using(data_column_id)'
        #     '  join well using(well_id)'
        #     '  where screen.facility_id = %(facility_id)s '
        #     '  group by well.plate_number order by well.plate_number),'
        #     'replicates_assay_plate_screened as ( '
        #     'select plate_number, count(*) '
        #     '  from assay_plate ap '
        #     '  join library_screening ls on(ap.library_screening_id=ls.activity_id) ' 
        #     '  join screen using(screen_id) '
        #     '  where screen.facility_id = %(facility_id)s '
        #     '  group by plate_number order by plate_number )'
        #     'select '
        #     'rp.plate_number, '
        #     'rp.plate_max, '
        #     'rps.count '
        #     'from replicates_plate_loaded rp '
        #     'left join replicates_assay_plate_screened rps using(plate_number) '
        #     'where rp.plate_max > rps.count or rps.count is null; '
        #     )
        # conn = self.bridge.get_engine().connect()
        # result_proxy = conn.execute(
        #     sql, { 'facility_id': screen_result.screen.facility_id })
        #  
        # assay_plates_created = []
        # for (plate_number, replicates_needed, replicates_extant ) in result_proxy:
        #     if not replicates_extant:
        #         replicates_extant = 0
        #     replicates_to_create = (replicates_needed-replicates_extant)
        #     logger.info('plate_number: %r, replicates_to_create: %d', 
        #         plate_number, replicates_to_create)
        #     for i in range(replicates_extant, replicates_extant+replicates_needed):
        #         assay_plates_created.append(
        #             AssayPlate.objects.create(
        #                 screen=screen_result.screen,
        #                 plate_number=plate_number,
        #                 replicate_ordinal=(i)))
        #  
        # logger.info('TODO: create user message: created assay plates: %d, %r', 
        #     len(assay_plates_created), 
        #     [ap.plate_number for ap in assay_plates_created])
        #  
        # # step two: set the new 'is_loaded' flag for all assay plates loaded 
        # # in this screen result
        

    @transaction.atomic
    def create_data_loading_statistics(self, screen_result):
        with get_engine().connect() as conn:
            # NOTE: mixing Django connection with SQA connection
            # - thrown exceptions will rollback the nested SQA transaction
            # see: http://docs.sqlalchemy.org/en/latest/core/connections.html
            screen_facility_id = screen_result.screen.facility_id
            sql_experimental_wells_loaded = (
                'select count(*) '
                'from screen s '
                'join screen_result sr using(screen_id) '
                'join assay_well aw using(screen_result_id) '
                'join well w using(well_id) '
                'where w.library_well_type = %s '
                'and s.facility_id = %s; ')
            screen_result.experimental_well_count = int(
                conn.execute(
                    sql_experimental_wells_loaded,
                    (WELL_TYPE.EXPERIMENTAL, screen_facility_id))
                .scalar() or 0)
            
            sql_replicate_count = (
                 'select max(replicate_ordinal) '
                 'from data_column ' 
                 'join screen_result using(screen_result_id) ' 
                 'join screen using (screen_id) '
                 'where facility_id = %s;')
            screen_result.replicate_count = int(
                conn.execute(
                    sql_replicate_count, screen_facility_id)
                .scalar() or 0)
            if screen_result.replicate_count == 0:
                screen_result.replicate_count = 1
                
            sql_channel_count = (
                 'select max(channel) '
                 'from data_column ' 
                 'join screen_result using(screen_result_id) ' 
                 'join screen using (screen_id) '
                 'where facility_id = %s;')
            screen_result.channel_count = int(
                conn.execute(
                    sql_channel_count, screen_facility_id)
                .scalar() or 0)
            if screen_result.channel_count == 0:
                screen_result.channels_count = 1
                
        screen_result.save()
            

class DataColumnAuthorization(ScreenAuthorization):
    
    def __init__(self, *args, **kwargs):
        ScreenAuthorization.__init__(self, *args, **kwargs)
        self.screen_resource = None
        
    def get_screen_resource(self):
        if self.screen_resource is None:
            self.screen_resource = ScreenResource()
        return self.screen_resource
    
    def filter(self, user, filter_expression):
        # TODO: test: replace with filter_in_sql for performance
        
        if self.is_restricted_view(user) is not True:
            return filter_expression
         
        logger.info('create authorized data columns filter for user: %r', user)
        screensaver_user = ScreensaverUser.objects.get(
            username=user.username)
         
        screen_access_dict = self.get_screen_access_level_table(user.username)
         
        # All fields are visible on data access level 2-3 
        # (user & screen dsl 1, screen dsl 0)
        or_clause = [column('screen_data_sharing_level') == DSL.SHARED]
        level_2_3_screen_ids = [ screen['facility_id']
            for screen in screen_access_dict.values() 
                if screen['user_access_level_granted'] 
                    in [ACCESS_LEVEL.MUTUALLY_SHARED, ACCESS_LEVEL.ALL]]
        if level_2_3_screen_ids:
            or_clause.append(column('screen_facility_id').in_(level_2_3_screen_ids))
        # Positive columns only for data access level 1
        level_1_screen_ids = [ screen['facility_id']
            for screen in screen_access_dict.values() 
                if screen['user_access_level_granted']==ACCESS_LEVEL.OVERLAPPING_ONLY]
        if level_1_screen_ids:
            or_clause.append(and_(
                column('positives_count')>0,
                column('screen_facility_id').in_(level_1_screen_ids)))
        
        auth_filter = or_(*or_clause)
        # add or clauses conditionally instead
        # auth_filter = or_(
        #     column('screen_data_sharing_level') == 0,
        #     column('screen_facility_id').in_(level_2_3_screen_ids),
        #     and_(column('positives_count')>0,
        #         column('screen_facility_id').in_(level_1_screen_ids))
        #     )
        logger.debug('created data column filter: %r', auth_filter)   
        if filter_expression is not None:
            filter_expression = and_(filter_expression, auth_filter)
        else:
            filter_expression = auth_filter
          
        return filter_expression

    def filter_in_sql(self, user, stmt, screen_table, dc_table):
        return stmt
    
        # if self.is_restricted_view(user) is False:
        #     return stmt
        # 
        # logger.info('create authorized data columns filter for user: %r', user)
        # 
        # screen_access_dict = self.get_screen_access_level_table(user.username)
        # 
        # # All fields are visible on data access level 2-3 
        # # (user & screen dsl 1, screen dsl 0)
        # level_2_3_screen_ids = [ screen['facility_id']
        #     for screen in screen_access_dict.values() 
        #         if screen['user_access_level_granted'] in [2,3]]
        # # Positive columns only for data access level 1
        # level_1_screen_ids = [ screen['facility_id']
        #     for screen in screen_access_dict.values() 
        #         if screen['user_access_level_granted']==1]
        # stmt = stmt.where(or_(
        #     screen_table.c.data_sharing_level == 0,
        #     screen_table.c.facility_id.in_(level_2_3_screen_ids),
        #     and_(dc_table.c.positives_count>0,
        #         screen_table.c.facility_id.in_(level_1_screen_ids))
        #     ))
        # return stmt
    
    def get_row_property_generator(self, user, fields, extant_generator):
        '''
        Filter result properties based on authorization rules
        '''
        return self.get_access_level_property_generator(
           user, fields, extant_generator)
    
    def get_access_level_property_generator(self, user, fields, extant_generator):
        # Effective User-Screen Access Level:
        # 
        # ACCESS_LEVEL.LIMITED_ONLY: 0 - Field level 0 only
        # ACCESS_LEVEL.OVERLAPPING_ONLY: 1 - Field level 0,1
        # ACCESS_LEVEL.MUTUALLY_SHARED: 2 
        #     - Field level 0,1,2, Screen Results, Positives Summary
        # ACCESS_LEVEL.ALL: 3 
        #     - Field level 0,1,2,3, Screen Results, CPRs, Activities, Visits...
        
        if self.is_restricted_view(user) is not True:
            return extant_generator
        else:
            logger.debug(
                'get_access_level_property_generator for user: %r', user)
            screensaver_user = ScreensaverUser.objects.get(username=user.username)

            fields_by_level = self.get_fields_by_level(fields)
            screen_access_dict = self.get_screen_access_level_table(user.username)

            class Row:
                def __init__(self, row):
                    self.row = row
                    self.facility_id = facility_id = row['screen_facility_id']
                    
                    effective_access_level = None
                    screen_data = screen_access_dict.get(facility_id, None)
                    if screen_data:
                        effective_access_level = \
                            screen_data.get('user_access_level_granted',None)
                    if DEBUG_SCREEN_ACCESS:
                        logger.info('screen: %r, effective_access_level: %r', 
                            facility_id, effective_access_level)
                    self.effective_access_level = effective_access_level
                    
                    if ( self.effective_access_level is None
                        or self.effective_access_level < ACCESS_LEVEL.OVERLAPPING_ONLY ):
                        logger.error(
                            'dc shown for not visible screen: '
                            '%r, %r, access level: %r',
                            row['data_column_id'], facility_id, 
                            self.effective_access_level)
                            
                    self.allowed_fields = set()
                    if self.effective_access_level is not None:
                        for level in range(0,self.effective_access_level+1):
                            self.allowed_fields.update(fields_by_level[level])
                            if DEBUG_DC_ACCESS:
                                logger.info(
                                    'allow level: %r: %r, %r', 
                                    level, fields_by_level[level], 
                                    self.allowed_fields)
                    else: 
                        logger.warn('user: %r has no access level for screen: %r',
                            user.username, facility_id)
                    if DEBUG_DC_ACCESS:
                        logger.info(
                            'dc: %r:%r:%r user: %r, effective access level: %r',
                            row['data_column_id'],row['name'], facility_id, 
                            screensaver_user.username, self.effective_access_level )
                    if DEBUG_DC_ACCESS:
                        logger.info('allowed fields: %r', self.allowed_fields)
                def has_key(self, key):
                    if key == 'user_access_level_granted':
                        return True
                    return self.row.has_key(key)
                def keys(self):
                    return self.row.keys()
                def __getitem__(self, key):
                    logger.debug(
                        'key: %r, allowed: %r', key, key in self.allowed_fields)
                    if key == 'user_access_level_granted':
                        return self.effective_access_level
                    if self.row[key] is None:
                        return None
                    else:
                        if key in self.allowed_fields:
                            logger.debug('allow %r: %r', key, self.row[key])
                            return self.row[key]
                        elif DEBUG_DC_ACCESS:
                            logger.info(
                                '%r filter field: %r for restricted user: %r',
                                self.facility_id, key, user.username)
                        return None

            def dc_property_generator(cursor):
                if extant_generator is not None:
                    cursor = extant_generator(cursor)
                for row in cursor:
                    yield Row(row)
            
            return dc_property_generator
        
class DataColumnResource(DbApiResource):

    class Meta:

        queryset = DataColumn.objects.all()  # .order_by('facility_id')
        authentication = MultiAuthentication(
            IccblBasicAuthentication(), IccblSessionAuthentication())
        resource_name = 'datacolumn'
        authorization = DataColumnAuthorization(resource_name)
        ordering = []
        serializer = LimsSerializer()

    def __init__(self, **kwargs):
        super(DataColumnResource, self).__init__(**kwargs)
        self.screen_resource = None
        self.dc_types_vocab = None
        
    def get_screen_resource(self):
        if self.screen_resource is None:
            self.screen_resource = ScreenResource()
        return self.screen_resource
    
    def get_data_column_types(self):
        if self.dc_types_vocab is None:
            self.dc_types_vocab = self.get_vocab_resource()\
                ._get_vocabularies_by_scope('datacolumn.data_type')
        return self.dc_types_vocab
        
    def prepend_urls(self):

        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<data_column_id>\d+)%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url((r"^(?P<resource_name>%s)/screen/"
                 r"(?P<screen_facility_id>([\w]+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_list'), name="api_dispatch_list"),
            url((r"^(?P<resource_name>%s)/for_screen/"
                 r"(?P<for_screen_facility_id>([\w]+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_datacolumn_other_screens_view'), 
                name="api_dispatch_datacolumn_other_screens_view"),
        ]    

    def dispatch_datacolumn_other_screens_view(self, request, **kwargs):
        return self.dispatch('list', request, **kwargs)    

    @read_authorization
    def get_detail(self, request, **kwargs):

        data_column_id = kwargs.get('data_column_id', None)
        if not data_column_id:
            raise MissingParam('data_column_id')
        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, **kwargs):
        
        logger.info('build datacolumn response...')
        outer_self = self
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        schema = super(DataColumnResource, self).build_schema(user=request.user)
        
        is_for_detail = kwargs.pop('is_for_detail', False)
        
        
        try:
            # general setup
            screen_facility_id = param_hash.pop('screen_facility_id', None)
            for_screen_facility_id = param_hash.pop('for_screen_facility_id', None)
            manual_field_includes = set(param_hash.get('includes', []))
            # Add fields required to build the system representation
            manual_field_includes.update([
                'screen_id','screen_data_sharing_level',
                'name','data_type','decimal_places','ordinal',
                'screen_facility_id','data_column_id','user_access_level_granted'])
            
            (filter_expression, filter_hash, readable_filter_hash) = \
                SqlAlchemyResource.build_sqlalchemy_filters(
                    schema, param_hash=param_hash)
            filename = self._get_filename(
                readable_filter_hash, schema, is_for_detail)

            filter_expression = self._meta.authorization.filter(
                request.user, filter_expression)


            order_params = param_hash.get('order_by', 
                ['-study_type','screen_facility_id','ordinal'])
            field_hash = self.get_visible_fields(
                schema['fields'], filter_hash.keys(), manual_field_includes,
                param_hash.get('visibilities'),
                exact_fields=set(param_hash.get('exact_fields', [])),
                order_params=order_params)
            order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
                order_params, field_hash)
             
            max_ordinal = len(field_hash)
            def create_dc_generator(extant_generator=None):
                ''' 
                Transform the DataColumn records into Resource.Fields for the UI
                '''
                
                class DataColumnRow:
                    def __init__(self, row,i):
                        self._dict = \
                            outer_self._create_datacolumn_from_row(
                                row,i)
                    def has_key(self, key):
                        return key in self._dict
                    def keys(self):
                        return self._dict.keys()
                    def __getitem__(self, key):
                        if key in self._dict:
                            return self._dict[key]

                def datacolumn_fields_generator(cursor):
                    if extant_generator:
                        cursor = extant_generator(cursor)
                    # NOTE: using the given sort order to override the ordinal
                    for i,row in enumerate(cursor):
                        yield DataColumnRow(row, max_ordinal+i)
                return datacolumn_fields_generator
            
            rowproxy_generator = create_dc_generator()
            rowproxy_generator = \
                self._meta.authorization.get_row_property_generator(
                    request.user, field_hash, rowproxy_generator)
            if use_vocab is True:
                rowproxy_generator = \
                    DbApiResource.create_vocabulary_rowproxy_generator(
                        field_hash, rowproxy_generator)

            # specific setup 

            _dc = self.bridge['data_column']
            _sr = self.bridge['screen_result']
            _screen = self.bridge['screen']
            _dc_derived_link = self.bridge['data_column_derived_from_columns']
            _dc_derived = _dc.alias('dc_derived')
            
            base_query_tables = [
                'data_column', 'screen']
            
            custom_columns = {
                'key': (
                    _concat('dc_',_screen.c.facility_id, '_', 
                        cast(_dc.c.data_column_id,sqlalchemy.sql.sqltypes.Text))),
                # default to admin level; auth row property generator will update
                'user_access_level_granted': literal_column('3'),
                'derived_from_columns': (
                    select([func.array_to_string(
                        func.array_agg(literal_column('name')), 
                        LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(
                        select([_dc_derived.c.name])
                        .select_from(_dc_derived.join(
                            _dc_derived_link,
                            _dc_derived_link.c.to_datacolumn_id
                                ==_dc_derived.c.data_column_id))
                        .where(_dc_derived_link.c.from_datacolumn_id
                            ==literal_column('data_column.data_column_id'))
                        .order_by(_dc_derived.c.ordinal).alias('inner'))
                    )
                }
            
            columns = self.build_sqlalchemy_columns(
                field_hash.values(), base_query_tables=base_query_tables,
                custom_columns=custom_columns)

            # build the query statement
            
            j = _dc
            j = j.join(_sr, _dc.c.screen_result_id == _sr.c.screen_result_id)
            j = j.join(_screen, _sr.c.screen_id == _screen.c.screen_id)
            stmt = select(columns.values()).select_from(j)
            
            # TODO: test if more efficient filtering in sql
            # stmt = self._meta.authorization.filter_in_sql(
            #     request.user, stmt, _screen, _dc)
            
            if screen_facility_id:
                stmt = stmt.where(_screen.c.facility_id == screen_facility_id)
            if for_screen_facility_id:
                for_screen = Screen.objects.get(facility_id=for_screen_facility_id)
                stmt = stmt.where(_screen.c.screen_type == for_screen.screen_type)
                stmt = stmt.where(_screen.c.facility_id != for_screen_facility_id)

            # general setup
             
            (stmt, count_stmt) = self.wrap_statement(
                stmt, order_clauses, filter_expression)
            stmt = stmt.order_by('ordinal')

            if DEBUG_SCREENRESULT is True:
                compiled_stmt = str(stmt.compile(
                    dialect=postgresql.dialect(),
                    compile_kwargs={"literal_binds": True}))
                logger.info('compiled_stmt %s', compiled_stmt)
            
            title_function = None
            if use_titles is True:
                def title_function(key):
                    return field_hash[key]['title']
            
            return self.stream_response_from_statement(
                request, stmt, count_stmt, filename,
                field_hash=field_hash,
                param_hash=param_hash,
                is_for_detail=is_for_detail,
                rowproxy_generator=rowproxy_generator,
                title_function=title_function, meta=kwargs.get('meta', None),
                use_caching=True )
             
        except Exception, e:
            logger.exception('on get list')
            raise e  

    data_type_lookup = {
        'partition_positive_indicator': {
            'vocabulary_scope_ref': 'resultvalue.partitioned_positive',
            'data_type': 'string',
            'edit_type': 'select',
            },
        'boolean_positive_indicator': {
            'data_type': 'boolean',
#             'vocabulary_scope_ref': 'resultvalue.boolean_positive_indicator'
            },
        'confirmed_positive_indicator': {
            'vocabulary_scope_ref': 'resultvalue.confirmed_positive_indicator',
            'data_type': 'string',
            'edit_type': 'select' 
            },
    }
            
    def _create_datacolumn_from_row(self, row_or_dict, ordinal_override=0 ):
        ''' Transform a row as defined by the DataColumnResource query into a
        schema field descriptor.
        '''
        _dict = {
            'data_type': 'string',
            'edit_type': None,
            'display_options': None,
            'ordinal': 0,
            'visibility': ['l', 'd'],
            'filtering': True,
            'ordering': True,
            'is_datacolumn': True,
        }
        for k in row_or_dict.keys():
            if k not in _dict:
                _dict[k] = row_or_dict[k]
        # TODO: 20170731: migrate the data_type of the datacolumn:
        # "numeric" has been replaced by "integer" and "decimal";
        # "text" has been replaced by "string"
        # TODO: 20170731: migrate the screenresult datacolumn format to use 
        # "vocabulary_scope_ref" for the "positive" column types
        key = row_or_dict['key']
        dc_data_type = row_or_dict['data_type']
        # NOTE: (hack) cache the "assay_data_type" here so that the 
        # screen_result_importer.create_output_data can use it as the "output"
        # data_type; the importer does not use vocabulary_scope_ref, and this 
        # is neeed to preserve symmetry of read/write files
        # (see TODO above)
        _dict['assay_data_type'] = dc_data_type
        if dc_data_type  in ['numeric','decimal','integer']:
            if row_or_dict.has_key('decimal_places'):
                decimal_places = row_or_dict['decimal_places']
                if decimal_places > 0:
                    _dict['data_type'] = 'decimal'
                    _dict['display_options'] = (
                         '{ "decimals": %s }' % decimal_places)
                else:
                    _dict['data_type'] = 'integer'
        elif dc_data_type in DataColumnResource.data_type_lookup:
            _dict.update(DataColumnResource.data_type_lookup[dc_data_type])
            if not _dict.get('description',None):
                vocab = self.get_data_column_types()[dc_data_type]
                _dict['description'] = vocab.get('description', None)
        elif dc_data_type in ['string','text']:
            _dict['display_type'] = 'full_string'
        if ordinal_override > 0:
            _dict['ordinal'] = ordinal_override                           
        logger.debug('created datacolumn from row: %r, %r', key, _dict)
        return _dict

    # FIXME: deprecated
    def _create_datacolumn_from_orm(self, dc):
        ''' Transform an ORM DataColumn record into a
        schema field descriptor.
        '''

        screen_facility_id = dc.screen_result.screen.facility_id
        screen = Screen.objects.get(facility_id=screen_facility_id)
        columnName = "dc_%s_%s" % (screen_facility_id, default_converter(dc.name))
        _dict = {}
        _dict.update(model_to_dict(dc))
        _dict['title'] = '%s [%s]' % (dc.name, screen_facility_id) 
        _dict['description'] = _dict['description'] or _dict['title']
        _dict['mouseover'] = '%s: %s - %s' % (screen_facility_id, screen.title, dc.name)
        _dict['comment'] = dc.comments
        _dict['is_datacolumn'] = True
        _dict['key'] = columnName
        _dict['scope'] = 'datacolumn.screenresult-%s' % screen_facility_id
        _dict['screen_facility_id'] = screen_facility_id
        _dict['assay_data_type'] = dc.data_type
        _dict['derived_from_columns'] = [x.name for x in dc.derived_from_columns.all()]
        _dict['visibility'] = ['api']
        _dict['filtering'] = True
        _dict['edit_type'] = None

        dc_types_vocab = self.get_data_column_types()
        # TODO: 20170731: migrate the data_type of the datacolumn:
        # "numeric" has been replaced by "integer" and "decimal";
        # "text" has been replaced by "string"
        if dc.data_type in ['numeric','decimal','integer']:
            if _dict.get('decimal_places', 0) > 0:
                _dict['data_type'] = 'decimal'
                _dict['display_options'] = \
                    '{ "decimals": %s, "orderSeparator": "" }' % _dict['decimal_places']
            else:
                _dict['data_type'] = 'integer'
        elif dc.data_type in DataColumnResource.data_type_lookup:
            _dict.update(DataColumnResource.data_type_lookup[dc.data_type])
            if not _dict.get('description',None):
                vocab = dc_types_vocab[dc_data_type]
                _dict['description'] = vocab.get('description', None)
        else:
            _dict['data_type'] = 'string'
        _dict['data_column_id'] = dc.data_column_id
        logger.debug('create dc from orm: %r: %r', columnName, _dict)
        return (columnName, _dict)

    @write_authorization
    @un_cache
    @transaction.atomic
    def patch_detail(self, request, **kwargs):
        # TODO: 20170731: allow data_column_id to be passed as an arg so 
        # that the DataColumn may be patched external from the Screen Result
        raise ApiNotImplemented(self._meta.resource_name, 'patch_detail')
    
    @write_authorization
    @un_cache
    @transaction.atomic
    def patch_list(self, request, **kwargs):
        # TODO: 20170731: allow data_column_id to be passed as an arg so 
        # that the DataColumn may be patched external from the Screen Result
        raise ApiNotImplemented(self._meta.resource_name, 'patch_list')
    
    @write_authorization
    @transaction.atomic    
    def patch_obj(self, request, deserialized, screen_result=None, **kwargs):
        
        # TODO: 20170731: allow data_column_id to be passed as an arg so 
        # that the DataColumn may be patched external from the Screen Result
        if screen_result is None:
            raise BadRequestError(key='screen_result', msg = 'required')
        logger.debug('patch_obj %s', deserialized)
        
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        fields = schema['fields']
        # FIXME: default values
        initializer_dict = {
            'is_derived': False,
            'is_follow_up_data': False,
            'positives_count': 0,
            'strong_positives_count': 0,
            'medium_positives_count': 0,
            'weak_positives_count': 0
        }
        
        for key in fields.keys():
            if deserialized.get(key, None) is not None:
                val = deserialized[key]
                initializer_dict[key] = parse_val(
                    val, key, fields[key]['data_type']) 
                logger.debug('key: %r, val: %r, parsed: %r',
                    key, val, initializer_dict[key])

        errors = self.validate(initializer_dict, schema=schema, patch=False)
        if errors:
            raise ValidationError(errors)

        # TODO: 20170731: migrate the data_type of the datacolumn:
        # "numeric" has been replaced by "integer" and "decimal";
        # "text" has been replaced by "string"
        if initializer_dict['data_type'] == 'numeric':
            if initializer_dict.get('decimal_places',0) > 0:
                initializer_dict['data_type'] = 'decimal'
            else:
                initializer_dict['data_type'] = 'integer'
        if initializer_dict['data_type'] == 'text':
            initializer_dict['data_type'] = 'string'
                
        try:
            
            logger.debug('initializer dict: %s', initializer_dict)
            data_column = DataColumn(
                screen_result=screen_result)
            for key, val in initializer_dict.items():
                if hasattr(data_column, key):
                    setattr(data_column, key, val)
            data_column.save()
            return data_column
        except Exception, e:
            logger.exception('on patch detail')
            raise e  
    

class CopyWellResource(DbApiResource):
    
    COPYWELL_KEY = '{library_short_name}/{copy_name}/{well_id}'
    
    class Meta:
        
        queryset = CopyWell.objects.all().order_by('well_id')
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'copywell'
        authorization = UserGroupAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        always_return_data = True 

    def __init__(self, **kwargs):

        super(CopyWellResource, self).__init__(**kwargs)
        self.plate_resource = None
        
    def get_plate_resource(self):
        if self.plate_resource is None: 
            self.plate_resource = LibraryCopyPlateResource()
        return self.plate_resource

    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)/%s/(?P<%s>[\d]+)%s$" 
                % (self._meta.resource_name, SCHEMA.URI_PATH_COMPLEX_SEARCH, 
                    SCHEMA.API_PARAM_COMPLEX_SEARCH_ID, TRAILING_SLASH),
                self.wrap_view('search'), name="api_search"),
            url(r"^(?P<resource_name>%s)"
                r"/(?P<copy_name>[\w.\-\+ ]+)" 
                r"/(?P<well_id>\d{1,5}\:[a-zA-Z]{1,2}\d{1,2})%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)/(?P<library_short_name>[\w.\-\+: ]+)"
                r"/(?P<copy_name>[\w.\-\+ ]+)" 
                r"/(?P<well_id>\d{1,5}\:[a-zA-Z]{1,2}\d{1,2})%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)/(?P<library_short_name>[\w.\-\+: ]+)"
                r"/(?P<copy_name>[\w.\-\+: ]+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_list'), name="api_dispatch_list"),
        ]

    @classmethod
    def parse_well_search(cls, well_search_data):
        '''
        Parse a Well search by line into an array of search lines of the form:
        input: 
        - lines separated by a newline char 
        - space or comma separated values, 
        output:
        search_line: {
            plates: [],
            plate_ranges: [],
            wellnames: [],
            well_ids: []
            copies: []
        }
        '''
        # Use unquote to decode form data from a post
        if not isinstance(well_search_data, (list,tuple)):
            well_search_data = urllib.unquote(well_search_data)
        else:
            cleaned_searches = []
            for ps in well_search_data:
                if isinstance(ps, six.string_types):
                    cleaned_searches.append(urllib.unquote(ps))
                else:
                    cleaned_searches.append(ps)
            well_search_data = cleaned_searches    
        
        if DEBUG_WELL_PARSE:
            logger.info('well_search_data: %r', well_search_data)
        
        parsed_searches = []
        errors = []
        
        # Process the patterns by line; or semicolon (to support URL encoded)
        parsed_lines = well_search_data
        if isinstance(parsed_lines, basestring):
            parsed_lines = re.split(
                lims_utils.PLATE_SEARCH_LINE_SPLITTING_PATTERN,parsed_lines)
            if DEBUG_WELL_PARSE:
                logger.info('parsed_lines: %r', parsed_lines)
        
        for _line in parsed_lines:
            _line = _line.strip()
            if not _line:
                continue
            
            parts = lims_utils.QUOTED_WORD_SPLITTING_PATTERN.findall(_line)
            if DEBUG_WELL_PARSE:
                logger.info('parse copywell search: line parts: %r', parts)
            
            parsed_search = defaultdict(list)
            parsed_search['line'] = parts
            for part in parts:
                # unquote
                part = re.sub(r'["\']+','',part)

                if DEBUG_WELL_PARSE:
                    logger.info('test part: %r', part)
                if PLATE_PATTERN.match(part):
                    plate_number = int(part)
                    if DEBUG_WELL_PARSE:
                        logger.info('from PLATE: %r to %d', part, plate_number)
                    parsed_search['plates'].append(plate_number)
                elif PLATE_RANGE_PATTERN.match(part):
                    match = PLATE_RANGE_PATTERN.match(part)
                    plate_range = sorted([
                        int(match.group(1)), int(match.group(2))])
                    if DEBUG_WELL_PARSE:
                        logger.info('from PLATE_RANGE: %r, %r', part, plate_range)
                    parsed_search['plate_ranges'].append(plate_range)
                elif WELL_ID_PATTERN.match(part):
                    parsed_search['well_ids'].append(lims_utils.parse_well_id(part))
                elif WELL_NAME_PATTERN.match(part):
                    match = WELL_NAME_PATTERN.match(part)
                    wellrow = match.group(1).upper()
                    wellcol = match.group(2)
                    wellname = '%s%s' % (wellrow, str(wellcol).zfill(2))
                    if DEBUG_WELL_PARSE:
                        logger.info('from WELL_NAME: %r to %s', part, wellname)
                    parsed_search['wellnames'].append(wellname)
                elif COPY_NAME_PATTERN.match(part):
                    parsed_search['copies'].append(part)
                else:
                    errors.append('part not recognized: %r' % part)
              
            if not errors:
                # Validation rules:
                # Aside from copies, must specify a valid well search, so using the
                # rules from WellResource.parse_well_search
                if 'plates' not in parsed_search \
                    and 'plate_ranges' not in parsed_search \
                    and 'well_ids' not in parsed_search:
                    errors.append(
                        'Must specify either a plate, plate range, or well_id: %r' % _line)
                if 'well_ids' in parsed_search \
                    and ( 'plates' in parsed_search 
                        or 'plate_ranges' in parsed_search):
                    errors.append(
                        'Well ids may not be defined on the same line with plate or '
                        'plate ranges: %r' % _line)
                # Wellname matches require either plate, plate range, or wellid
                if 'wellnames' in parsed_search \
                    and 'plates' not in parsed_search \
                    and 'plate_ranges' not in parsed_search:
                        if 'well_ids' in parsed_search:
                            well_ids = parsed_search['well_ids']
                            if len(well_ids) > 1:
                                errors.append(
                                    'Well names may not be defined with multiple '
                                    'well_ids: %r' % _line)
                            else:
                                match = WELL_ID_PATTERN.match(well_ids[0])
                                plate = int(match.group(1))
                                wellrow = match.group(3).upper()
                                wellcol = match.group(4)
                                wellname = '%s%s' % (wellrow, str(wellcol).zfill(2)) 
                                parsed_search['wellnames'].append(wellname)
                                parsed_search['plates'].append(plate)
                                del parsed_search['well_ids']
                        else:
                            errors.append(
                                'Must specify either a plate, plate range, or well_id'
                                'for wellnames: %r' % _line)
                if DEBUG_WELL_PARSE:
                    logger.info('parsed: %r from %r', parsed_search, _line)
                parsed_searches.append(parsed_search)
        
        if errors:
            raise ValidationError(key=SCHEMA.API_PARAM_SEARCH, msg=', '.join(errors))
        if not parsed_searches:
            raise ValidationError(key=SCHEMA.API_PARAM_SEARCH, msg='no search lines found')
        
        for parsed_search in parsed_searches:
            for key, list_val in parsed_search.items():
                parsed_search[key] = sorted(list_val)

        if DEBUG_WELL_PARSE:
            logger.info('parse_well_search: %r', parsed_searches)
        return parsed_searches
                
    @classmethod
    def create_well_base_query(cls, parsed_searches):
        
        if DEBUG_WELL_PARSE:
            logger.info('create_well_base_query: %r', parsed_searches)

        # Compress searches by copy/plate (to help SQL efficiency)
        pc_searches = defaultdict(list)
        copy_wellname_searches = defaultdict(list)
        well_only_searches = []
        for parsed_search in parsed_searches:
            if DEBUG_WELL_PARSE:
                logger.info('parsed search: %r', parsed_search)
            
            # Get the elements from the parse_well_search output
            plates = parsed_search.get('plates',[])
            plate_ranges = parsed_search.get('plate_ranges',[])
            well_ids = parsed_search.get('well_ids',[])
            well_names = parsed_search.get('wellnames',[])
            copies = parsed_search.get('copies',[])

            # Also include standard filter searches:
            plate_number = parsed_search.get('plate_number')
            if plate_number:
                plates.append(plate_number)
            plate_number_in = parsed_search.get('plate_number__in')
            if plate_number_in:
                plates.extend(plate_number_in)
            well_ids_in = parsed_search.get('well_id__in')
            if well_ids_in:
                well_ids.extend(well_ids_in)
            well_name = parsed_search.get('well_name')
            if well_name:
                well_names.append(well_name)
            well_name_in = parsed_search.get('well_name__in')
            if well_name_in:
                well_names.extend(well_name_in)
            copy = parsed_search.get('copy_name')
            if copy:
                copies.append(copy)
            
            if copies:
                # Convert well_ids to plate/well_names
                if well_ids:
                    for well_id in well_ids:
                        plates.append(lims_utils.well_id_plate_number(well_id))
                        well_names.append(lims_utils.well_id_name(well_id))
                if plates or plate_ranges:
                    plate_copies = []
                    for copy in copies:
                        for plate in plates:
                            plate_copies.append('{}/{}'.format(copy,plate))
                        for plate_range in plate_ranges:
                            for plate in xrange(plate_range[0],plate_range[1]+1):
                                plate_copies.append('{}/{}'.format(copy,plate))
                
                    for plate_copy in plate_copies:
                        if well_names:
                            pc_searches[plate_copy].extend(well_names)
                        else:
                            pc_searches[plate_copy].extend([])
                else:
                    if well_names:
                        copy_wellname_searches[copy].extend(wellnames)
                    else:
                        raise ValidationError(key='copies', msg='Must be specified with plates or wells')
            else:
                well_only_searches.append(parsed_search)
        if DEBUG_WELL_PARSE:
            logger.info('pc_searches: %r', pc_searches)
            logger.info('copy_wellname_searches: %r', copy_wellname_searches)
            logger.info('well_only_searches: %r', well_only_searches)
        
        bridge = get_tables()
        _well = bridge['well']
        _plate = bridge['plate']
        _copy = bridge['copy']

        well_query = (
            select([
                _well.c.well_id,
                _copy.c.copy_id
            ])
            .select_from(
                _well.join(_plate, _well.c.plate_number==_plate.c.plate_number)
                    .join(_copy, _copy.c.copy_id==_plate.c.copy_id))
            .group_by(_well.c.well_id, _copy.c.copy_id))

        clauses = []
        for pc,well_names in pc_searches.items():
            (copy,plate) = pc.split('/')
            if DEBUG_WELL_PARSE:
                logger.info('copywells: finding copy: %r, plate: %r, well_names: %r', 
                    copy, plate, well_names)
            clause = and_(
                _well.c.plate_number==plate,
                _copy.c.name==copy)
            if well_names:
                clause = and_(clause, _well.c.well_name.in_(well_names))
            clauses.append(clause)

        for copy, wellnames in copy_wellname_searches.items():
            clause = and_(
                _copy.c.name==copy,
                _well.c.well_name.in_(well_names))
            clauses.append(clause)
        
        # TODO: could optimize well only searches as in WellResource
        for well_search in well_only_searches:
            logger.info('well only search: %r', well_search)
            well_clause = None
            plates = well_search.get('plates')
            plate_ranges = well_search.get('plate_ranges')
            well_names = well_search.get('wellnames')
            well_ids = well_search.get('well_ids')
            
            if well_ids:
                if well_names:
                    raise ValidationError(key='well_ids',msg='cannot be specified with well names')
                if plates or plate_ranges:
                    raise ValidationError(key='well_ids',msg='cannot be specified with plates or plate ranges')
                well_clause = _well.c.well_id.in_(well_ids)
                
            if well_names:
                if not plates or plate_ranges:
                    raise ValidationError(key='well_names',msg='cannot be specified without plates or plate_ranges')
                well_clause = _well.c.well_name.in_(well_names)
                logger.info('well_names: %r', well_names)
            plate_clauses = []
            if plate_ranges:
                for plate_range in plate_ranges:
                    plate_clauses.append(_well.c.plate_number.between(*plate_range, symmetric=True))
            if plates:
                plate_clauses.append(_well.c.plate_number.in_(plates))
            if plate_clauses:
                if len(plate_clauses) > 1:
                    plate_clauses = or_(*plate_clauses)
                else:
                    plate_clauses = plate_clauses[0]
            
                if well_clause is not None:
                    well_clause = and_(well_clause, plate_clauses)
                else:
                    well_clause = plate_clauses
            
            if well_clause is not None:
                clauses.append(well_clause)
            else:
                raise Exception('well_only_search invalid: %r', well_search)
            
        if not clauses:
            raise ValidationError(key='well_search_data', msg='no searches found')
        if len(clauses) == 1:    
            well_query = well_query.where(clauses[0])
        else:
            well_query = well_query.where(or_(*clauses))

        # compiled_stmt = str(well_query.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('well_query %s', compiled_stmt)
        
        return well_query


    @read_authorization
    def get_detail(self, request, **kwargs):

        library_short_name = kwargs.get('library_short_name', None)
        if not library_short_name:
            logger.info('no library_short_name provided')
         
        copy_name = kwargs.get('copy_name', None)
        if not copy_name:
            raise MissingParam('copy_name')
        
        well_id = kwargs.get('well_id', None)
        if not well_id:
            raise MissingParam('well_id')

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, schema=None, **kwargs):

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        is_for_detail = kwargs.pop('is_for_detail', False)

        if schema is None:
            raise Exception('schema not initialized')
        well_id = param_hash.pop('well_id', None)
        if well_id:
            param_hash['well_id__eq'] = well_id
        copy_name = param_hash.pop('copy_name', None)
        if copy_name:
            param_hash['copy_name__eq'] = copy_name
        library_short_name = param_hash.pop('library_short_name', None)
        if library_short_name:
            param_hash['library_short_name__eq'] = library_short_name
        library_screen_type = param_hash.pop('library_screen_type',None)

        # well search data is raw line based text entered by the user
        well_search_data = param_hash.pop(SCHEMA.API_PARAM_SEARCH, None)
        well_base_query = None
        if well_search_data is not None:
            parsed_searches = self.parse_well_search(well_search_data)
            well_base_query = self.create_well_base_query(parsed_searches)

        parsed_well_search_data = param_hash.pop(SCHEMA.API_PARAM_NESTED_SEARCH, None)
        if parsed_well_search_data is not None:
            if well_base_query is not None:
                raise Exception('May not specify both %r and %r', 
                    SCHEMA.API_PARAM_SEARCH, SCHEMA.API_PARAM_NESTED_SEARCH)
            else:
                well_base_query = \
                    self.create_well_base_query(parsed_well_search_data)

        if well_base_query is not None:
            well_base_query = well_base_query.cte('well_base_query')

        # general setup
          
        manual_field_includes = set(param_hash.get('includes', []))
  
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filename = self._get_filename(
            readable_filter_hash, schema, is_for_detail)
        filter_expression = \
            self._meta.authorization.filter(request.user,filter_expression)

        # TODO: remove this restriction if the query can be optimized
        # if filter_expression is None:
        #     raise InformationError(
        #         key='Input filters ',
        #         msg='Please enter a filter expression to begin')
        # else:
        logger.debug('filters: %r', readable_filter_hash)                      
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
             
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            # use "use_vocab" as a proxy to also adjust siunits for viewing
            rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
                field_hash, rowproxy_generator)
        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)

        # specific setup 
        base_query_tables = [
            'copy_well', 'copy', 'plate', 'well', 'library', 'plate_location']
        
        _cw = self.bridge['copy_well']
        _c = self.bridge['copy']
        _l = self.bridge['library']
        _p = self.bridge['plate']
        _w = self.bridge['well']
        _pl = self.bridge['plate_location']
            

        # TODO: optimize query join order copy-plate, then all-copy-plate-well, 
        # then copy-plate-well to copy_well
        # copy_plate = (
        #     select([
        #         _p.c.plate_id, _c.c.copy_id,
        #         _p.c.plate_number, _c.c.name,
        #         _p.c.status, _c.c.usage_type,
        #         _p.c.well_volume,_p.c.remaining_well_volume,
        #         _p.c.mg_ml_concentration, _p.c.molar_concentration,
        #         _p.c.screening_count, _p.c.cplt_screening_count,
        #         _l.c.short_name ])
        #     .select_from(_p.join(_c, _p.c.copy_id==_c.c.copy_id)
        #         .join(_l, _c.c.library_id==_l.c.library_id))
        #         ).cte('copy_plate')
        # all_copy_wells = (
        #     select([_w.c.well_id]))
        
        custom_columns = {
            'copywell_id': (
                _concat(_c.c.name,'/',_w.c.well_id)),
            'volume': case([
                (_w.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                     func.coalesce(_cw.c.volume, 
                         _p.c.remaining_well_volume, _p.c.well_volume) )],
                else_=None),
            'initial_volume': case([
                (_w.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                     func.coalesce(_cw.c.initial_volume,_p.c.well_volume) )],
                 else_=None),
            'consumed_volume': case([
                (_w.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                    func.coalesce(_cw.c.initial_volume,_p.c.well_volume)-
                        func.coalesce(_cw.c.volume, _p.c.remaining_well_volume) )],
                else_=None),
            'mg_ml_concentration': case([
                (_w.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                    func.coalesce(
                        _cw.c.mg_ml_concentration,_p.c.mg_ml_concentration,
                        _w.c.mg_ml_concentration ) )],
                else_=None),
            'molar_concentration': case([
                (_w.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                    func.coalesce(
                        _cw.c.molar_concentration,_p.c.molar_concentration,
                        _w.c.molar_concentration ) )],
                else_=None),
            'cumulative_freeze_thaw_count': (
                (func.coalesce(_p.c.screening_count,0) 
                    + func.coalesce(_p.c.cplt_screening_count,0))),
            'location': (
                _concat_with_sep(
                    (_pl.c.room,_pl.c.freezer,_pl.c.shelf,_pl.c.bin),'-')
                ),
            
            # 'adjustments': case([
            #     (_w.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
            #         func.coalesce(_cw.c.adjustments, 0) )],
            #     else_=None),
            # Note: the query plan makes this faster than the hash join of 
            # copy-copy_well
            # 'copy_name': literal_column(
            #    '( select copy.name from copy where copy.copy_id=copy_well.copy_id )'
            #    ).label('copy_name')
        }
            
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)

        # build the query statement

        j = join(_w, _l, _w.c.library_id == _l.c.library_id)
        j = j.join(_c, _l.c.library_id == _c.c.library_id)
        j = j.join(_p, and_(
            _p.c.copy_id == _c.c.copy_id,
            _w.c.plate_number == _p.c.plate_number))
        j = j.join(_pl, _p.c.plate_location_id==_pl.c.plate_location_id, isouter=True)
        j = j.join(_cw, and_(
            _cw.c.well_id == _w.c.well_id,
            _cw.c.copy_id == _c.c.copy_id), isouter=True )

        if well_base_query is not None:
            j = j.join(well_base_query, and_(
                _w.c.well_id == well_base_query.c.well_id,
                _c.c.copy_id == well_base_query.c.copy_id))
        
        stmt = select(columns.values()).select_from(j)
        
        if library_screen_type is not None:
            stmt = stmt.where(_l.c.screen_type==library_screen_type)
        # general setup
         
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        
        stmt = stmt.order_by('plate_number', 'well_id','copy_name')
        
        # compiled_stmt = str(stmt.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)
        
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
        
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None))
             
    def put_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_detail')
                
    @write_authorization
    @un_cache        
    @transaction.atomic    
    def delete_obj(self, request, deserialized, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'delete_obj')

    def get_id(self, deserialized, validate=False, schema=None, **kwargs):
        
        id_kwargs = DbApiResource.get_id(
            self, deserialized, validate=validate, schema=schema, **kwargs)
        
        well_id = id_kwargs.get('well_id')
        # For now: well_id overrides other keys;
        # TODO: raise exception if conflicts
        if well_id:
            id_kwargs['plate_number'] = lims_utils.well_id_plate_number(well_id)
            id_kwargs['well_name'] = lims_utils.well_id_name(well_id)
            
        copywell_id = deserialized.get('copywell_id')
        if copywell_id:
            logger.info('copywell_id: %r', copywell_id)
            ( copy_name, plate_number, well_id, well_name) = \
                lims_utils.parse_copywell_id(copywell_id)
            
            # For now: copywell_id overrides other keys;
            # TODO: raise exception if conflicts
            if copy_name:
                id_kwargs['copy_name'] = copy_name
                id_kwargs['plate_number'] = plate_number
                id_kwargs['well_name'] = well_name
            else:
                logger.warn('invalid copywell_id: %r', copywell_id)
                if validate:
                    raise ValidationError(
                        key='copywell_id', 
                        msg='invalid format: %r' % copywell_id)
                    
        return id_kwargs

    def _parse_list_ids(self, deserialized, schema):
        (id_query_params, rows_to_ids) = DbApiResource._parse_list_ids(self, deserialized, schema)
    
        
        if id_query_params:
            id_kwargs_list = id_query_params.get(SCHEMA.API_PARAM_NESTED_SEARCH)
            if not id_kwargs_list:
                raise Exception('Expected \"%s\" in _parse_list_ids' % SCHEMA.API_PARAM_NESTED_SEARCH)
            
            # Group IDS by copy/plate
            
            copy_plate_to_well_names = defaultdict(set)
            for id_kwargs in id_kwargs_list:
                copy_plate = (id_kwargs['copy_name'],id_kwargs['plate_number'])
                copy_plate_to_well_names[copy_plate].add(id_kwargs['well_name'])
            
            nested_search = []
            for (copy_name, plate_number),well_names in copy_plate_to_well_names.items():
                nested_search.append({
                    'copy_name': copy_name, 
                    'plate_number': plate_number,
                    'well_name__in': well_names })
            
            return (
                { SCHEMA.API_PARAM_NESTED_SEARCH: nested_search },
                rows_to_ids )
        else:
            return (id_query_params, rows_to_ids)
    
    @write_authorization
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):
        # TODO: optimize for list inputs (see well.patch)
        logger.debug('patch_obj %s', deserialized)

        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        fields = schema['fields']
        initializer_dict = {}
        id_kwargs = self.get_id(deserialized, schema=schema, validate=True, **kwargs)
        well_id = id_kwargs['well_id']

        try:
            well = Well.objects.get(well_id=well_id)
            library = well.library
        except ObjectDoesNotExist:
            msg = 'well not found: %r' % well_id
            logger.info(msg)
            raise Http404(msg)

        if well.library_well_type != WELL_TYPE.EXPERIMENTAL:
            logger.info('CopyWell patch: ignore non experimental well: %r', well_id)
            return None
        
        copy_name = id_kwargs['copy_name']
        try:
            librarycopy = Copy.objects.get(
                name=copy_name, library=library)
        except ObjectDoesNotExist:
            msg = 'copy_name not found: %r' % copy_name
            logger.info(msg)
            raise Http404(msg)
        
        try:
            plate = Plate.objects.get(
                plate_number=well.plate_number, copy=librarycopy)
        except ObjectDoesNotExist:
            msg = 'plate not found: %r:%r' % (library.short_name,copy_name)
            logger.info(msg)
            raise Http404(msg)

        # TODO: wrapper for parsing
        logger.debug('fields: %r, deserialized: %r', fields.keys(), deserialized)
        for key in fields.keys():
            if deserialized.get(key, None) is not None:
                initializer_dict[key] = parse_val(
                    deserialized.get(key, None), key, fields[key]['data_type']) 
        
        volume = initializer_dict.get('volume')        
        mg_ml_concentration = initializer_dict.get('mg_ml_concentration')
        molar_concentration = initializer_dict.get('molar_concentration')
        cherry_pick_screening_count = initializer_dict.get('cherry_pick_screening_count')
        
        if not any(v is not None for v in 
            [volume, molar_concentration, mg_ml_concentration, cherry_pick_screening_count]):
            msg = (
                'Must submit one of [volume, mg_ml_concentration, '
                'molar_concentration, cherry_pick_screening_count]: well: %r' % well_id)
            raise ValidationError({
                'volume': msg,
                'mg_ml_concentration': msg,
                'molar_concentration': msg,
                'cherry_pick_screening_count': msg
                })
        has_update = False
        try:
            copywell = CopyWell.objects.get(
                well=well, copy=librarycopy)
            
            # FIXME: only "set" adjustments if sent from the user
            # if volume is not None:
            #    copywell.adjustments += 1
        except ObjectDoesNotExist:
            # If creating, check that something is updated from the plate values
            if plate.remaining_well_volume == volume:
                volume = None
            if plate.mg_ml_concentration == mg_ml_concentration:
                mg_ml_concentration = None
            if plate.molar_concentration == molar_concentration:
                molar_concentration = None
            
            if all(v is None for v in 
                [volume,mg_ml_concentration,molar_concentration]):
                logger.debug('Nothing to edit for: %r', deserialized)
                logger.debug('Patch: no changes for %r', id_kwargs)
                return None
            
            copywell = CopyWell.objects.create(
                well=well, copy=librarycopy, plate=plate,
                initial_volume = plate.well_volume)
            copywell.save()
            logger.debug('created cw: %r', id_kwargs)
        
        
        if volume is not None:
            has_update = True
            copywell.volume = volume
        
        # Removed: screening count is tracked only on the plate level, and 
        # adjustment count is simple count of apilogs for volume changes
        # if adjustments is not None:
        #     copywell.adjustments = adjustments
        if mg_ml_concentration is not None:
            has_update = True
            copywell.mg_ml_concentration = mg_ml_concentration
        if molar_concentration is not None:
            has_update = True
            copywell.molar_concentration = molar_concentration
        if cherry_pick_screening_count is not None:
            
            if cherry_pick_screening_count < 0:
                raise ValidationError(
                    key='cherry_pick_screening_count',
                    msg='must be positive')
            
            
            has_update = True
            copywell.cherry_pick_screening_count = cherry_pick_screening_count
        
        if has_update is True:
            copywell.save()
        else:
            return None
        logger.debug('patch_obj done: %r', id_kwargs)
        return { API_RESULT_OBJ: copywell }
    
    @un_cache
    @transaction.atomic
    def _deallocate_well_volumes(
        self, volume, copywells_to_deallocate, parent_log):
        
        copywells_deallocated = set()
        meta = {}
        for copywell in copywells_to_deallocate:
            copy = copywell.copy
            new_volume = copywell.volume + volume
            
            log = self.make_child_log(parent_log)
            log.key = '/'.join([copy.name, copywell.well_id])
            log.uri = '/'.join([
                log.ref_resource_name,'library',copy.library.short_name, log.key])
            log.diffs = {
                'volume': [copywell.volume, new_volume]}
            log.parent_log = parent_log
            logger.info('copywell adjusted: %r, %r', log, log.diffs)
            log.save()
            # adjust volume
            copywell.volume = new_volume
            copywell.save()
            copywells_deallocated.add(copywell)
            
        meta[API_MSG_COPYWELLS_DEALLOCATED] = len(copywells_deallocated)
        return meta
    
    @un_cache
    @transaction.atomic
    def _allocate_well_volumes(
        self, volume, copywells_to_allocate, parent_log):
        
        meta = {}
        copywell_volume_warnings = []
        copywells_allocated = set()
        for copywell in copywells_to_allocate:
            copy = copywell.copy
            key = '/'.join([copy.name, copywell.well_id])
            logger.debug('copywell: %r: vol: %r, requested: %r', 
                copywell, copywell.volume, volume)
            if copywell.volume < volume:
                copywell_volume_warnings.append(
                    'CopyWell: %s, '
                    '(available: %s uL)' 
                        % si_unit.convert_decimal(
                            copywell.volume, 1e-6, 1),
                    '(requested: %s uL)' 
                        % si_unit.convert_decimal(
                            volume, 1e-6, 1))

            new_volume = copywell.volume - volume
            
            log = self.make_child_log(parent_log)
            log.key = key
            # NOTE: make the log uri more robust, with library name as well
            log.uri = '/'.join([
                log.ref_resource_name,'library',copy.library.short_name,log.key])
            log.diffs = {
                'volume': [copywell.volume, new_volume]}
            log.parent_log = parent_log
            logger.info('copywell adjusted: %r, %r', log, log.diffs)
            log.save()
            # adjust volume
            copywell.volume = new_volume
            copywell.save()
            copywells_allocated.add(copywell)
        if copywell_volume_warnings:
            logger.info('%r:%r', 
                API_MSG_LCPS_INSUFFICIENT_VOLUME, copywell_volume_warnings)
            meta[API_MSG_LCPS_INSUFFICIENT_VOLUME] = copywell_volume_warnings or '0'
        meta[API_MSG_COPYWELLS_ALLOCATED] = len(copywells_allocated)
        return meta
            
            
    @un_cache
    @transaction.atomic
    def deallocate_cherry_pick_volumes(
        self, cpr, lab_cherry_picks_to_deallocate, parent_log,
        set_deselected_to_zero=False,
        update_screening_count=True):
        '''
        @param update_screening_count (default True) if false this deallocation
        does not affect the copywell.cherry_pick_screening_count;
        NOTE: if we want to track this, then should create a new 
        "update reservation" method; which will only adjust counts if all wells 
        for a plate are deselected.
        '''
        logger.debug('deallocate_cherry_pick_volumes: %r, %r, %r, %r',
            cpr, lab_cherry_picks_to_deallocate, set_deselected_to_zero, 
            update_screening_count)
        copywells_deallocated = []
        plates_adjusted = set()
        # find copy-wells
        for lcp in lab_cherry_picks_to_deallocate:
            copy = lcp.copy
            if copy is None:
                raise ProgrammingError(
                    'LabCherryPick is already deallocated: %r', lcp)
            plate = Plate.objects.get(
                plate_number=lcp.source_well.plate_number, 
                copy=lcp.copy)
            plates_adjusted.add(plate)
            copywell_id = self.COPYWELL_KEY.format(
                well_id=lcp.source_well.well_id,
                library_short_name=copy.library.short_name,
                copy_name=copy.name)
            logger.info('copywell to deallocate: %r', copywell_id)
            try:
                copywell = CopyWell.objects.get(
                    well=lcp.source_well, copy=lcp.copy)
                
            except ObjectDoesNotExist:
                logger.error(
                    'copywell to deallocate not located: %r', copywell_id)
                
            log = self.make_child_log(parent_log)
            log.key = '/'.join([copy.name, copywell.well_id])
            # NOTE: make the log uri more robust, with library name as well
            log.uri = '/'.join([
                log.ref_resource_name,'library',copy.library.short_name,log.key])
            if set_deselected_to_zero is False:
                new_volume = copywell.volume + cpr.transfer_volume_per_well_approved
            else:
                logger.info('set deallocated copywell to zero: %r', copywell_id)
                new_volume = 0
            log.diffs = {
                'volume': [copywell.volume, new_volume]}
            if update_screening_count is True:
                if copywell.cherry_pick_screening_count > 0:
                    current_cp_screening_count = copywell.cherry_pick_screening_count
                    new_cp_screening_count =  current_cp_screening_count - 1
                    log.diffs['cherry_pick_screening_count'] = [
                        current_cp_screening_count, 
                        new_cp_screening_count]
                    copywell.cherry_pick_screening_count = new_cp_screening_count
            log.parent_log = parent_log
            logger.info('copywell adjusted: %r, %r', log, log.diffs)
            log.save()
            # adjust volume
            copywell.volume = new_volume
            copywell.save()
            copywells_deallocated.append(copywell)
        logger.debug('copywells_deallocated: %r', copywells_deallocated)    
        logger.info('copywells_deallocated: %d', len(copywells_deallocated))    

        if update_screening_count is True:
            logger.debug('plates_adjusted: %r', plates_adjusted)
            logger.info('plates_adjusted: %d', len(plates_adjusted))
            for plate in plates_adjusted:
                plate_log = self.get_plate_resource().make_child_log(parent_log)
                plate_log.key = '/'.join([
                    plate.copy.name, str(plate.plate_number)])
                # NOTE: make the log uri more robust, with library name as well
                plate_log.uri = '/'.join([
                    log.ref_resource_name,'library',copy.library.short_name,log.key])
                if plate.cplt_screening_count < 1:
                    logger.warn(
                        'deallocation: plate: %r, cplt_screening_count already 0',
                        plate_log.key)
                    new_cp_screening_count = 0
                else:
                    new_plate_cplt_screening_count = plate.cplt_screening_count-1
                plate_log.diffs = {
                    'cplt_screening_count': [
                        plate.cplt_screening_count,
                        new_plate_cplt_screening_count ],
                    }
                plate_log.save()
                plate.cplt_screening_count = new_plate_cplt_screening_count
                plate.save()
            
        return { 
            'CPR #': cpr.cherry_pick_request_id,
            API_MSG_COPYWELLS_DEALLOCATED: len(copywells_deallocated),
         }
        
        
    def reserve_cherry_pick_volumes(
        self, cpr, fulfillable_lcps, parent_log, plates_to_ignore=None):
        '''
        @param plates_to_ignore (set) plates to ignore when adjusting the 
        cplt_screening_count
        '''
        copywells_adjusted = []
        plates_adjusted = set()
        # find copy-wells
        for lcp in fulfillable_lcps:
            copy = lcp.copy
            plate = Plate.objects.get(
                plate_number=lcp.source_well.plate_number, 
                copy=lcp.copy)
            plates_adjusted.add(plate)
            try:
                copywell = CopyWell.objects.get(
                    well=lcp.source_well, copy=lcp.copy)
                if copywell.initial_volume is None:
                    # Copywell may have no volumes set, if the copywell was
                    # created to track well-specific concentrations
                    # (the API reports the plate.remaining_well_volume for these)
                    copywell.initial_volume = plate.remaining_well_volume
                    if copywell.volume is not None:
                        logger.warn(
                            'copywell has initial_volume, but not volume: %r/%r, %r', 
                            copy.name, lcp.source_well, copywell.initial_volume)
                    copywell.volume = plate.remaining_well_volume
            except ObjectDoesNotExist:
                # create copy-wells that dne
                try:
                    logger.info('Reserve CPR %r volume: copywell dne: %r/%r, creating', 
                        cpr.cherry_pick_request_id, copy.name, lcp.source_well)
                    logger.info('plate: %r, remaining_well_volume: %r', 
                        plate, plate.remaining_well_volume)
                    copywell = CopyWell.objects.create(
                        well=lcp.source_well, 
                        copy=lcp.copy, 
                        plate=plate,
                        volume=plate.remaining_well_volume,
                        initial_volume=plate.remaining_well_volume)
                    
                except ObjectDoesNotExist:
                    msg = ('plate not found: %r:%r' 
                        % (lcp.source_well.plate_number, lcp.copy.name))
                    logger.warn(msg)
                    raise ValidationError(
                        key='library_plate',
                        msg=msg)
            log = self.make_child_log(parent_log)
            log.key = '/'.join([copy.name, copywell.well_id])
            # NOTE: make the log uri more robust, with library name as well
            log.uri = '/'.join([
                log.ref_resource_name,'library',copy.library.short_name,log.key])
            logger.debug('copywell: %r, volume: %r, cpr.transfer volume: %r', 
                copywell, copywell.volume, cpr.transfer_volume_per_well_approved)
            new_volume = copywell.volume - cpr.transfer_volume_per_well_approved
            current_cp_screening_count = copywell.cherry_pick_screening_count or 0
            new_cp_screening_count =  current_cp_screening_count + 1
            log.diffs = {
                'volume': [copywell.volume, new_volume],
                'cherry_pick_screening_count': [
                    current_cp_screening_count, 
                    new_cp_screening_count]
                }
            log.parent_log = parent_log
            log.save()
            # adjust volume
            copywell.volume = new_volume
            copywell.cherry_pick_screening_count = new_cp_screening_count
            copywell.save()
            copywells_adjusted.append(copywell)
            
        logger.info('plates_adjusted: %r', 
            [(plate.copy.name,plate.plate_number) for plate in plates_adjusted])
        for plate in plates_adjusted:
            
            if plates_to_ignore and plate in plates_to_ignore:
                logger.info('ignoring plate: %r, will not adjust cplt_screening_count',
                    plate)
                continue
            
            plate_log = self.get_plate_resource().make_child_log(parent_log)
            plate_log.key = '/'.join([ 
                plate.copy.name, str(plate.plate_number)])
            # NOTE: make the log uri more robust, with library name as well
            plate_log.uri = '/'.join([
                log.ref_resource_name,'library',copy.library.short_name,log.key])
            new_plate_cplt_screening_count = plate.cplt_screening_count +1
            plate_log.diffs = {
                'cplt_screening_count': [
                    plate.cplt_screening_count,
                    new_plate_cplt_screening_count ],
                }
            plate_log.save()
            plate.cplt_screening_count = new_plate_cplt_screening_count
            plate.save()
            
        return { 
            'CPR #': cpr.cherry_pick_request_id,
            API_MSG_COPYWELLS_ALLOCATED: len(copywells_adjusted),
            API_MSG_LCP_SOURCE_PLATES_ALLOCATED: len(plates_adjusted)
         }

class CherryPickRequestAuthorization(ScreenAuthorization):        

    def _is_resource_authorized(
        self, user, permission_type, **kwargs):
        authorized = \
            super(CherryPickRequestAuthorization, self)\
                ._is_resource_authorized(user, permission_type, **kwargs)
        if authorized is True:
            return True
        
        return user.is_active

    def filter(self, user, filter_expression):
        
        if self.is_restricted_view(user) is False:
            return filter_expression
        
        screensaver_user = ScreensaverUser.objects.get(username=user.username)
        my_screens = self.get_user_screens(screensaver_user)
        # Can only see own screens
        
        auth_filter = column('screen_facility_id').in_(
            [screen.facility_id for screen in my_screens])
        if filter_expression is not None:
            filter_expression = and_(filter_expression, auth_filter)
        else:
            filter_expression = auth_filter

        return filter_expression

    def get_row_property_generator(self, user, fields, extant_generator):
        # If the user may see the CPR, there are no property restrictions
        return extant_generator
    
    def has_cherry_pick_read_authorization(self, user, cherry_pick_request_id):
        if self.is_restricted_view(user) is False:
            return True
        
        screensaver_user = ScreensaverUser.objects.get(username=user.username)
        my_screens = self.get_user_screens(screensaver_user)
        
        cpr = CherryPickRequest.objects.get(cherry_pick_request_id=cherry_pick_request_id)
        
        return cpr.screen in my_screens
        
class CherryPickRequestResource(DbApiResource):        
    
    class Meta:
    
        queryset = CherryPickRequest.objects.all().order_by('well_id')
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'cherrypickrequest'
        authorization = CherryPickRequestAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        always_return_data = True 

    def __init__(self, **kwargs):

        super(CherryPickRequestResource, self).__init__(**kwargs)
        
        self.well_resource = None
        self.screen_resource = None
        self.copywell_resource = None
        self.labcherrypick_resource = None
        self.screenercherrypick_resource = None
        self.cpp_resource = None
        self.screensaver_user_resource = None
        self.librarycopyplate_resource = None
        
    def get_librarycopyplate_resource(self):
        if self.librarycopyplate_resource is None:
            self.librarycopyplate_resource = LibraryCopyPlateResource()
        return self.librarycopyplate_resource
    
    def get_screen_resource(self):
        if self.screen_resource is None:
            self.screen_resource = ScreenResource()
        return self.screen_resource
    
    def get_user_resource(self):
        if self.screensaver_user_resource is None:
            self.screensaver_user_resource = ScreensaverUserResource()
        return self.screensaver_user_resource
        
    def get_copywell_resource(self):
        if self.copywell_resource is None:
            self.copywell_resource = CopyWellResource()
        return self.copywell_resource
    
    def get_labcherrypick_resource(self):
        if self.labcherrypick_resource is None:
            self.labcherrypick_resource = LabCherryPickResource()
        return self.labcherrypick_resource
    
    def get_screenercherrypick_resource(self):
        if self.screenercherrypick_resource is None:
            self.screenercherrypick_resource = ScreenerCherryPickResource()
        return self.screenercherrypick_resource
    
    def get_cherrypickplate_resource(self):
        if self.cpp_resource is None:
            self.cpp_resource = CherryPickPlateResource()
        return self.cpp_resource
    
    def clear_cache(self, request, **kwargs):
        logger.info('clear_cache: CherryPickRequestResource...')
        DbApiResource.clear_cache(self, request, **kwargs)
        # NOTE: don't clear dependent resources to avoid circular refererence 
        # recursion; DbApiResource.clear_cache will clear all caches, for now
        # self.get_labcherrypick_resource().clear_cache()
        # self.get_screenercherrypick_resource().clear_cache()
        self.get_screen_resource().clear_cache(request, **kwargs)
        
    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            
            url((r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)" 
                 r"/screener_cherry_pick%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screener_cherry_pick_view'),
                name="api_dispatch_screener_cherry_pick_view"),
            url((r"^(?P<resource_name>%s)/(?P<cherry_pick_request_id>[\d]+)"
                 r"/screener_cherry_pick/schema%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_scp_schema'),
                name="api_get_scp_schema"),

            url((r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)" 
                 r"/warnings%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_cpr_warnings'),
                name="api_dispatch_cpr_warnings_view"),
            
            url((r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)" 
                 r"/lab_cherry_pick%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_lab_cherry_pick_view'),
                name="api_dispatch_lab_cherry_pick_view"),
            url((r"^(?P<resource_name>%s)/(?P<cherry_pick_request_id>[\d]+)"
                 r"/lab_cherry_pick/schema%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_lcp_schema'),
                name="api_get_lcp_schema"),
            url((r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)" 
                 r"/source_plate%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_source_plate_view'),
                name="api_dispatch_source_plate_view"),

            url((r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)" 
                 r"/cherry_pick_plate%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_cherry_pick_plate_view'),
                name="api_dispatch_cherry_pick_plate_view"),
            
            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)/set_lab_cherry_picks%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_set_lab_cherry_picks'), 
                name="api_dispatch_set_lab_cherry_picks"),

            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)/set_duplex_lab_cherry_picks%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_set_duplex_lab_cherry_picks'), 
                name="api_dispatch_set_duplex_lab_cherry_picks"),

            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)/reserve_map_lab_cherry_picks%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_reserve_map_lab_cherry_picks'), 
                name="api_dispatch_reserve_map_lab_cherry_picks"),
                
            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)/cancel_reservation%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_cancel_reservation'), 
                name="api_dispatch_cancel_reservation"),
                
            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)/delete_lab_cherry_picks%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_delete_lab_cherry_picks'), 
                name="api_delete_lab_cherry_picks"),
                
            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)"
                r"/plate_mapping_file%s$"
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view(
                    'get_plate_mapping_file'), 
                    name="api_get_plate_mapping_file"),

            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)"
                r"/lab_cherry_pick_plating%s$"
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view(
                    'dispatch_lab_cherry_pick_plating'), 
                    name="api_dispatch_lab_cherry_pick_plating"),
            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)"
                r"/lab_cherry_pick_plating/schema%s$"
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view(
                    'get_lab_cherry_pick_plating_schema'), 
                    name="get_get_lab_cherry_pick_plating_schema"),
        ]
    
    def dispatch_source_plate_view(self, request, **kwargs):

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        manual_field_includes = set(param_hash.get('includes', []))
        if ('-copy_usage_type' not in manual_field_includes):
            manual_field_includes.add('copy_usage_type')
        kwargs['includes']=manual_field_includes
        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        kwargs['schema'] = \
            self.get_librarycopyplate_resource().build_schema(user=request.user)
        # NOTE: authorization is performed in LibraryCopyPlateResource
        return self.get_librarycopyplate_resource()\
            .build_list_response(request, **kwargs)
        
    @read_authorization
    def get_plate_mapping_file(self, request, **kwargs):
        
        request_method = request.method.lower()
        if request_method != 'get':
            raise BadRequestError(key='method', msg='Only GET is allowed')

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        schema = super(CherryPickRequestResource, self)\
            .build_schema(user=request.user)

        cpr_id = param_hash['cherry_pick_request_id']
        # add custom authorization for screening users
        if self._meta.authorization.has_cherry_pick_read_authorization(
                request.user, cpr_id) is False:
            raise PermissionDenied
         
        logger.info('get the cpr: %r', cpr_id) 
        cpr_obj = CherryPickRequest.objects.get(
            cherry_pick_request_id=cpr_id)
        cpr = self._get_detail_response_internal(**kwargs)
        
        logger.info('get the cpps: %r', cpr_id) 
        cpps = self.get_cherrypickplate_resource()._get_list_response_internal(
            **{
                'cherry_pick_request_id': cpr_id,
                'visibilities': ['l','d']
            })
        cpps = {cpp['plate_ordinal']:cpp for cpp in cpps }
        
        lcp_schema = self.get_labcherrypick_resource().build_schema(
            user=request.user, library_classification=cpr_obj.screen.screen_type)
        columns_to_include = [
            'library_plate','source_copy_name','source_well_name',
            'source_plate_type','destination_well','destination_plate_type']

        logger.info('get the lcps: %r', cpr_id) 
        lcps = self.get_labcherrypick_resource()._get_list_response_internal(
            **{
                'cherry_pick_request_id': cpr_id,
                'destination_well__is_null': False,
                'includes': [
                    'source_plate_type','destination_plate_type',
                    'location','-structure_image','-molfile',
                    '-library_plate_comment_array'],
                'order_by': ['destination_well']
            })
        plate_name_to_types = defaultdict(set)
        source_plate_to_dest_plates = defaultdict(set)
        plate_name_template = (
            '{requested_by_name}'
            ' ({screen_facility_id})'
            ' CP{cherry_pick_request_id}'
            '_plate_{{cherry_pick_plate_number}}'
            '_of_{number_plates}'
            ).format(**cpr)
        for lcp in lcps:
            plate_name = plate_name_template.format(**lcp)
            plate_name_to_types[plate_name].add(lcp['source_plate_type'])
            plate_copy = '%s/%s' % (
                str(lcp['library_plate']),lcp['source_copy_name'])
            source_plate_to_dest_plates[plate_copy].add(plate_name)
        plate_map = defaultdict(list)
        location_map = {}
        for lcp in lcps:
            plate_name = plate_name_template.format(**lcp)
            if len(plate_name_to_types[plate_name])>1:
                plate_name += ' ' + lcp['source_plate_type']
            plate_map[plate_name].append(lcp)    
            plate_copy = '%s/%s' % (
                str(lcp['library_plate']),lcp['source_copy_name'])
            location_map[plate_copy] = [
                lcp['library_plate'],lcp['source_copy_name'],lcp['location']]
        # Sort internally
        for plate_name in plate_map.keys():
            lcps = plate_map[plate_name]
            lcps = sorted(lcps, key=lambda lcp: lcp['source_well_id'])
            plate_map[plate_name] = lcps
        
        locations = [location_map[x] for x in sorted(location_map.keys())]
        
        vocabularies = DbApiResource.get_vocabularies(lcp_schema['fields'])
        def vocab_function(key,val):
            if key in vocabularies:
                return vocabularies[key][val]['title']
            else:
                return val
        def title_function(key):
            return lcp_schema['fields'][key]['title']

        extra_values = OrderedDict((
            ('Person Visiting', cpr['requested_by_name']),
            ('Screen Number', cpr['screen_facility_id']),
            (u'Volume (uL)', 
                Decimal(cpr['transfer_volume_per_well_approved']) * Decimal(1e6)),
            # NOTE: UTF-8 encodings are supported, but avoid using them because
            # Excel has poor support for reading properly opening encoded csv files
            # (u'Volume (\u00B5L)', 
            #     Decimal(cpr['transfer_volume_per_well_approved']) * Decimal(1e6)),
        ))
        
        # Outputs:
        # Zip file:
        # - assay plate files as: Jen Smith (729) CP44451  Plate 01 of 1 (Run1)
        # - plate locations file
        # - README.txt
        
        readme_types_warning = '\n'.join([
            'WARNING: Some cherry pick plates will be created from multiple ',
            'source plates of non-uniform plate types!',
            'The following cherry pick plates are specified across multiple files:',])

        reload_plates_warning = '\n'.join([
            'WARNING: Some cherry pick plates will be created from the same source plate!',
            'You will need to reload one or more source plates for each of the ',
            'following cherry pick plates:',])
        reload_plate_msg = 'Cherry pick plate %s requires reload of source plate: %s'

        
        readme_text = [
            'This zip file contains plate mappings for Cherry Pick Request %r' 
            % str(cpr_id) ]
        readme_text.append('Cherry pick plates:')
        plating_text = '{{filename}} Plated ({plating_date} by {plated_by_name})'
        # Open with delete=False; file will be closed and deleted 
        # by the FileWrapper1 instance when streaming is finished.
        zip_dir_name = \
            'screen_{screen_facility_id}_cp_{cherry_pick_request_id}'.format(**cpr)
        with  NamedTemporaryFile(delete=False) as temp_file:
            with ZipFile(temp_file, 'w') as zipfile:
                    
                # Plate maps
                for plate_name,lcps in plate_map.items():
                    raw_data = cStringIO.StringIO()
                    writer = unicodecsv.writer(
                        raw_data, encoding='utf-8',lineterminator='\r\n')
                    for i,lcp in enumerate(lcps):
                        if i==0:
                            title_row = [title_function(x) for x in columns_to_include]
                            title_row.extend(extra_values.keys())
                            writer.writerow(title_row)
                        values = []
                        for key in columns_to_include:
                            values.append(vocab_function(key, lcp[key]))
                        values.extend(extra_values.values())
                        writer.writerow(values)
                    filename = '%s.csv' % plate_name
                    logger.info('write; %r', plate_name)
                    cherry_pick_plate_number = lcps[0]['cherry_pick_plate_number']
                    cpp = cpps[cherry_pick_plate_number]
                    if cpp['plating_date']:
                        _text = plating_text.format(**cpp)
                        _text = _text.format(filename=filename)
                        readme_text.append(_text)
                    else:
                        readme_text.append(filename)
                    
                    zipi= ZipInfo()
                    # give full access to included file:
                    # NOTE: second high bit is used in external_attr (4 bytes)
                    zipi.external_attr = 0777 << 16L 
                    zipi.filename= '%s/%s' % (zip_dir_name,filename)
                    zipfile.writestr(zipi, raw_data.getvalue())
                
                # Location map
                raw_data = cStringIO.StringIO()
                writer = unicodecsv.writer(raw_data)
                writer.writerow(['Source Plate','Source Copy','Location'])
                for location in locations:
                    writer.writerow(location)
                zipi= ZipInfo()
                zipi.filename= '%s/%s' % (zip_dir_name,'plate-copy-location.csv')
                zipi.external_attr = 0777 << 16L
                zipfile.writestr(zipi, raw_data.getvalue())
                
                # Readme
                extra_plate_messages = []
                for plate_name,types in plate_name_to_types.items():
                    if len(types)>1:
                        extra_plate_messages.append(plate_name)
                if extra_plate_messages:
                    logger.info('extra_plate_messages: %r', extra_plate_messages)
                    extra_plate_messages.insert(0,readme_types_warning)
                    readme_text.append('\n')
                    readme_text.extend(extra_plate_messages)
                
                split_plate_messages = []
                dest_plates_to_source_plates = defaultdict(set)
                for plate_copy,dest_plates in source_plate_to_dest_plates.items():
                    if len(dest_plates) > 1:
                        for dest_plate in dest_plates:
                            logger.info('dest_plate: %r, has source plate: %r',
                                dest_plate,plate_copy)
                            dest_plates_to_source_plates[dest_plate].add(plate_copy)
                for dest_plate in sorted(dest_plates_to_source_plates.keys()):
                    source_plates = dest_plates_to_source_plates[dest_plate]
                    for source_plate in source_plates:
                        split_plate_messages.append(
                            reload_plate_msg % (dest_plate,source_plate))
                if split_plate_messages:
                    logger.info('split_plate_messages: %r', split_plate_messages)
                    split_plate_messages.insert(0,reload_plates_warning)
                    readme_text.append('\n')
                    readme_text.extend(split_plate_messages)
                            
                zipi= ZipInfo()
                zipi.filename= '%s/%s' % (zip_dir_name,'readme.txt')
                zipi.external_attr = 0777 << 16L
                zipfile.writestr(zipi, '\n'.join(readme_text))
            logger.info('wrote file %r', temp_file)
        
            temp_file.seek(0, os.SEEK_END)
            size = temp_file.tell()
            temp_file.seek(0)   
        
        filename = 'testCPR%d_plating_file.zip' % cpr['cherry_pick_request_id']
        logger.info('download zip file: %r',filename)
        _file = file(temp_file.name)
        response = StreamingHttpResponse(FileWrapper1(_file)) 
        response['Content-Length'] = size
        response['Content-Type'] = '%s; charset=utf-8' % ZIP_MIMETYPE
        response['Content-Disposition'] = \
            'attachment; filename=%s' % filename
        return response
        
    # TODO: implement the plate mapping using standard resource endpoints for 
    # plate_mapping view (of lab cherry pick) and plate location view (of cpr: tbi)        
    #         kwargs = {}
    #         kwargs['limit'] = 0
    #         kwargs['cherry_pick_request_id'] = cpr_id
    #         kwargs['includes'] = columns_to_include
    #                 for cherry_pick_plate_number in range(1,cpr['number_plates']+1):
    #                     kwargs['cherry_pick_plate_number'] = cherry_pick_plate_number
    #                     response = self.get_labcherrypick_resource().get_list(
    #                         request,
    #                         format='csv',
    #                         **kwargs)
    #                     logger.debug('response: %r', response)
    #                     
    #                     name = ((
    #                         'CP{{cherry_pick_request_id}}'
    #                         '_Screen{{screen_facility_id}}'
    #                         '_plate_{cherry_pick_plate_number}'
    #                         '_of_{{number_plates}}_{{requested_by_username}}.csv'
    #                         ).format(cherry_pick_plate_number=cherry_pick_plate_number)
    #                         .format(**cpr))
    #                     zipfile.writestr(name, LimsSerializer.get_content(response))

    def get_scp_schema(self, request, **kwargs):
        return self.get_screenercherrypick_resource().get_schema(request, **kwargs)    

    def get_lcp_schema(self, request, **kwargs):
        return self.get_labcherrypick_resource().get_schema(request, **kwargs)    
        
    @read_authorization
    def dispatch_screener_cherry_pick_view(self, request, **kwargs):
        return self.get_screenercherrypick_resource().dispatch('list', request, **kwargs)
    
    @read_authorization
    def dispatch_lab_cherry_pick_view(self, request, **kwargs):
        return self.get_labcherrypick_resource().dispatch('list', request, **kwargs)    

    @read_authorization
    def dispatch_lab_cherry_pick_plating(self, request, **kwargs):
        ''' 
        Show the lab cherry pick view after plating
        (Method used by the UI)
        - modify schema and colums to show plate-mapping fields 
        { "cherry_pick_assay_plate", "destination_well", etc. }
        - modify field ordering
        - filter: status=='plated' (TODO: implement "show_all")
        '''

        schema = self.get_labcherrypick_resource()\
            .build_lab_cherry_pick_plating_schema(request.user, **kwargs)
        kwargs['plating_schema'] = schema
        
        param_hash = self._convert_request_to_dict(request)

        show_copy_wells = parse_val(
            param_hash.get(API_PARAM_SHOW_COPY_WELLS, False),
            API_PARAM_SHOW_COPY_WELLS, 'boolean')
        logger.info('%r: %r', API_PARAM_SHOW_COPY_WELLS,show_copy_wells)
        show_available_and_retired_copy_wells = parse_val(
            param_hash.get(API_PARAM_SHOW_RETIRED_COPY_WELlS, False),
            API_PARAM_SHOW_RETIRED_COPY_WELlS, 'boolean')
        logger.info('%r: %r', 
            API_PARAM_SHOW_RETIRED_COPY_WELlS,show_available_and_retired_copy_wells)
        show_unfulfilled = parse_val(
            param_hash.get(API_PARAM_SHOW_UNFULFILLED, False),
            API_PARAM_SHOW_UNFULFILLED, 'boolean')
        if not any([show_copy_wells, show_available_and_retired_copy_wells, 
            show_unfulfilled]):
            kwargs['status__eq'] = 'plated'
            kwargs['order_by'] = param_hash.get(
                'order_by', ['cherry_pick_plate_number','destination_well'])
            kwargs['includes'] = param_hash.get(
                'includes', ['-library_plate_comment_array'])
        return self.get_labcherrypick_resource()\
            .dispatch('list',request, **kwargs)    

    @read_authorization
    def get_lab_cherry_pick_plating_schema(self, request, **kwargs):
        return self.get_labcherrypick_resource()\
            .get_lab_cherry_pick_plating_schema(request, **kwargs)    

    def dispatch_cherry_pick_plate_view(self, request, **kwargs):
        return self.get_cherrypickplate_resource()\
            .dispatch('list', request, **kwargs)    

    @read_authorization
    def get_detail(self, request, **kwargs):

        cherry_pick_request_id = kwargs.get('cherry_pick_request_id', None)
        if not cherry_pick_request_id:
            raise Http404('must provide a cherry_pick_request_id parameter')
        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    @read_authorization
    def dispatch_cpr_warnings(self, request, **kwargs):

        cpr_id = kwargs.get('cherry_pick_request_id', None)
        if not cpr_id:
            raise MissingParam('cherry_pick_request_id')
        
        self.is_authenticated(request)

        resource_name = kwargs.pop('resource_name', self._meta.resource_name)
        authorized = self._meta.authorization._is_resource_authorized(
            request.user, 'read', **kwargs)
        if authorized is not True:
            raise PermissionDenied
        
        if request.method.lower() != 'get':
            raise ApiNotImplemented(self._meta.resource_name, '[method]_cpr_warnings')
        
        warnings = self.get_warnings(cpr_id)
        
        return self.build_response(
            request, {API_RESULT_META: warnings }, response_class=HttpResponse, **kwargs)
            
    def get_warnings(self, cpr_id):    
        
        # Get warning fields of cpr
        logger.info('get warnings for cherry pick request: %r', cpr_id)
        warning_fields = ['screen_type','screener_picks_not_screened',
            'duplicate_screener_cherry_picks','deprecated_picks',
            'restricted_libraries','cherry_pick_allowance',
            'screener_cherry_pick_count_cumulative','screener_cherry_picks',
            'screened_experimental_well_count'
        ];
        
        cpr_data = self._get_detail_response_internal(**{
            'cherry_pick_request_id': str(cpr_id),
            'exact_fields': warning_fields
            })

        logger.info('cpr warning data retrieved..')
        
        if not cpr_data or not cpr_data['screener_cherry_picks']:
            logger.info('no warnings, cherry pick has not begun...')
            return {}
        
        meta = cpr_data.copy()
        del meta['screener_cherry_picks']

        # Parse library restrictions
        # if meta['restricted_libraries']:
        #     vocab_scope = 'library.screening_status'
        #     library_screening_status_vocab = \
        #         self.get_vocab_resource()._get_vocabularies_by_scope(vocab_scope)
        #     
        #     if not library_screening_status_vocab:
        #         logger.warn('no vocabulary found for scope: %r',vocab_scope)
        #     else:
        #         temp = defaultdict(list)
        #         restricted_libraries = meta['restricted_libraries']
        #         for restriction in restricted_libraries:
        #             (vocab,short_name) = restriction.split(':')
        #             if vocab in library_screening_status_vocab:
        #                 temp[library_screening_status_vocab[vocab]['title']].append(short_name)
        #             else:
        #                 temp[vocab].append(short_name)
        #                 
        #         meta['restricted_libraries'] = temp
        
        # Check the Cherrry Pick Allowance
        
        cumulative_count = cpr_data.get('screener_cherry_pick_count_cumulative',0)
        allowance = cpr_data.get('cherry_pick_allowance',0)
        
        allow_percent = APP_PUBLIC_DATA.small_molecule_cherry_pick_ratio_allowed
        if cpr_data['screen_type'] == VOCAB.screen.screen_type.RNAI:
            allow_percent = APP_PUBLIC_DATA.rnai_cherry_pick_ratio_allowed
        allow_percent = float(si_unit.convert_decimal(allow_percent,1,3,100))
        meta['cherry_pick_allowance_percent'] = allow_percent
        if cumulative_count > allowance:
              meta['cherry_pick_allowance_warning'] = (
                  'The cumulative number of cherry picks requested for this screen: '
                  '{} exceeds the allowed number: {} '
                  '({}% of experimental wells screened).').format(
                      cumulative_count, allowance, allow_percent)
        
        # Find plates having no available cpr copy
        
        plates_required = defaultdict(set)
        for scp_well in cpr_data['screener_cherry_picks']:
            plates_required[lims_utils.well_id_plate_number(scp_well)].add(scp_well)
        logger.info('plates_required: %r', plates_required.keys())
        
        with get_engine().connect() as conn:
            _plate = self.bridge['plate']
            _copy = self.bridge['copy']
            temp = (
                select([distinct(_plate.c.plate_number)])
                .select_from(_plate.join(_copy, _plate.c.copy_id==_copy.c.copy_id))
                .where(_plate.c.plate_number.in_(plates_required.keys()))
                .where(_plate.c.status == VOCAB.plate.status.AVAILABLE)
                .where(_copy.c.usage_type == VOCAB.copy.usage_type.CHERRY_PICK_SOURCE_PLATES)
                )
            plates_available = set([x[0] for x in conn.execute(temp)])
            logger.info('plates_available: %r', plates_available)
            meta['copyplates_not_found'] = {plate_number:list(scps) for plate_number, scps 
                            in plates_required.items()
                            if plate_number not in plates_available }
            
        # TODO: copyplate resource is slow here...
        # copy_plates_available = self.get_librarycopyplate_resource()\
        #     ._get_list_response_internal(**{
        #         'plate_number__in': plates_required.keys(),
        #         'status__in': [ VOCAB.plate.status.AVAILABLE ],
        #         'copy_usage_type__eq': VOCAB.copy.usage_type.CHERRY_PICK_SOURCE_PLATES,
        #         'exact_fields': ['plate_number','status','copy_usage_type']
        #     })
        # copy_plates_available = set([cp['plate_number'] for cp in copy_plates_available])
        # logger.info('copyplates_available: %r', copy_plates_available)
        # meta['copyplates_not_found'] = {plate_number:list(scps) for plate_number, scps 
        #                     in plates_required.items()
        #                     if plate_number not in copy_plates_available }
        
        # Check for other wells with the same reagent that have higher concentrations
        
        logger.info('cpr warnings: find alternates...')
        scp_alternates = self.get_screenercherrypick_resource()\
            ._get_list_response_internal(
                **{
                    'cherry_pick_request_id': cpr_id,
                    'show_other_reagents': True,
                    'includes': ['molar_concentration','mg_ml_concentration','library_short_name']
                })
        scp_selected_map = { scp['searched_well_id']:scp for scp
            in scp_alternates if scp['selected'] == True }
        scp_alt_map = defaultdict(list)
        for scp in scp_alternates:
            selected_well_id = scp['searched_well_id']
            selected_library = scp_selected_map[selected_well_id]['library_short_name']
            if scp['selected'] is not True and scp['library_short_name']==selected_library:
                scp_alt_map[scp['searched_well_id']].append(scp)
        scp_warn_map = defaultdict(list)
        for searched_well_id,alt_list in  scp_alt_map.items():
            scp_selected = scp_selected_map.get(searched_well_id)
            if scp_selected:
                molar_concentration = scp_selected.get('molar_concentration')
                mg_ml_concentration = scp_selected.get('mg_ml_concentration')
                library_short_name = scp_selected.get('library_short_name')
                if molar_concentration:
                    best_alt = sorted(alt_list, reverse=True,
                                      key=lambda x: x['molar_concentration'])[0]
                    alt_library = best_alt['library_short_name']
                    logger.debug('scp: %r, best_alt: %r', searched_well_id, best_alt)
                    if library_short_name == alt_library \
                        and best_alt['molar_concentration'] > molar_concentration:
                        scp_warn_map[searched_well_id] = {
                            'library_short_name': library_short_name,
                            'selected_concentration': u'{} {}M'.format(
                                si_unit.convert_decimal(
                                    molar_concentration,1e-3, 3),
                                si_unit.get_siunit_symbol(1e-3)),
                            'alternate_well': best_alt['screened_well_id'],
                            'alternate_concentration':  u'{} {}M'.format(
                                si_unit.convert_decimal(
                                    best_alt['molar_concentration'],1e-3, 3),
                                si_unit.get_siunit_symbol(1e-3)),
                            }
                elif mg_ml_concentration:
                    best_alt = sorted(alt_list, reverse=True,
                                      key=lambda x: x['mg_ml_concentration'])[0]
                    alt_library = best_alt['library_short_name']
                    if library_short_name == alt_library \
                        and best_alt['mg_ml_concentration'] > mg_ml_concentration:
                        scp_warn_map[searched_well_id] = {
                            'library_short_name': library_short_name,
                            'selected_concentration': u'{} mg/ml'.format(
                                si_unit.convert_decimal(
                                    mg_ml_concentration,1, 3)),
                            'alternate_well': best_alt['screened_well_id'],
                            'alternate_concentration':  u'{} mg/ml'.format(
                                si_unit.convert_decimal(
                                    best_alt['mg_ml_concentration'],1, 3)),
                            }
                else:
                    logger.warn('no concentration for selected well: %r', searched_well_id)
        if scp_warn_map:
            meta[API_MSG_CPR_CONCENTRATIONS] = scp_warn_map
        
        logger.info('cpr warnings done')
        
        return meta
    
    def build_list_response(self, request, schema=None, **kwargs):
        
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        
        is_for_detail = kwargs.pop('is_for_detail', False)
        if schema is None:
            raise Exception('schema not initialized')
        cherry_pick_request_id = param_hash.pop('cherry_pick_request_id', None)
        if cherry_pick_request_id:
            param_hash['cherry_pick_request_id__eq'] = cherry_pick_request_id
        if 'cherry_pick_request_id__eq' in param_hash:
            cherry_pick_request_id = param_hash.get('cherry_pick_request_id__eq')
        
        # general setup
      
        manual_field_includes = set(param_hash.get('includes', []))
        manual_field_includes.add('has_pool_screener_cherry_picks')
        manual_field_includes.add('has_alternate_screener_cherry_pick_selections')
        
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filename = self._get_filename(
            readable_filter_hash, schema, is_for_detail)
        
        filter_expression = \
            self._meta.authorization.filter(request.user,filter_expression)
        
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
         
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            # use "use_vocab" as a proxy to also adjust siunits for viewing
            rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
                field_hash, rowproxy_generator)

        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)
                
        # specific setup 
        base_query_tables = ['cherry_pick_request', 'screen', 'screen_result']
        _cpr = self.bridge['cherry_pick_request']
        _screen = self.bridge['screen']
        _sr = self.bridge['screen_result']
        _su = self.bridge['screensaver_user']
        _lhsu = _su.alias('lhsu')
        _scp = self.bridge['screener_cherry_pick']
        _sirna_pool_duplexes = self.bridge['silencing_reagent_duplex_wells']
        _well = self.bridge['well']
        _library = self.bridge['library']
        _lcp = self.bridge['lab_cherry_pick']
        _cpp = self.bridge['cherry_pick_assay_plate']
        _cpp2 = _cpp.alias('cpp2')
                
        _user_cte = ScreensaverUserResource.get_user_cte().cte('users_cpr')
        _requestors = _user_cte.alias('requestors')
        _approvers = _user_cte.alias('approvers')
        _lead_screeners = _user_cte.alias('lead_screeners')
        lab_head_table = ScreensaverUserResource.get_lab_head_cte().cte('lab_heads')
        
        _lcp_subquery = (
            select([
                _lcp.c.cherry_pick_request_id,
                func.count().label('lcp_count'),
                func.count(_lcp.c.copy_id).label('lcp_fullfilled_count')])
            .select_from(_lcp)
            .group_by(_lcp.c.cherry_pick_request_id)).cte('lcp_subquery')
        
        # Duplicate Screener Cherry Picks subquery
        _cpr2 = _cpr.alias('cpr2')
        _other_cprs = (
            select([
                _cpr.c.cherry_pick_request_id,
                (select([func.array_agg(_cpr2.c.cherry_pick_request_id)])
                    .select_from(_cpr2)
                    .where(_cpr2.c.screen_id==_cpr.c.screen_id)
                    .where(_cpr2.c.cherry_pick_request_id!=_cpr.c.cherry_pick_request_id)
                ).label('other_cpr_ids')])
            .select_from(_cpr)
            ).cte('other_cprs')
        _scp2 = _scp.alias('scp2')    
        _other_scps = (
            select([
                _other_cprs.c.cherry_pick_request_id,
                _other_cprs.c.other_cpr_ids,
                _scp.c.screened_well_id])
            .select_from(_other_cprs)
            .where(_scp.c.cherry_pick_request_id==_other_cprs.c.cherry_pick_request_id)
            # TODO: replace with "any_()" from sqlalchemy 1.1 when avail
            .where(_scp2.c.cherry_pick_request_id
                   == text('any(other_cprs.other_cpr_ids)'))
            .where(_scp2.c.screened_well_id
                   == _scp.c.screened_well_id)
            .order_by(_other_cprs.c.cherry_pick_request_id)
            .order_by(_scp.c.screened_well_id)
            ).cte('other_scps')
        
        # Deprecated subqueries
        _deprecated_picks = (union(
            select([
                _scp.c.cherry_pick_request_id,
                _scp.c.screened_well_id.label('well_id'),
                _well.c.deprecation_reason
                ])
            .select_from(_scp.join(_well, _scp.c.screened_well_id==_well.c.well_id))
            .where(_well.c.is_deprecated==True)
            .where(_scp.c.selected),
            select([
                _lcp.c.cherry_pick_request_id,
                _lcp.c.source_well_id.label('well_id'),
                _well.c.deprecation_reason
                ])
            .select_from(_lcp.join(_well, _lcp.c.source_well_id==_well.c.well_id))
            .where(_well.c.is_deprecated==True)
            ).order_by(text('well_id'))).cte('deprecated_picks')
        _deprecations = (
            select([
                _deprecated_picks.c.cherry_pick_request_id,
                _deprecated_picks.c.deprecation_reason,
                func.array_to_string(func.array_agg(
                    _deprecated_picks.c.well_id),', ').label('well_ids')
                ])
            .select_from(_deprecated_picks)
            .group_by(
                _deprecated_picks.c.cherry_pick_request_id,
                _deprecated_picks.c.deprecation_reason)
            .order_by(
                _deprecated_picks.c.cherry_pick_request_id,
                _deprecated_picks.c.deprecation_reason)
            ).cte('deprecations')
        
        # restricted_library subqueries
        _libraries = (union(
            select([
                _scp.c.cherry_pick_request_id,
                _library.c.library_id])
            .select_from(_scp
                .join(_well, _scp.c.screened_well_id==_well.c.well_id)
                .join(_library,_well.c.library_id==_library.c.library_id))
            .where(_scp.c.selected)
            .group_by(_scp.c.cherry_pick_request_id, _library.c.library_id),
            select([
                _lcp.c.cherry_pick_request_id,
                _library.c.library_id])
            .select_from(_lcp
                .join(_well, _lcp.c.source_well_id==_well.c.well_id)
                .join(_library,_well.c.library_id==_library.c.library_id))
            .group_by(_lcp.c.cherry_pick_request_id, _library.c.library_id),
        )).cte('libraries')
        _restricted_libraries = (
            select([
                _libraries.c.cherry_pick_request_id,
                _library.c.short_name,
                _library.c.screening_status
                ])
            .select_from(_libraries.join(_library,
                _libraries.c.library_id==_library.c.library_id))
            .where(_library.c.screening_status!=SCREENING_STATUS.ALLOWED)
            .group_by(_libraries.c.cherry_pick_request_id,
                      _library.c.screening_status,
                      _library.c.short_name)
            .order_by(_library.c.screening_status,_library.c.short_name)
            ).cte('restricted_libraries')

        # Wells that have not been screened?
        _aw = self.bridge['assay_well']
        _scps_not_loaded = (
            select([
                _cpr.c.cherry_pick_request_id,
                _scp.c.screened_well_id
            ])
            .select_from(_cpr.join(
                _scp, _cpr.c.cherry_pick_request_id==_scp.c.cherry_pick_request_id)
                .join(_sr, _sr.c.screen_id==_cpr.c.screen_id, isouter=True)
                .join(_aw, and_(
                    _aw.c.screen_result_id==_sr.c.screen_result_id,
                    _aw.c.well_id ==_scp.c.screened_well_id), isouter=True)
            )
            .where(_aw.c.well_id==None))
        if cherry_pick_request_id:
            # Note this will be slow for general queries
            _scps_not_loaded = _scps_not_loaded.where(
                _cpr.c.cherry_pick_request_id==cherry_pick_request_id )
        _scps_not_loaded= _scps_not_loaded.cte('_scps_not_loaded')
    
        _a = self.bridge['activity']
        _ls = self.bridge['library_screening']
        _ap = self.bridge['assay_plate']
        _scps_not_screened = (
            select([
                _cpr.c.cherry_pick_request_id,
                _scp.c.screened_well_id
            ])
            .select_from(_cpr.join(
                _scp, _cpr.c.cherry_pick_request_id==_scp.c.cherry_pick_request_id)
                .join(_well, _scp.c.screened_well_id==_well.c.well_id)
            )
            .where(_scp.c.selected)
            .where(~exists(
                select([None])
                .select_from(_a.join(_ls, _ls.c.activity_id==_a.c.activity_id)
                             .join(_ap, _ap.c.library_screening_id==_ls.c.activity_id))
                .where(_ap.c.plate_number==_well.c.plate_number)
                .where(_cpr.c.screen_id==_a.c.screen_id)
                ))
            .group_by(_cpr.c.cherry_pick_request_id, _scp.c.screened_well_id)
            .order_by(_cpr.c.cherry_pick_request_id, _scp.c.screened_well_id))
        if cherry_pick_request_id:
            # Note this will be slow for general queries
            _scps_not_screened = _scps_not_screened.where(
                _cpr.c.cherry_pick_request_id==cherry_pick_request_id )
        _scps_not_screened= _scps_not_screened.cte('_scps_not_screened')
            
        custom_columns = {
            'requested_by_id': _requestors.c.screensaver_user_id,
            'requested_by_name': _requestors.c.name,
            'volume_approved_by_username': _approvers.c.username,
            'volume_approved_by_name': _approvers.c.name,
            'lab_name': lab_head_table.c.lab_name_full,
            'lab_head_id': lab_head_table.c.screensaver_user_id,
            'lab_head_username': lab_head_table.c.username,
            'lead_screener_name': _lead_screeners.c.name,
            'lead_screener_id': _lead_screeners.c.screensaver_user_id,
            'lead_screener_username': _lead_screeners.c.username,
            'number_plates': (
                select([func.count(None)])
                    .select_from(_cpp)
                    .where(_cpp.c.cherry_pick_request_id
                        ==_cpr.c.cherry_pick_request_id)
                ),
            'number_plates_completed': (
                select([func.count(None)])
                    .select_from(_cpp)
                    .where(_cpp.c.cherry_pick_request_id
                        ==_cpr.c.cherry_pick_request_id)
                    .where(_cpp.c.plating_date!=None)
                ),
            'number_plates_screened': (
                select([func.count(None)])
                    .select_from(_cpp)
                    .where(_cpp.c.cherry_pick_request_id
                        ==_cpr.c.cherry_pick_request_id)
                    .where(_cpp.c.screening_date!=None)
                ),
            'is_completed': (
                case(
                    [(
                        (select([func.count(None)])
                         .select_from(_cpp)
                         .where(_cpp.c.cherry_pick_request_id==
                             _cpr.c.cherry_pick_request_id).as_scalar()>0), 
                        (select([func.count(None)])
                         .select_from(_cpp)
                         .where(_cpp.c.cherry_pick_request_id
                             ==_cpr.c.cherry_pick_request_id)
                         .where(_cpp.c.plating_date==None)).as_scalar()==0),
                    ],
                    else_=text('false'))
                ),
            'number_unfulfilled_lab_cherry_picks': (
                func.coalesce(
                    cast(_lcp_subquery.c.lcp_count - 
                            _lcp_subquery.c.lcp_fullfilled_count,
                         sqlalchemy.sql.sqltypes.Integer),0)),                    
            'total_number_lcps': func.coalesce(
                cast(_lcp_subquery.c.lcp_count,
                    sqlalchemy.sql.sqltypes.Integer),0),
            'last_plating_activity_date': (
                select([func.max(_cpp.c.plating_date)])
                .select_from(_cpp)
                .where(_cpp.c.cherry_pick_request_id
                    ==_cpr.c.cherry_pick_request_id)),
            'last_screening_activity_date': (
                select([func.max(_cpp.c.screening_date)])
                .select_from(_cpp)
                .where(_cpp.c.cherry_pick_request_id
                    ==_cpr.c.cherry_pick_request_id)),
            
            # TODO: new: when the lcp's were reserved and mapped
            # 'date_volume_reserved': literal_column("'2016-12-07'"),
            
            'screener_cherry_picks': (
                select([func.array_to_string(
                    func.array_agg(literal_column('screened_well_id')), 
                    LIST_DELIMITER_SQL_ARRAY) ])
                .select_from(
                    select([_scp.c.screened_well_id])
                        .select_from(_scp)
                        # NOTE: when doing an inner select, must use literal_column,
                        # otherwise SQalchemy thinks it is another table to add
                        .where(_scp.c.cherry_pick_request_id
                            ==literal_column(
                                'cherry_pick_request.cherry_pick_request_id'))
                        .where(_scp.c.selected)
                        .order_by('screened_well_id').alias('inner_scps'))
                    ),
            'screener_cherry_pick_count': (
                select([func.count(None)])
                    .select_from(_scp)
                    .where(_scp.c.cherry_pick_request_id
                        ==_cpr.c.cherry_pick_request_id)
                    .where(_scp.c.selected)),
            'screener_picks_not_screened': (
                select([func.array_to_string(
                    func.array_agg(literal_column('screened_well_id')), 
                    LIST_DELIMITER_SQL_ARRAY) ])
                .select_from(
                    select([_scps_not_screened.c.screened_well_id])
                        .select_from(_scps_not_screened)
                        # NOTE: when doing an inner select, must use literal_column,
                        # otherwise SQalchemy thinks it is another table to add
                        .where(_scps_not_screened.c.cherry_pick_request_id
                            ==literal_column(
                                'cherry_pick_request.cherry_pick_request_id'))
                        .order_by('screened_well_id').alias('inner_nscp'))
                    ),
            'has_pool_screener_cherry_picks': (
                select([func.count(None)>0])
                    .select_from(
                        _scp.join(_well, 
                            _scp.c.screened_well_id==_well.c.well_id)
                        .join(_library, 
                            _well.c.library_id==_library.c.library_id))
                    .where(_library.c.is_pool==True)
                    .where(_scp.c.cherry_pick_request_id
                        ==_cpr.c.cherry_pick_request_id).limit(1)),
            'has_alternate_screener_cherry_pick_selections': (
                select([func.count(None)>0])
                    .select_from(_scp)
                    .where(_scp.c.cherry_pick_request_id
                        ==_cpr.c.cherry_pick_request_id)
                    .where(_scp.c.searched_well_id!=_scp.c.screened_well_id)
                ),
            'duplicate_screener_cherry_picks': (
                select([func.array_to_string(func.array_agg(
                    _other_scps.c.screened_well_id),LIST_DELIMITER_SQL_ARRAY)])
                .where(_other_scps.c.cherry_pick_request_id
                       == _cpr.c.cherry_pick_request_id)
                ),
            'deprecated_picks': (
                select([func.array_to_string(func.array_agg(_concat(
                    _deprecations.c.deprecation_reason,': [',_deprecations.c.well_ids, ']')
                    ),LIST_DELIMITER_SQL_ARRAY)])
                .select_from(_deprecations)
                .where(_deprecations.c.cherry_pick_request_id
                       == _cpr.c.cherry_pick_request_id)
                ),
            'restricted_libraries': (
                select([func.array_to_string(func.array_agg(_concat(
                    _restricted_libraries.c.screening_status,': ',_restricted_libraries.c.short_name)
                    ),LIST_DELIMITER_SQL_ARRAY)])
                .select_from(_restricted_libraries)
                .where(_restricted_libraries.c.cherry_pick_request_id
                       == _cpr.c.cherry_pick_request_id)
                ),
            'cherry_pick_allowance': (
                cast(case([
                    (_screen.c.screen_type=='small_molecule',
                        func.ceil(settings.APP_PUBLIC_DATA.small_molecule_cherry_pick_ratio_allowed
                            *_screen.c.screened_experimental_well_count)
                     )],
                    else_=(
                        func.ceil(settings.APP_PUBLIC_DATA.rnai_cherry_pick_ratio_allowed
                            *_screen.c.screened_experimental_well_count)
                        )
                    ), sqlalchemy.types.INTEGER)
            ),
            'screener_cherry_pick_count_cumulative': (
                select([func.count(None)])
                .select_from(_scp2.join(_cpr2,_scp2.c.cherry_pick_request_id
                    ==_cpr2.c.cherry_pick_request_id))
                .where(_cpr2.c.screen_id
                       == _cpr.c.screen_id)
                .where(_scp2.c.selected)
                ),
            # SS1 method
            # 'cherry_pick_allowance': (
            #     case([
            #         (_screen.c.screen_type=='small_molecule',
            #         select([
            #             (settings.APP_PUBLIC_DATA.small_molecule_cherry_pick_ratio_allowed
            #                 *func.count(func.distinct(_smr_unique.c.smiles)))                    
            #             ])
            #         .select_from(_smr_unique)
            #         .where(_smr_unique.c.cherry_pick_request_id
            #                ==_cpr.c.cherry_pick_request_id).as_scalar()
            #          )],
            #         else_=settings.APP_PUBLIC_DATA.rnai_cherry_pick_ratio_allowed
            #     )),
        }
        
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)

        j = join(_cpr, _screen, _cpr.c.screen_id == _screen.c.screen_id)
        j = j.join(_sr, _cpr.c.screen_id == _sr.c.screen_id, isouter=True)
        j = j.join(
            _requestors, 
            _cpr.c.requested_by_id==_requestors.c.screensaver_user_id,
            isouter=True)
        j = j.join(
            _approvers, 
            _cpr.c.volume_approved_by_id==_approvers.c.screensaver_user_id,
            isouter=True)
        j = j.join(
            lab_head_table, 
            lab_head_table.c.screensaver_user_id==_screen.c.lab_head_id,
            isouter=True)
        j = j.join(
            _lead_screeners,
            _lead_screeners.c.screensaver_user_id==_screen.c.lead_screener_id,
            isouter=True)

        if ('number_unfulfilled_lab_cherry_picks' in field_hash
             or 'total_number_lcps' in field_hash):
            j = j.join(
                _lcp_subquery, _cpr.c.cherry_pick_request_id
                    ==_lcp_subquery.c.cherry_pick_request_id, isouter=True)
        
        stmt = select(columns.values()).select_from(j)
        # general setup
         
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        
        # compiled_stmt = str(stmt.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)
        
        if not order_clauses:
            stmt = stmt.order_by(
                nullslast(desc(column('cherry_pick_request_id'))))
        
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
        
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None),
            use_caching=True)
             
    
    @write_authorization
    @un_cache
    @transaction.atomic
    def post_detail(self, request, **kwargs):
        # FIXME: Set the log URI using the containing screen URI
        return DbApiResource.post_detail(
            self, request, full_create_log=True, **kwargs)

    @write_authorization
    @un_cache  
    @transaction.atomic      
    def patch_detail(self, request, **kwargs):
        '''
        Override to generate informational summary for callee
        '''
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        deserialized = kwargs.pop('data', None)
        # allow for internal data to be passed
        deserialize_meta = None
        if deserialized is None:
            deserialized, deserialize_meta = self.deserialize(
                request, format=kwargs.get('format', None))
        
        if API_RESULT_DATA in deserialized:
            deserialized = deserialized[API_RESULT_DATA][0]
            
        logger.debug('patch detail %s, %s', deserialized,kwargs)

        # cache state, for logging
        # Look for id's kwargs, to limit the potential candidates for logging
#         id_attribute = schema['id_attribute']
        kwargs_for_log = self.get_id(
            deserialized, schema=schema, validate=True,**kwargs)

        original_data = None
        if kwargs_for_log:
            try:
                original_data = self._get_detail_response_internal(**kwargs_for_log)
                kwargs['original_data'] = original_data
            except Exception, e: 
                logger.exception('exception when querying for existing obj: %s', 
                    kwargs_for_log)
        try:
            parent_log = kwargs.get('parent_log', None)
            log = self.make_log(request)
            log.parent_log = parent_log
            log.save()
            kwargs['parent_log'] = log
            patch_response = self.patch_obj(request, deserialized, **kwargs)
        except ValidationError as e:
            logger.exception('Validation error: %r', e)
            raise e

        # get new state, for logging
        new_data = self._get_detail_response_internal(**kwargs_for_log)
        logger.debug('original: %r, new: %r', original_data, new_data)
        log = self.log_patch(
            request, original_data,new_data,log=log, full_create_log=False, 
            excludes=['screener_cherry_picks'],
            **kwargs_for_log)
        # Set the log URI using the containing screen URI
        if log:
            # NOTE: make the log uri more robust, with screen id as well
            log.uri = '/'.join([
                log.ref_resource_name,'screen',new_data['screen_facility_id'],
                log.key])
            log.save()
            logger.debug('log info: %r, %r', log, log.diffs )
        
        meta = {}
        if API_RESULT_META in patch_response:
            meta = patch_response[API_RESULT_META]
        if deserialize_meta:
            meta.update(deserialize_meta)
        
        return self.build_response(
            request,  { API_RESULT_META: meta }, 
            response_class=HttpResponse, **kwargs)
            
    @write_authorization
    @un_cache  
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):
        '''
        Create a new Cherry Pick Request
        - set the screener cherry picks if included
        '''
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        fields = schema['fields']
        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        logger.debug('param_hash: %r', param_hash)
        
        patch = bool(id_kwargs)
        initializer_dict = self.parse(deserialized, schema=schema, create=not patch)
        errors = self.validate(initializer_dict, schema=schema, patch=patch)
        if errors:
            raise ValidationError(errors)

        cpr = None
        if patch is True:
            try:
                cpr = CherryPickRequest.objects.get(**id_kwargs)
            except ObjectDoesNotExist:
                raise Http404(
                    'Cherry Pick Request does not exist for: %r', id_kwargs)
        
        if patch is not True:
            _key = 'screen_facility_id'
            _val = deserialized.get(_key, None)
            if _val is None:
                _val = kwargs.get(_key,None)
            if _val is None:
                raise ValidationError(
                    key='screen_facility_id',msg='required')
            try:
                screen = Screen.objects.get(facility_id=_val)
                initializer_dict['screen'] = screen
            except ObjectDoesNotExist:
                raise ValidationError(
                    key=_key,
                    msg='does not exist: {val}'.format(val=_val))
        else:
            screen = cpr.screen
            
        _key = 'requested_by_id'
        _val = deserialized.get(_key, None)
        if _val:
            try:
                requested_by_user = ScreensaverUser.objects.get(screensaver_user_id=_val)
                if requested_by_user not in screen.get_screen_users():
                    raise ValidationError(
                        key='requested_by_id',
                        msg='"%s" must be one of the screen users: %r'
                            %(_val, [x.screensaver_user_id 
                                for x in screen.get_screen_users()]))
                initializer_dict['requested_by'] = requested_by_user
            except ObjectDoesNotExist:
                raise ValidationError(
                    key=_key,
                    msg='does not exist: {val}'.format(val=_val))

        _key = 'volume_approved_by_username'
        _val = deserialized.get(_key, None)
        if _val:
            try:
                volume_approved_by_user = ScreensaverUser.objects.get(username=_val)
                
                if not self._meta.authorization._is_resource_authorized(
                        volume_approved_by_user.user.user,'write'):
                    raise ValidationError(
                        key='volume_approved_by_username',
                        msg='user %r does not have %r %r authorization' 
                            % (_val, self._meta.resource_name, 'write'))
                self.validate_volume_approver(volume_approved_by_user)
                initializer_dict['volume_approved_by'] = volume_approved_by_user
            except ObjectDoesNotExist:
                raise ValidationError(
                    key=_key,
                    msg='does not exist: {val}'.format(val=_val))
        _meta = {}
        try:
            if patch is not True:
                cpr = CherryPickRequest()

            allocated_lcps_query = cpr.lab_cherry_picks.filter(
                cherry_pick_assay_plate__isnull=False)
            if allocated_lcps_query.exists():
                disallowed_fields = [
                    'assay_plate_type',
                    'keep_source_plate_cherry_picks_together'
                    'is_randomized_assay_plate_layout'
                    'wells_to_leave_empty'
                    'transfer_volume_per_well_approved']
                if set(disallowed_fields) & set(initializer_dict.keys()):
                    raise ValidationError(
                        key=API_MSG_LCPS_MUST_BE_DELETED,
                        msg='Disallowed fields: %r' % disallowed_fields)
            model_field_names = [
                x.name for x in cpr._meta.get_fields()]
            for key, val in initializer_dict.items():
                if key in model_field_names:
                    setattr(cpr, key, val)

            cpr.save()
            logger.info('patch cpr created: %r', cpr)
            
            final_warn_msg = []
            wells_to_leave_empty = deserialized.get('wells_to_leave_empty', None)
            if wells_to_leave_empty is not None:
                parsed_wells_to_leave_empty = \
                    lims_utils.parse_wells_to_leave_empty(
                        wells_to_leave_empty, cpr.assay_plate_size)
                cpr.wells_to_leave_empty = ', '.join(parsed_wells_to_leave_empty)
                cpr.save()
                
            if 'screener_cherry_picks' in deserialized:
                plated_assay_plates_query = \
                    cpr.cherry_pick_assay_plates.filter(
                        plating_date__isnull=False)                
                if plated_assay_plates_query.exists():
                    raise ValidationError({
                        SCHEMA.API_MSG_NOT_ALLOWED: 
                            ('Screener cherry picks may not be reassigned after '
                             'plates have been plated'),
                        API_MSG_LCP_ASSAY_PLATES_PLATED: plated_assay_plates_query.count()
                })
                
                if cpr.lab_cherry_picks.exists():
                    raise ValidationError({
                        'total_number_lcps': 
                            ('Lab cherry picks already assigned: (%d); '
                            'delete lab cherry picks to change '
                            'screener cherry pick selections' 
                            % cpr.lab_cherry_picks.count()),
                        API_MSG_LCPS_MUST_BE_DELETED: cpr.lab_cherry_picks.count()
                    })
                    
                # TODO: Test override
                override_param = parse_val(
                    param_hash.get(API_PARAM_OVERRIDE, False),
                        API_PARAM_OVERRIDE, 'boolean')
                
                raw_screener_cps = deserialized['screener_cherry_picks']
                if raw_screener_cps is None or len(raw_screener_cps) == 0:
                    logger.info('removing screener_cherry_picks')
                    # FIXME: remove lab_cherry_picks, iif:
                    # - not plated
                    # - replace allocate well volumes
                    # - create a log for the action
                    _meta[API_MSG_SCPS_DELETED] = cpr.screener_cherry_picks.all().count()
                    cpr.screener_cherry_picks.all().delete()
                    cpr.save()
                else:
                    cherry_pick_wells = self.find_wells(raw_screener_cps)
                    logger.info(
                        'found screener_cherry_picks: %d', len(cherry_pick_wells))
                    not_allowed_libraries = set()
                    discarded_libraries = set()
                    wrong_screen_type = set()
                    for well in cherry_pick_wells:
                        screen_type = cpr.screen.screen_type
                        if well.library.screen_type != screen_type:
                            wrong_screen_type.add(well.well_id)
                        if wrong_screen_type:
                            continue
                        if well.library.screening_status == 'discarded':
                            discarded_libraries.add(well.library)
                        if well.library.screening_status != 'allowed':
                            not_allowed_libraries.add(well.library)
                        if len(not_allowed_libraries)>0 and override_param is not True:
                            continue
                        screener_cherry_pick = ScreenerCherryPick.objects.create(
                            cherry_pick_request=cpr,
                            screened_well=well,
                            searched_well=well,
                            selected=True)
                    if wrong_screen_type:
                        wrong_screen_type = sorted(wrong_screen_type)
                        raise ValidationError(
                            key='Wrong Well screen_type, must be %s' 
                                % cpr.screen.screen_type,
                            msg=wrong_screen_type)
                    if discarded_libraries:
                        discarded_libraries = sorted([
                            '%s - status: %s' % (l.short_name,l.screening_status)
                                for l in discarded_libraries])
                        raise ValidationError({
                            'screener_cherry_picks': 'Discarded libraries',
                            'Libraries': discarded_libraries
                            })
                        
                    if not_allowed_libraries:
                        not_allowed_libraries = sorted([
                            '%s - status: %s' % (l.short_name,l.screening_status)
                                for l in not_allowed_libraries])
                    if len(not_allowed_libraries)>0 and override_param is not True:
                        raise ValidationError({
                            API_PARAM_OVERRIDE: 'required',
                            'screener_cherry_picks': (
                                'Override required to screen libraries that are '
                                'not allowed'),
                            'Libraries': not_allowed_libraries
                            }
                        )
                    if len(not_allowed_libraries)>0:
                        final_warn_msg.append(
                            ('Override used for libraries', not_allowed_libraries))
                    _meta[API_MSG_SCPS_CREATED] = cpr.screener_cherry_picks.all().count()    

            if final_warn_msg:
                _meta[SCHEMA.API_MSG_WARNING] = final_warn_msg
                            
            response = { API_RESULT_OBJ: cpr, API_RESULT_META: _meta }
            logger.info('response: %r', response)
            return response
        except Exception, e:
            logger.exception('on patch_obj')
            raise e
    
    @staticmethod
    def _get_plate_size(assay_plate_type):
        parts = assay_plate_type.split('_')
        if len(parts) != 2:
            raise ValidationError(
                key='assay_plate_type', 
                msg='not a recognized type: %r' % assay_plate_type)
        plate_size = int(parts[1])
        return plate_size

    
    @classmethod
    def find_wells(cls, cherry_pick_well_patterns ):
        logger.debug('find wells for patterns: %r', cherry_pick_well_patterns)
        if not isinstance(cherry_pick_well_patterns, (list,tuple)):
            cherry_pick_well_patterns = (cherry_pick_well_patterns,)

        wells = set()
        try:
            wells = WellResource.find_wells(cherry_pick_well_patterns)
        except ValidationError,e:
            # re-raise
            raise ValidationError(
                {'screener_cherry_picks':e.errors[SCHEMA.API_PARAM_SEARCH] })
            
        non_experimental_wells = []
        for well in wells:
            if well.library_well_type != WELL_TYPE.EXPERIMENTAL:
                non_experimental_wells.append(well)
        if non_experimental_wells:
            raise ValidationError(   
                key='Can not screen non-experimental wells',
                msg=', '.join([well.well_id for well in non_experimental_wells]))
            
        logger.debug('found wells: %r', wells)
        return wells
            
            
    def validate_cpr_for_plating(self, cpr):
        logger.info('validating: %r', cpr)      
        if not cpr.transfer_volume_per_well_approved:
            raise ValidationError(
                key='transfer_volume_per_well_approved',
                msg='required')
        self.validate_volume_approver(cpr.volume_approved_by)
        
    def validate_volume_approver(self, user):
        logger.info('validating: %r', user)      
        if not user:
            raise ValidationError(
                key='volume_approved_by_username',
                msg='required')
        else:
            # find the user, verify that they have cherrypickrequest admin permission
            user_data = \
                self.get_user_resource()._get_detail_response_internal(**{
                'username': user.username 
                })
            if not user_data:
                raise ValidationError(
                    key='volume_approved_by_username', msg='not a valid user')
            if user_data.get('is_superuser', False) is True:
                return
            required_permission = 'resource/cherrypickrequest/write'
            if required_permission not in user_data['all_permissions']:
                logger.warn('user: %r, permissions: %r, does not contain: %r',
                    user_data['username'], user_data['all_permissions'], 
                    required_permission)
                raise ValidationError(
                    key='volume_approved_by_username', msg='user is not authorized')
    
    @write_authorization
    @un_cache
    @transaction.atomic
    def dispatch_reserve_map_lab_cherry_picks(self, request, **kwargs):
        '''
        Reserve (allocate) the Copy Well volume for the Lab Cherry Picks 
        on the Cherry Pick Request:
        - only consider fulfilled LCPs (where a source copy has been assigned) 
        '''
        
        request_method = request.method.lower()
        if request_method != 'post':
            raise BadRequestError(key='method', msg='Only POST is allowed')
        convert_request_method_to_put(request)
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        cherry_pick_request_id = param_hash['cherry_pick_request_id']
        logger.info(
            'dispatch_reserve_map_lab_cherry_picks for: %r...', 
            cherry_pick_request_id)
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=cherry_pick_request_id)
        if not cpr.lab_cherry_picks.filter(copy__isnull=False).exists():
            raise ValidationError(
                key='total_number_lcps', 
                msg='No (fulfilled) Lab Cherry Picks have been created')
        
        self.validate_cpr_for_plating(cpr) 
        
        # Create parent log: cherry pick request: date_volume_reserved
        parent_log = self.make_log(request)
        parent_log.key = str(cpr.cherry_pick_request_id)
        # NOTE: make the log uri more robust, with screen id as well
        parent_log.uri = '/'.join([
            parent_log.ref_resource_name, 'screen',cpr.screen.facility_id, 
            parent_log.key])        
        parent_log.save()
        
        previous_date_reserved = cpr.date_volume_reserved
        cpr.date_volume_reserved = _now().date() 
        cpr.save()
        if previous_date_reserved != cpr.date_volume_reserved:
            parent_log.diffs = {
                'date_volume_reserved': [previous_date_reserved, cpr.date_volume_reserved ]}
        
        status_messages = []
        copywell_deallocation_meta = None
        previous_number_of_plates = None           
        
        logger.info('check for previous plating assignments...')
        allocated_lcps_query = cpr.lab_cherry_picks.filter(
            cherry_pick_assay_plate__isnull=False)
        if allocated_lcps_query.exists():
            plated_assay_plates_query = \
                cpr.cherry_pick_assay_plates.filter(
                    plating_date__isnull=False)                
            if plated_assay_plates_query.exists():
                raise ValidationError({
                    SCHEMA.API_MSG_NOT_ALLOWED: API_MSG_CPR_PLATED_CANCEL_DISALLOWED, 
                    API_MSG_LCP_ASSAY_PLATES_PLATED: plated_assay_plates_query.count()
            })
            raise ValidationError({
                SCHEMA.API_MSG_NOT_ALLOWED: 
                    ('Lab cherry pick plates already assigned; '
                    'reservation must be canceled before reassignment is allowed'),
                API_MSG_LCP_PLATES_ASSIGNED: cpr.cherry_pick_assay_plates.count()
            })
        
        logger.info('Find the fulfillable lab cherry picks...')
        available_assay_plate_wells = cpr.assay_plate_available_wells
        logger.debug('available_assay_plate_wells: %r',available_assay_plate_wells)
        logger.info('available_assay_plate_wells len: %d', 
            len(available_assay_plate_wells))

        fulfillable_lcps = (
            cpr.lab_cherry_picks.filter(copy__isnull=False)
                .order_by('source_well__plate_number','copy__name') )

        logger.info('re-Check and reserve copy volumes...')
        lab_cherry_pick_copywells = \
            { lcp['source_copywell_id']: lcp for lcp in
                self.get_labcherrypick_resource()._get_list_response_internal(
                    **{
                        'cherry_pick_request_id': cpr.cherry_pick_request_id,
                        'source_copy_name__is_null': False,
                        'includes': [
                            'source_plate_type','destination_plate_type',
                            'source_copywell_id','source_copy_well_volume',
                            'volume_approved',
                            '-structure_image','-molfile','-library_plate_comment_array'],
                    })}
        logger.info('fetch output readable format...')
        lab_cherry_pick_copywells_output = \
            { lcp['source_copywell_id']: lcp for lcp in
                self.get_labcherrypick_resource()._get_list_response_internal(
                    **{
                        'cherry_pick_request_id': cpr.cherry_pick_request_id,
                        'source_copy_name__is_null': False,
                        'includes': [
                            'source_plate_type','destination_plate_type',
                            'source_copywell_id','source_copy_well_volume',
                            'volume_approved',
                            '-structure_image','-molfile','-library_plate_comment_array'],
                        HTTP_PARAM_USE_VOCAB: True,
                    })}
        logger.info('Check for insufficient well volumes...')
        unfulfillable_wells = []
        override_well_volume = parse_val(
            param_hash.get(API_PARAM_VOLUME_OVERRIDE, False),
            API_PARAM_VOLUME_OVERRIDE, 'boolean')
        
        for copywell_id,lcp_data in lab_cherry_pick_copywells.items():
            lcp_scwv = Decimal(lcp_data['source_copy_well_volume'])
            logger.debug('consider lcp_cw: %r, %r to %r', 
                copywell_id, lcp_scwv, cpr.transfer_volume_per_well_approved)
            if ( lcp_scwv < cpr.transfer_volume_per_well_approved ):
                lcp_output = lab_cherry_pick_copywells_output[copywell_id]
                unfulfillable_wells.append(
                    (copywell_id, 
                        '(available: %s uL)' % (lcp_output['source_copy_well_volume'] or 0),
                        '(requested: %s uL)' % lcp_output['volume_approved']))
        unfulfillable_wells = sorted(unfulfillable_wells, key=lambda x: x[0])
        if unfulfillable_wells and override_well_volume is not True:
            raise ValidationError({
                'transfer_volume_per_well_approved':
                    '%s: %r ' % (API_MSG_LCPS_INSUFFICIENT_VOLUME, 
                                 unfulfillable_wells),
                API_MSG_LCPS_INSUFFICIENT_VOLUME: unfulfillable_wells,
                API_PARAM_VOLUME_OVERRIDE: 'required',
                
            })
        elif unfulfillable_wells:
            logger.info(
                'Override: unfulfillable wells: %r' % unfulfillable_wells)
        else:
            logger.info('all wells are fulfillable')

        logger.info('Reserve copy well volumes...')
        copywell_reservation_meta = \
            self.get_copywell_resource().reserve_cherry_pick_volumes(
                cpr, fulfillable_lcps, parent_log)
        
        logger.info('Create the assay_plates...')  
#         # - if true, randomize the plate layout wells
#         if cpr.is_randomized_assay_plate_layout:
#             logger.debug('randomize the available assay plate wells: %r',
#                 available_assay_plate_wells)
#             random.shuffle(available_assay_plate_wells)
#             logger.debug('randomized available assay plate wells: %r',
#                 available_assay_plate_wells)
                  
        next_plate_ordinal = [1]
        def create_next_assay_plate():
            assay_plate = CherryPickAssayPlate.objects.create(
                cherry_pick_request=cpr,
                plate_ordinal=next_plate_ordinal[0],
                # TODO: deprecate attempt_ordinal
                attempt_ordinal=0,
                assay_plate_type=cpr.assay_plate_type,
                # TODO: deprecate cpapt
                cherry_pick_assay_plate_type='CherryPickAssayPlate',
            )
            next_plate_ordinal[0] = next_plate_ordinal[0]+1
            return assay_plate
        
        # Create a sortable multimap
        lcp_by_plate_copy = defaultdict(list)
        for lcp in fulfillable_lcps.all():
            plate_copy = '%s:%s' % (
                str(lcp.source_well.plate_number),lcp.copy.name)
#                 str(lcp.source_well.plate_number).zfill(5),lcp.copy.name)
            lcp_by_plate_copy[plate_copy].append(lcp)
        # Sort internally
        for plate_copy in lcp_by_plate_copy.keys():
            lcps = lcp_by_plate_copy[plate_copy]
            lcps = sorted(lcps, key=lambda lcp: lcp.source_well_id)
            lcp_by_plate_copy[plate_copy] = lcps
            
        if cpr.keep_source_plate_cherry_picks_together is True:

            logger.info("keep_source_plate_cherry_picks_together...")
            logger.info("use the bin packer to fit the lcp's to bins...")
            capacity = len(available_assay_plate_wells)
            packages = [ { 'name': plate_copy, 'size': len(lcps) } 
                for plate_copy,lcps in lcp_by_plate_copy.items() ]
            packed_bins = bin_packer.pack_bins(capacity, packages)
            packed_bins = sorted(
                packed_bins,
                key=lambda bin: (capacity-bin_packer.sum_bin(bin),bin[0]['name']),
                reverse=False)
            logger.info('packed bins: %r', 
                [ (bin_packer.sum_bin(bin),bin[0]['name']) for bin in packed_bins])
            
            logger.info(
                "assign the packed_bins to assay_plates, "
                "order by sum_bin, librarycopyplate.key")
            ordered_bins = []
            for packed_bin in packed_bins:
                if packed_bin not in ordered_bins:
                    ordered_bins.append(packed_bin)
                    partially_packed_plate_copy = [
                        package['name'] for package in packed_bin
                            if (len(lcp_by_plate_copy[package['name']]) 
                                > package['size']) ]
                    if len(partially_packed_plate_copy) > 1:
                        raise ProgrammingError(
                            'Packed bins can not contain more than one '
                            'partially packed plate_copies: %r' 
                            % partially_packed_plate_copy)
                    # NOTE: assume that source plate will never need > 2 assay plates
                    if partially_packed_plate_copy:
                        plate_copy = partially_packed_plate_copy[0]
                        for second_bin in packed_bins:
                            if second_bin != packed_bin:
                                if plate_copy in [ p['name'] for p in second_bin]:
                                    ordered_bins.append(second_bin)
            
            logger.info('create assay plates, in order...')
            assay_plates_created = []

            for packed_bin in ordered_bins:
            
                logger.info('using packed bin: %r', packed_bin)
                assay_plate_wells_to_use = \
                    available_assay_plate_wells[:bin_packer.sum_bin(packed_bin)]                                
                if cpr.is_randomized_assay_plate_layout:                    
                    random.shuffle(assay_plate_wells_to_use)
                assay_plate = create_next_assay_plate()
                assay_plates_created.append(assay_plate)
                assay_plate_well_index = 0
                    
                for package in packed_bin:

                    logger.info('package: %r', package)
                    
                    plate_copy = package['name']
                    size = package['size']
                    plate_copy_lcps = lcp_by_plate_copy[plate_copy]
                    lcps_to_plate = plate_copy_lcps[:size]
                    if size < len(plate_copy_lcps):
                        lcp_by_plate_copy[plate_copy] = plate_copy_lcps[size:]
                    
                    logger.info('lcps: %r, assay_plate.plate_ordinal: %r', 
                        len(lcps_to_plate),assay_plate.plate_ordinal)
                    
                    
                    for lcp in lcps_to_plate:
                        lcp.cherry_pick_assay_plate = assay_plate
                        well_name = assay_plate_wells_to_use[assay_plate_well_index]
                        lcp.assay_plate_row = lims_utils.well_name_row_index(well_name)
                        lcp.assay_plate_column = lims_utils.well_name_col_index(well_name)
                        assay_plate_well_index += 1
                        lcp.save()
        else: 
            logger.info('create assay plates, in order...')
            logger.info("do not keep_source_plate_cherry_picks_together")
            # keep_source_plate_cherry_picks_together == False
            # no bin packing
            capacity = len(available_assay_plate_wells)

            def randomize_lcps(lcps):
                current_well_assignments = [
                    (lcp.assay_plate_row,lcp.assay_plate_column) 
                    for lcp in lcps]
                random.shuffle(current_well_assignments)
                for i,lcp in enumerate(lcps):
                    shuffled_rc = current_well_assignments[i]
                    lcp.assay_plate_row = shuffled_rc[0]
                    lcp.assay_plate_column = shuffled_rc[1]
                    lcp.save()
            
            assay_plates_created = []
            assay_plate = None
            current_plate_lcps = []
            
            plate_copies = sorted(lcp_by_plate_copy.keys())
            for plate_copy in plate_copies:

                logger.info('plating: %r', plate_copy)
                lcps_to_plate = lcp_by_plate_copy[plate_copy]
                for lcp in lcps_to_plate:

                    if assay_plate is None:
                        assay_plate = create_next_assay_plate()
                        assay_plates_created.append(assay_plate)
                        assay_plate_well_index = 0

                    lcp.cherry_pick_assay_plate = assay_plate
                    well_name = available_assay_plate_wells[assay_plate_well_index]
                    lcp.assay_plate_row = lims_utils.well_name_row_index(well_name)
                    lcp.assay_plate_column = lims_utils.well_name_col_index(well_name)
                    lcp.save()
                    current_plate_lcps.append(lcp)
                    
                    assay_plate_well_index += 1
                    if assay_plate_well_index >= capacity:
                        assay_plate_well_index = 0
                        assay_plate = None
                    
            if cpr.is_randomized_assay_plate_layout:                    
                randomize_lcps(current_plate_lcps)
            
#             for lcp in fulfillable_lcps.all():
#                 
#                 copy_plate = '%s:%s' % (lcp.copy.name, lcp.source_well.plate_number)
#                 
#                 if assay_plate is None:
#                     assay_plate = create_next_assay_plate()
#                     assay_plates_created.append(assay_plate)
#                     assay_plate_well_index = 0
# 
#                 lcp.cherry_pick_assay_plate = assay_plate
#                 well_name = available_assay_plate_wells[assay_plate_well_index]
#                 lcp.assay_plate_row = lims_utils.well_name_row_index(well_name)
#                 lcp.assay_plate_column = lims_utils.well_name_col_index(well_name)
#                 lcp.save()
# 
#                 assay_plate_well_index += 1
#                 if assay_plate_well_index >= capacity:
#                     assay_plate_well_index = 0
#                     assay_plate = None

        # verify that all lcps have been assigned
        for lcp in fulfillable_lcps.all():
            copy_plate = '%s:%s' % (lcp.copy.name, lcp.source_well.plate_number)
            if lcp.cherry_pick_assay_plate is None:
                raise ProgrammingError(
                    'lcp has not been assigned: %s, %r' 
                    % lcp, copy_plate)
        
        # return some stats
        cpap_assignments = [ 
            'Plate ordinal: %d, Picks: %d' 
                % (ap.plate_ordinal, ap.labcherrypick_set.all().count()) 
                for ap in assay_plates_created ]
        
        plate_copies = sorted(lcp_by_plate_copy.keys())
#         copy_plate_assigned_msg = [
#             (plate_copy, len(lcp_by_plate_copy[plate_copy]))
#                 for plate_copy in plate_copies] 
        _meta = {
            API_MSG_LCP_PLATES_ASSIGNED: plate_copies,
            API_MSG_LCP_ASSAY_PLATES_CREATED: cpap_assignments
        }
        _meta.update(copywell_reservation_meta)
        if unfulfillable_wells:
            _meta[API_MSG_LCPS_VOLUME_OVERRIDDEN] = unfulfillable_wells

        # log
        parent_log.diffs.update({
            'number_plates': [previous_number_of_plates, len(assay_plates_created)]
            })
        # TODO: ApiLog.save() will encode the json_field if it is a dict
        parent_log.json_field = json.dumps(_meta, cls=LimsJSONEncoder)
        parent_log.save()
        
        return self.build_response(
            request, { API_RESULT_META: _meta }, 
            response_class=HttpResponse, **kwargs)
    
    @write_authorization
    @un_cache
    @transaction.atomic
    def dispatch_delete_lab_cherry_picks(self, request, **kwargs):
        
        request_method = request.method.lower()
        if request_method != 'post':
            raise BadRequestError(key='method', msg='Only POST is allowed')
        convert_request_method_to_put(request)

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        cpr_id = param_hash['cherry_pick_request_id']
        logger.info('dispatch_delete_lab_cherry_picks: %r', cpr_id) 
        schema = super(CherryPickRequestResource, self).build_schema(
            user=request.user)
         
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=cpr_id)
        lcp_query = cpr.lab_cherry_picks.all() 
        if lcp_query.exists():
            allocated_lcp_query = lcp_query.filter(cherry_pick_assay_plate__isnull=False)
            if allocated_lcp_query.exists():
                plated_assay_plates_query = \
                    cpr.cherry_pick_assay_plates.filter(
                        plating_date__isnull=False)                
                if plated_assay_plates_query.exists():
                    raise ValidationError({
                        SCHEMA.API_MSG_NOT_ALLOWED: 
                            ('Lab Cherry Picks may not be deleted after plates are plated'),
                        API_MSG_LCP_PLATES_ASSIGNED: cpr.cherry_pick_assay_plates.count(),
                        API_MSG_LCP_ASSAY_PLATES_PLATED: plated_assay_plates_query.count()
                })
                raise ValidationError({
                    SCHEMA.API_MSG_NOT_ALLOWED: 
                        ('Lab cherry pick plates already assigned; '
                        'cancel reservation to deallocate plates'),
                    API_MSG_LCP_PLATES_ASSIGNED: cpr.cherry_pick_assay_plates.count()
                })

            original_cpr = self._get_detail_response_internal(**{
                'cherry_pick_request_id': cpr_id,
                'includes': '-screener_cherry_picks'
             })
            logger.debug('original_cpr: %r', original_cpr)
            parent_log = self.make_log(request)
            parent_log.key = str(cpr_id)
            # NOTE: make the log uri more robust, with screen id as well
            parent_log.uri = '/'.join([
                parent_log.ref_resource_name,'screen',cpr.screen.facility_id,
                parent_log.key])        
            parent_log.comment = API_MSG_LCPS_REMOVED
            
            meta = {}
            meta[API_MSG_LCPS_REMOVED] = cpr.lab_cherry_picks.all().count()
            logger.info('delete lcps...')
            LabCherryPick.objects.filter(cherry_pick_request=cpr).delete()
            logger.info('lcp delete done')
            new_cpr = self._get_detail_response_internal(**{
                'cherry_pick_request_id': cpr_id,
                'includes': '-screener_cherry_picks'
            })
            logger.debug('new_cpr: %r', new_cpr)
            parent_log = self.log_patch(
                request, original_cpr, new_cpr, log=parent_log, 
                excludes=['screener_cherry_picks'])
            parent_log.save()
            
            return self.build_response(
                request,  {API_RESULT_META: meta }, 
                response_class=HttpResponse, **kwargs)
        else:
            logger.info('no LCPs found to delete')
        # Empty response if no action taken
        return self.build_response(
            request,  {API_RESULT_META: 'no LCPs found to delete' }, 
            response_class=HttpResponse, **kwargs)

    @write_authorization
    @un_cache
    @transaction.atomic
    def dispatch_cancel_reservation(self, request, **kwargs):
        
        request_method = request.method.lower()
        if request_method != 'post':
            raise BadRequestError(key='method', msg='Only POST is allowed')
        convert_request_method_to_put(request)

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        cpr_id = param_hash['cherry_pick_request_id']
        logger.info('dispatch_cancel_reservation: %r', cpr_id) 
        schema = super(CherryPickRequestResource, self).build_schema(
            user=request.user)
         
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=cpr_id)
        lcp_query = cpr.lab_cherry_picks.all() 
        if lcp_query.exists():
            allocated_lcp_query = lcp_query.filter(cherry_pick_assay_plate__isnull=False)
            if allocated_lcp_query.exists():
           
                original_cpr = self._get_detail_response_internal(**{
                    'cherry_pick_request_id': cpr_id })
                parent_log = self.make_log(request)
                parent_log.key = str(cpr_id)
                # NOTE: make the log uri more robust, with screen id as well
                parent_log.uri = '/'.join([
                    parent_log.ref_resource_name, 'screen',
                    cpr.screen.facility_id,parent_log.key])        
        
                meta = self._cancel_reservation(cpr, parent_log)

                new_cpr = self._get_detail_response_internal(**{
                    'cherry_pick_request_id': cpr_id })
                parent_log = self.log_patch(
                    request, original_cpr, new_cpr, parent_log, 
                    excludes=['screener_cherry_picks'])
                parent_log.save()
                
                return self.build_response(
                    request,  {API_RESULT_META: meta }, 
                    response_class=HttpResponse, **kwargs)
            else: 
                logger.warn('no allocated LCPs found to delete for CPR: %r', cpr_id)
        else: 
            logger.warn('no LCPs found to delete for CPR: %r', cpr_id)
        # Empty response if no action taken
        return self.build_response(
            request,  {API_RESULT_META: 'no LCPs found to cancel' }, 
            response_class=HttpResponse, **kwargs)
    
    def _cancel_reservation(self, cpr, parent_log):
        logger.info(
            '_cancel_reservation for: %r...', cpr)
        
        cpr_id = cpr.cherry_pick_request_id 
        meta = {}    
        
        if not cpr.screener_cherry_picks.filter(selected=True).exists():
            logger.warn('No screener cherry picks found for %r', cpr)

        lcp_query = cpr.lab_cherry_picks.all() 
        if lcp_query.exists():
            allocated_lcp_query = lcp_query.filter(cherry_pick_assay_plate__isnull=False)
            if allocated_lcp_query.exists():
                plated_assay_plates_query = \
                    cpr.cherry_pick_assay_plates.filter(plating_date__isnull=False)
                if plated_assay_plates_query.exists():
                    raise ValidationError({
                        SCHEMA.API_MSG_NOT_ALLOWED: API_MSG_CPR_PLATED_CANCEL_DISALLOWED, 
                        API_MSG_LCP_PLATES_ASSIGNED: cpr.cherry_pick_assay_plates.count(),
                        API_MSG_LCP_ASSAY_PLATES_PLATED: plated_assay_plates_query.count()
                    })
                lab_cherry_picks_to_deallocate = (
                    cpr.lab_cherry_picks
                        .filter(cherry_pick_assay_plate__isnull=False)
                        .order_by('copy__name','source_well__plate_number') )
                logger.info('lcps to deallocate: %r', 
                    [str(lcp.source_well) for lcp in lab_cherry_picks_to_deallocate])
                parent_log.comment = API_MSG_PLATING_CANCELED
                parent_log.save()
                result_meta = \
                    self.get_copywell_resource().deallocate_cherry_pick_volumes(
                        cpr, lab_cherry_picks_to_deallocate, parent_log)
                meta.update(result_meta)
                
                cpr.date_volume_reserved = None
                cpr.save()

                logger.info('removing previous cherry pick assay plates for: %r, %d', 
                    cpr, cpr.cherry_pick_assay_plates.count())
                meta[API_MSG_CPR_ASSAY_PLATES_REMOVED] = \
                    cpr.cherry_pick_assay_plates.count()
                cpr.cherry_pick_assay_plates.all().delete()
                cpr.lab_cherry_picks.all().update(
                    assay_plate_row=None, assay_plate_column=None)
            else:
                logger.warn('No assay plates to cancel for %r', cpr)
        else:
            logger.warn('No lab cherry picks to cancel for %r', cpr)
        
        return meta
    
    @write_authorization
    @un_cache
    @transaction.atomic
    def dispatch_set_duplex_lab_cherry_picks(self, request, **kwargs):
        '''
        Create Lab Cherry Picks for the Screener Cherry Picks that have already
        been created for the CPR:
        - in this case, select the duplex wells corresponding to the pool wells
        that have been selected as Screener Cherry Picks.
        - see dispatch_set_lab_cherry_picks for the automatic copy selection 
        process.
        '''
        
        logger.info('dispatch_set_duplex_lab_cherry_picks')
        request_method = request.method.lower()
        if request_method != 'post':
            raise BadRequestError(key='method', msg='Only POST is allowed')
        convert_request_method_to_put(request)

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
         
        cpr_id = param_hash['cherry_pick_request_id']
        logger.info(
            'dispatch_set_duplex_lab_cherry_picks for: %r...', cpr_id)
         
        schema = super(CherryPickRequestResource, self).build_schema(
            user=request.user)
         
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=cpr_id)
        if not cpr.screener_cherry_picks.filter(selected=True).exists():
            raise ValidationError(
                key='screener_cherry_picks', 
                msg='No Screener Cherry Picks have been submitted')
        
        original_cpr = self._get_detail_response_internal(**{
            'cherry_pick_request_id': cpr_id,
            'includes': '-screener_cherry_picks'
         })
        logger.debug('original_cpr: %r', original_cpr)
        
        meta = {}    
        
        parent_log = self.make_log(request)
        parent_log.key = str(cpr_id)
        # NOTE: make the log uri more robust, with screen id as well
        parent_log.uri = '/'.join([
            parent_log.ref_resource_name, 'screen',
            cpr.screen.facility_id, parent_log.key])        
        
        lcp_query = cpr.lab_cherry_picks.all() 
        if lcp_query.exists():
            if cpr.lab_cherry_picks.exists():
                raise ValidationError({
                    SCHEMA.API_MSG_NOT_ALLOWED: 
                        ('Lab cherry picks already assigned: (%d); '
                        'delete lab cherry picks to change '
                        'screener cherry pick selections' 
                        % cpr.lab_cherry_picks.count()),
                    API_MSG_LCPS_MUST_BE_DELETED: cpr.lab_cherry_picks.count()
                })
        
        logger.info('create lcps (duplexes)...')
        # TODO: use bulk create to speed this up    
        lcps_created = []
        for scp in cpr.screener_cherry_picks.filter(selected=True):
            
            # Find duplex wells
            if ( not scp.screened_well.reagents.exists() 
                 or not hasattr(scp.screened_well.reagents.all()[0],'silencingreagent')):
                raise ValidationError(
                    key='screened_well',
                    msg='no silencing reagents found for screened well: %s' 
                        % scp.screened_well.well_id )
            sr = scp.screened_well.reagents.all()[0].silencingreagent
            if not sr.duplex_wells.exists():
                raise ValidationError(
                    key='screened_well',
                    msg='no duplex wells found for screened well: %s' 
                        % scp.screened_well.well_id )
            for duplex_well in sr.duplex_wells.all():
                logger.debug('creating lcp for scp: %r, %r', 
                    scp.screened_well.well_id, duplex_well.well_id)
                lab_cherry_pick = LabCherryPick.objects.create(
                    cherry_pick_request=cpr,
                    source_well=duplex_well,
                    screener_cherry_pick=scp)
                lcps_created.append(lab_cherry_pick)
            if len(lcps_created) %100 == 0:
                logger.info('created %d duplex lcps', len(lcps_created))
        logger.info('created %d duplex lcps', len(lcps_created))
        self.get_labcherrypick_resource().clear_cache(request)
        meta[API_MSG_LCPS_CREATED] = len(lcps_created)

        logger.info('Find and assign copies that are fulfillable...')
        meta_action = self._find_copies(cpr)
        if meta_action:
            meta.update(meta_action)
        
        new_cpr = self._get_detail_response_internal(**{
            'cherry_pick_request_id': cpr_id,
            'includes': '-screener_cherry_picks'
         })
        logger.debug('new_cpr: %r', new_cpr)

        self.log_patch(request, original_cpr, new_cpr, parent_log, 
            excludes=['screener_cherry_picks'])
        parent_log.json_field = json.dumps(meta, cls=LimsJSONEncoder)
        parent_log.save()
        logger.info('set_duplex_lab_cherry_picks: log: %r, %r', parent_log, parent_log.diffs)
        
        return self.build_response(
            request,  {API_RESULT_META: meta }, response_class=HttpResponse, **kwargs)
    
    @write_authorization
    @un_cache
    @transaction.atomic
    def dispatch_set_lab_cherry_picks(self, request, **kwargs):
        '''
        Create Lab Cherry Picks for the Screener Cherry Picks that have already
        been created for the CPR.
        - find copies for the LCPs:
            copy.usage_type = cherry_pick_source_plates
            plate.status = available
        - assign appropriate copies for the LCPs, if no appropriate copy can be
        found, leave the LCP unfulfilled (see _find_copies).
        '''
        logger.info('dispatch_set_lab_cherry_picks')
        request_method = request.method.lower()
        if request_method != 'post':
            raise BadRequestError(key='method', msg='Only POST is allowed')
        convert_request_method_to_put(request)

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
         
        cpr_id = param_hash['cherry_pick_request_id']
        logger.info(
            'dispatch_set_lab_cherry_picks for: %r...', cpr_id)
         
        schema = super(CherryPickRequestResource, self).build_schema(
            user=request.user)
         
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=cpr_id)
        if not cpr.screener_cherry_picks.filter(selected=True).exists():
            raise ValidationError(
                key='screener_cherry_picks', 
                msg='No Screener Cherry Picks have been submitted')
        
        original_cpr = self._get_detail_response_internal(**{
            'cherry_pick_request_id': cpr_id })
        
        meta = {}    
        
        parent_log = self.make_log(request)
        parent_log.key = str(cpr_id)
        # NOTE: make the log uri more robust, with screen id as well
        parent_log.uri = '/'.join([
            parent_log.ref_resource_name,'screen',cpr.screen.facility_id,
            parent_log.key])        
        
        lcp_query = cpr.lab_cherry_picks.all() 
        if lcp_query.exists():
            if cpr.lab_cherry_picks.exists():
                raise ValidationError({
                    SCHEMA.API_MSG_NOT_ALLOWED: 
                        ('Lab cherry picks already assigned: (%d); '
                        'delete lab cherry picks to change '
                        'screener cherry pick selections' 
                        % cpr.lab_cherry_picks.count()),
                    API_MSG_LCPS_MUST_BE_DELETED: cpr.lab_cherry_picks.count()
                })
            
        logger.info('create lcps...')    
        lcps_created = []
        for scp in cpr.screener_cherry_picks.filter(selected=True):
           lab_cherry_pick = LabCherryPick.objects.create(
                cherry_pick_request=cpr,
                source_well=scp.screened_well,
                screener_cherry_pick=scp)
           lcps_created.append(lab_cherry_pick)
        self.get_labcherrypick_resource().clear_cache(request)
        meta[API_MSG_LCPS_CREATED] = len(lcps_created)

        logger.info('Find and assign copies that are fulfillable...')
        meta_action = self._find_copies(cpr)
        if meta_action:
            meta.update(meta_action)
        
        new_cpr = self._get_detail_response_internal(**{
            'cherry_pick_request_id': cpr_id })
        self.log_patch(request, original_cpr, new_cpr, parent_log, 
            excludes=['screener_cherry_picks'])
        parent_log.json_field = json.dumps(meta, cls=LimsJSONEncoder)
        parent_log.save()
        logger.info('set_lab_cherry_picks: log: %r, %r', parent_log, parent_log.diffs)
        
        return self.build_response(
            request,  {API_RESULT_META: meta }, response_class=HttpResponse, **kwargs)

    def _find_copies(self,cpr):
        '''
        Find and assign copies that are fulfillable (sufficient volume, available) 
        for the Lab Cherry Picks on the Cherry Pick Request:
        - well copy
        @param cpr Cherry Pick Request with Lab Cherry Picks already created
        '''
        logger.info('find copies for cpr: %d lab cherry picks...', 
            cpr.cherry_pick_request_id)
        #select the "best" copy for each contiguous plate   
        
        if not cpr.lab_cherry_picks.all().exists():
            raise ValidationError(key='lab_cherry_picks', msg='must be set')
        
        self.clear_cache(None)
        logger.info('find eligible lab_cherry_pick copy wells...')
        eligible_lab_cherry_pick_copywells = \
            self.get_labcherrypick_resource()._get_list_response_internal(
                **{
                    'cherry_pick_request_id': cpr.cherry_pick_request_id,
                    API_PARAM_SHOW_COPY_WELLS: True,
                    'source_copy_well_volume__gte': cpr.transfer_volume_per_well_approved, 
                    'includes': [
                        'source_plate_type','destination_plate_type','source_well_id'
                        'source_copywell_id','-structure_image','-molfile',
                        '-library_plate_comment_array'],
                })
            
        # NOTE: do not consider the retired cherry_pick_source_plates for
        # the server-generated copy assignments
        eligible_lab_cherry_pick_copywells = [
            lcp for lcp in eligible_lab_cherry_pick_copywells
            if not (lcp['source_plate_type']=='cherry_pick_source_plates'
                    and lcp['source_plate_status']==SCHEMA.VOCAB.plate.status.RETIRED )]   
        
        logger.info('found %d eligible copy-wells for %d lab cherry picks', 
            len(eligible_lab_cherry_pick_copywells), 
            cpr.lab_cherry_picks.all().count())
        copy_wells_well_set = set([lcp['source_well_id'] for lcp in eligible_lab_cherry_pick_copywells])
        logger.info('lcp candidates found for source wells: %d', len(copy_wells_well_set))
        logger.debug('found eligible: %r', eligible_lab_cherry_pick_copywells)
        
        logger.info('Pick the best copy...')
        copy_sets_by_library = {}
        copy_instance_cache = {}
        pick_candidates_by_library = {}

        for pick_copy in eligible_lab_cherry_pick_copywells:
            
            source_well_id = pick_copy['source_well_id']
            copy_name = pick_copy['source_copy_name']
            library_short_name = pick_copy['library_short_name']
            copy_id = pick_copy['source_copy_id']
            copy_full_name = '%s:%s' % (library_short_name,copy_name)
            copy_instance_cache[copy_full_name] = Copy.objects.get(copy_id=copy_id)
            
            library_copy_set = copy_sets_by_library.get(library_short_name, set())
            library_copy_set.add(copy_full_name)
            copy_sets_by_library[library_short_name] = library_copy_set
            
            library_picks = pick_candidates_by_library.get(library_short_name,{})
            well_picks = library_picks.get(source_well_id, set())
            well_picks.add(copy_full_name)
            library_picks[source_well_id] = well_picks
            pick_candidates_by_library[library_short_name] = library_picks

        lcp_assigned_count = 0
        for library_short_name in pick_candidates_by_library.keys():
            
            copy_set = copy_sets_by_library.get(library_short_name,set())
            well_picks_for_library = pick_candidates_by_library[library_short_name]
            
            minimal_copy_set = lims_utils.find_minimal_satisfying_set(
                copy_set, well_picks_for_library.values())
            logger.info('library: %r, chosen minimal copy set: %r', 
                library_short_name, minimal_copy_set)
            if minimal_copy_set:
                # FIXME: only iterate the lcp's for the library
                for lcp in cpr.lab_cherry_picks.all():
                    if lcp.source_well_id in well_picks_for_library:
                        pick_copy_set = well_picks_for_library.get(lcp.source_well_id, None)
                        if pick_copy_set is None:
                            logger.info('no pick copy set for well: %r', lcp.source_well_id)
                            continue
                        eligible_copies = set(pick_copy_set) & set(minimal_copy_set)
                        if eligible_copies:
                            copy_full_name = sorted(eligible_copies)[0]
                            lcp.copy = copy_instance_cache[copy_full_name]
                            lcp.save()
                            lcp_assigned_count += 1
                        else:
                            logger.info('no eligible copies for %r, in %r',
                                lcp.source_well_id, pick_copy_set)
                    else:
                        logger.debug('no pick copy set for well %r', lcp.source_well_id)
            else:
                logger.info('no minimal copy sets found for library: %r', library_short_name)                
            
        meta = {}
        if lcp_assigned_count == 0:
            meta[SCHEMA.API_MSG_WARNING] = 'No eligible copies found'
        else:
            total_count = cpr.lab_cherry_picks.all().count()
            meta = {
                API_MSG_LCPS_CREATED: total_count,
                API_MSG_LCPS_ASSIGNED: lcp_assigned_count,
                API_MSG_LCPS_UNFULFILLED: (total_count-lcp_assigned_count)
            }
        return meta


class ScreenerCherryPickResource(DbApiResource):        

    class Meta:

        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'screenercherrypick'
        authorization = CherryPickRequestAuthorization(resource_name)
        
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        always_return_data = True 

    def __init__(self, **kwargs):

        super(ScreenerCherryPickResource, self).__init__(**kwargs)
        self.sr_resource = None
        self.smr_resource = None
        self.reagent_resource = None
        self.cpr_resource = None
        
    def get_sr_resource(self):
        if self.sr_resource is None:
            self.sr_resource = SilencingReagentResource()
        return self.sr_resource
    
    def get_smr_resource(self):
        if not self.smr_resource:
            self.smr_resource = SmallMoleculeReagentResource()
        return self.smr_resource
        
    def get_cpr_resource(self):
        if self.cpr_resource is None:
            self.cpr_resource = CherryPickRequestResource()
        return self.cpr_resource
    
    def clear_cache(self, request, **kwargs):
        DbApiResource.clear_cache(self, request, **kwargs)
        self.get_cpr_resource().clear_cache(request, **kwargs)
    
    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),

            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)"
                r"/(?P<source_well_id>[\d]+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
        ]

    @write_authorization
    @un_cache
    @transaction.atomic
    def post_list(self, request, schema=None, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'post_list')
    

    @write_authorization
    @transaction.atomic
    @un_cache        
    def patch_list(self, request, **kwargs):
        '''
        Allow patch list for changing selections only 
         - no new picks can be created through patch, picks can only be
         created on the "patch_detail" with the 
         cherry_pick_request.screener_cherry_picks attribute
        '''
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        
        if 'cherry_pick_request_id' not in kwargs:
            raise BadRequestError(key='cherry_pick_request_id', msg='required')
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=kwargs['cherry_pick_request_id'])
        logger.info(
            'patch_list: cpr: %r, screen: %r...', cpr, cpr.screen.facility_id)
         
        if cpr.lab_cherry_picks.exists():
            raise ValidationError({
                SCHEMA.API_MSG_NOT_ALLOWED: 
                    ('Lab cherry picks already assigned: (%d); '
                    'delete lab cherry picks to change '
                    'screener cherry pick selections' 
                    % cpr.lab_cherry_picks.count()),
                API_MSG_LCPS_MUST_BE_DELETED: cpr.lab_cherry_picks.count()
            })
            
        deserialized, deserialize_meta = self.deserialize(request)
        if self._meta.collection_name in deserialized:
            deserialized = deserialized[self._meta.collection_name]
 
#         id_attribute = schema['id_attribute']

        original_cpr_data = self.get_cpr_resource()._get_detail_response_internal(**{
            'cherry_pick_request_id': cpr.cherry_pick_request_id })
        
        original_data = self._get_list_response_internal(**{
            'includes': '*',
            'cherry_pick_request_id': kwargs['cherry_pick_request_id'],
            API_PARAM_SHOW_OTHER_REAGENTS: True })
        if not original_data: 
            raise BadRequestError(
                key='method',
                msg='Can not set Screener Cherry Picks using patch; '
                'must use the cherry pick request selection patch instead')
        
        original_selections = { scp_data['screened_well_id']: scp_data
            for scp_data in original_data }
        current_selections = {}
        for scp_data in original_data:
            screened_well_id = scp_data['screened_well_id']
            searched_well_id = scp_data['searched_well_id']
            if scp_data['selected'] is True:
                if searched_well_id in current_selections:
                    raise ProgrammingError(
                        'Original ScreenerCherryPicks have a duplicate:'
                        'searched_well: %r, has >1 selection: %r'
                        % (searched_well_id, original_data))
                current_selections[searched_well_id] = screened_well_id
        # Validate selections are valid and allowed:
        required_for_patch = set([
            'screened_well_id', 'searched_well_id', 'selected'])
        selection_updates = {}
        for selection_update in deserialized:
            if not required_for_patch.issubset(set(selection_update.keys())):
                raise ValidationError(
                    key='required_fields',
                    msg=(
                        'record: %r, missing fields: %r'
                        % (selection_update,
                        required_for_patch-set(selection_update.keys())))
                    )
            candidate_screened_well_id = selection_update['screened_well_id']
            if not candidate_screened_well_id in original_selections:
                raise ValidationError(
                    key='screened_well_id',
                    msg='not found in the current/alternate screener_cherry_picks: %r' 
                        % candidate_screened_well_id )
            selection_update['selected'] = parse_val(
                selection_update['selected'],'selected', 'boolean')
            selection_updates[selection_update['screened_well_id']]=selection_update
        # validate only one selection per searched_well
        for screened_well_id,selection_update in selection_updates.items():
            if selection_update['selected'] is True:
                searched_well_id = selection_update['searched_well_id']
                
                for other_well_id, su2 in selection_updates.items():
                    if other_well_id != screened_well_id:
                        if su2['searched_well_id'] == searched_well_id:
                            if su2['selected'] is True:
                                raise ValidationError(
                                    key='selected',
                                    msg=(
                                        'only one well can be selected for '
                                        'the searched_well_id: %r' % searched_well_id))
                if ( searched_well_id in current_selections
                    and searched_well_id not in selection_updates):
                    selection_updates[searched_well_id] = {
                        'selected': False,
                        'searched_well_id': searched_well_id,
                        'screened_well_id': screened_well_id }
        # make sure others in group are deselected. 
        # Fixme: tidy up this logic
        for scp_data in original_data:
            if scp_data['selected'] is True:
                for selection_update in selection_updates.values():
                    if selection_update['searched_well_id']==scp_data['searched_well_id']:
                        if selection_update['selected'] is True:
                            scp_data['selected'] = False
                            selection_updates[scp_data['screened_well_id']] = \
                                scp_data
                            break
        
        if not selection_updates:
            return self.build_response(
                request,  {API_RESULT_META: 'no new Selections found' }, 
                response_class=HttpResponse, **kwargs)
        
        logger.info('selection updates: %r', 
            [(scp['screened_well_id'],scp['selected'],scp['searched_well_id']) 
                for scp in selection_updates.values()])
                    
        original_scps = { 
            scp.screened_well_id: scp 
                for scp in cpr.screener_cherry_picks.all() }
        
        logger.debug('original_scps: %r', original_scps.keys())
        
        messages = []
        scps_to_create = []
        scps_to_reselect = []
        scps_to_unselect = []
        for screened_well_id, scp_selection_update in selection_updates.items():
            selected = scp_selection_update['selected']
            if selected is True:
                if screened_well_id in original_scps:
                    if original_scps[screened_well_id].selected is True:
                        messages.append(
                            'screened_well_id: %r is already selected'  % screened_well_id)
                    else:
                        scps_to_reselect.append(screened_well_id)
                else:
                    scps_to_create.append(screened_well_id)
            else:
                if screened_well_id in original_scps:
                    if original_scps[screened_well_id].selected is True:
                        scps_to_unselect.append(screened_well_id)
                    else:
                        messages.append(
                            'not currently selected: %r' % screened_well_id)
                else:
                    messages.append(
                        'not unselecting, well not found in original_scps: %r'
                        % screened_well_id)
        
        if not scps_to_create and not scps_to_reselect and not scps_to_unselect:
            _data = { API_RESULT_META: messages }
            return self.build_response(
                request, _data, response_class=HttpResponse, **kwargs)
        
        if scps_to_create:
            cherry_pick_wells = \
                CherryPickRequestResource.find_wells(scps_to_create)
                
            for well in cherry_pick_wells:
                full_selection = original_selections[well.well_id]
                searched_well_id = full_selection['searched_well_id']
                screener_cherry_pick = ScreenerCherryPick.objects.create(
                    cherry_pick_request=cpr,
                    screened_well=well,
                    searched_well_id=searched_well_id,
                    selected=True)
                
                # TODO: if selected well is restricted library, 'requires permission'
                # add user warning message
        if scps_to_unselect:
            for screened_well_id in scps_to_unselect:
                original_scps[screened_well_id].selected = False
                original_scps[screened_well_id].save()
        if scps_to_reselect:
            for screened_well_id in scps_to_reselect:
                original_scps[screened_well_id].selected = True
                original_scps[screened_well_id].save()
        
        result_message = {
            API_MSG_SCP_CREATED: ', '.join(scps_to_create),
            API_MSG_SCP_UNSELECTED: ', '.join(scps_to_unselect),
            API_MSG_SCP_RESELECTED: ', '.join(scps_to_reselect)
        }
        if messages:
            result_message[SCHEMA.API_MSG_COMMENTS] = messages
        
        self.get_cpr_resource().clear_cache(request)
        new_cpr_data = self.get_cpr_resource()._get_detail_response_internal(
            **{ 'cherry_pick_request_id': cpr.cherry_pick_request_id })
        logger.debug('old cpr: %r, new cpr: %r', original_cpr_data, new_cpr_data)
        log = self.get_cpr_resource().make_log(request)
        log.key = str(cpr.cherry_pick_request_id)
        # NOTE: make the log uri more robust, with screen id as well
        log.uri = '/'.join([
            log.ref_resource_name,'screen',cpr.screen.facility_id,log.key])        
        log.json_field = json.dumps(result_message, cls=LimsJSONEncoder)
        self.get_cpr_resource().log_patch(
            request, original_cpr_data,new_cpr_data, 
            excludes=['screener_cherry_picks'])
        logger.info('cpr_log: %r, %r',log, log.diffs)
        # Note: because the actual screener cherry picks are not being logged, 
        # log the count, even if it has not changed
        log.diffs['screener_cherry_pick_count'] = [
            original_cpr_data['screener_cherry_pick_count'],
            new_cpr_data['screener_cherry_pick_count']]
        log.save()
        if deserialize_meta:
            result_message.update(deserialize_meta)
        
        _data = { API_RESULT_META: result_message }
        return self.build_response(
            request, _data, response_class=HttpResponse, **kwargs)


    @read_authorization
    def get_detail(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def get_schema(self, request, **kwargs):
        if not 'cherry_pick_request_id' in kwargs:
            return self.build_response(request, 
                self.build_schema(user=request.user),**kwargs)
        
        cherry_pick_request_id = kwargs.pop('cherry_pick_request_id')
        try:
            cpr = CherryPickRequest.objects.get(
                cherry_pick_request_id=cherry_pick_request_id)
            # NOTE: does not support disinction between Small Molecule and Natural Product
            return self.build_response(
                request, 
                self.build_schema(
                    user=request.user, 
                    library_classification=cpr.screen.screen_type),
                **kwargs)
            
        except ObjectDoesNotExist, e:
            raise Http404(
                'Can not build schema - CherryPickRequest ID needed'
                'no cpr found for cherry_pick_request_id: %r' % cherry_pick_request_id)

    def build_schema(self, library_classification=None, user=None, **kwargs):
        logger.debug('build sreenercherrypick schema for library_classification: %r',
            library_classification)
        schema = deepcopy(
            super(ScreenerCherryPickResource, self).build_schema(user=user, **kwargs))
        original_fields = schema['fields']
        logger.debug('original_fields: %r', original_fields.keys())
        if library_classification:
            # Add in reagent fields
            sub_data = self.get_reagent_resource(library_classification)\
                .build_schema(user, **kwargs)
            
            newfields = {}
            newfields.update(sub_data['fields'])
            schema['fields'] = newfields
        
        # Add in well fields    
        well_schema = WellResource().build_schema(user, **kwargs)
        schema['fields'].update(well_schema['fields'])
        
        # Turn off the visibility of all inherited fields
        for key,field in schema['fields'].items():
            if not set(VOCAB.field.visibility.hidden_fields) & set(field['visibility']):
                field['visibility'] = []
        
        # Overlay the original scp fields on the top
        schema['fields'].update(original_fields)
        logger.debug('schema  fields: %r', schema['fields'].keys())
        logger.debug('schema  fields: %r', 
            [(key, field['visibility']) for key,field in schema['fields'].items()])
        return schema

    def build_sqlalchemy_columns(self, fields, base_query_tables=None, 
            custom_columns=None):
        sub_columns = self.get_sr_resource().build_sqlalchemy_columns(fields)
        sub_columns.update(self.get_smr_resource().build_sqlalchemy_columns(fields))
        if custom_columns is not None:
            sub_columns.update(custom_columns)
        return DbApiResource.build_sqlalchemy_columns(self, 
            fields, base_query_tables=base_query_tables, 
            custom_columns=sub_columns)

    def build_list_response(self, request, schema=None, **kwargs):

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False

        is_for_detail = kwargs.pop('is_for_detail', False)
        extra_params = {}
        cherry_pick_request_id = param_hash.pop('cherry_pick_request_id', None)
        if cherry_pick_request_id is None:
            raise BadRequestError(key='cherry_pick_request_id', msg='required')
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=cherry_pick_request_id)
        extra_params['CPR']=cherry_pick_request_id
        source_well_id = param_hash.pop('source_well_id', None)
        if source_well_id:
            param_hash['source_well_id__eq'] = source_well_id

        show_other_reagents = parse_val(
            param_hash.get(API_PARAM_SHOW_OTHER_REAGENTS, None),
            API_PARAM_SHOW_OTHER_REAGENTS, 'boolean')
        if show_other_reagents is True:
            extra_params[API_PARAM_SHOW_OTHER_REAGENTS] = None

        show_alternates = parse_val(
            param_hash.get(API_PARAM_SHOW_ALTERNATE_SELECTIONS, None),
            API_PARAM_SHOW_ALTERNATE_SELECTIONS, 'boolean')
        if show_alternates is True:
            extra_params[API_PARAM_SHOW_ALTERNATE_SELECTIONS] = None
            
        # Note: build schema for each request to use the subtype
        schema = self.build_schema(library_classification=cpr.screen.screen_type)
        logger.debug('schema  fields: %r', schema['fields'].keys())
        
        # general setup
         
        manual_field_includes = set(param_hash.get('includes', []))
        manual_field_includes.add('searched_well_id')
        manual_field_includes.add('selected')
        manual_field_includes.add('cherry_pick_request_id')
        
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filename = self._get_filename(
            readable_filter_hash, schema, **extra_params)
        filter_expression = \
            self._meta.authorization.filter(request.user,filter_expression)
                                 
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
            
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)

        # specific setup 
        base_query_tables = [
            'screen',
            'cherry_pick_request',
            'well',
            'reagent',
            'library'
            ]
        # build the query statement
        _cpr = self.bridge['cherry_pick_request']
        _scp = self.bridge['screener_cherry_pick']
        _lcp = self.bridge['lab_cherry_pick']
        _screen = self.bridge['screen']
        _well = self.bridge['well']
        _reagent = self.bridge['reagent']
        _library = self.bridge['library']
           
        if show_other_reagents is True or show_alternates is True:
            logger.info('show_other_reagents: %r, show_alternates: %r', 
                        show_other_reagents, show_alternates)
            _original_scps = (
                select([
                    _scp.c.screener_cherry_pick_id,
                    _scp.c.screened_well_id,
                    _scp.c.searched_well_id,
                    _scp.c.selected,
                    _scp.c.cherry_pick_request_id,
                    _reagent.c.vendor_identifier, 
                    _reagent.c.vendor_name])
                .select_from(
                    _scp.join(_reagent, _scp.c.screened_well_id==_reagent.c.well_id)
                        .join(_well,_reagent.c.well_id==_well.c.well_id))
                .where(_scp.c.cherry_pick_request_id==cherry_pick_request_id)
                .where(_scp.c.searched_well_id==_scp.c.screened_well_id)
                )
            _original_scps = _original_scps.cte('original_scps')
        
            _alternates = (
                select([
                    _reagent.c.well_id,
                    # NOTE: only allow an alternate to be listed for the first 
                    # searched_well_id, if there are multiple searched_well_id's 
                    # that match the reagent
                    func.min(_original_scps.c.searched_well_id).label('searched_well_id')
                ])
                .select_from(
                    _reagent.join(
                        _original_scps, 
                        and_(_reagent.c.vendor_identifier==_original_scps.c.vendor_identifier,
                             _reagent.c.vendor_name==_original_scps.c.vendor_name)))
                .where(_reagent.c.vendor_identifier != '')
                .where(_reagent.c.well_id!=_original_scps.c.searched_well_id)
                .where(not_(_reagent.c.well_id.in_(
                    select([_scp.c.screened_well_id])
                    .select_from(_scp)
                    .where(_scp.c.cherry_pick_request_id==cherry_pick_request_id))))
                .group_by(_reagent.c.well_id)
                ).cte('alternate_scps')
            combined_scps = union(
                select([
                    _scp.c.screener_cherry_pick_id,
                    _scp.c.searched_well_id,
                    _scp.c.screened_well_id,
                    _scp.c.selected,
                    _scp.c.cherry_pick_request_id,
                    _reagent.c.vendor_identifier,
                    _reagent.c.vendor_name])
                .select_from(
                    _scp.join(
                        _reagent,_scp.c.screened_well_id==_reagent.c.well_id))
                .where(_scp.c.cherry_pick_request_id==cherry_pick_request_id),
                select([
                    literal_column("0"),
                    _alternates.c.searched_well_id,
                    _alternates.c.well_id,
                    literal_column('false').label('selected'),
                    literal_column(cherry_pick_request_id).label('cherry_pick_request_id'),
                    _reagent.c.vendor_identifier,
                    _reagent.c.vendor_name
                    ])
                .select_from(
                    _alternates.join(
                        _reagent,_alternates.c.well_id==_reagent.c.well_id))
                # .where(not_(_alternates.c.well_id.in_(
                #     select([_scp.c.screened_well_id])
                #     .select_from(_scp)
                #     .where(_scp.c.cherry_pick_request_id==cherry_pick_request_id))))
                )
            combined_scps = combined_scps.cte('combined')
            
            working_scp = combined_scps
        else:
            working_scp = _scp
               
        custom_columns = {
            'screener_cherry_pick_id': working_scp.c.screener_cherry_pick_id,
            'screened_well_id': working_scp.c.screened_well_id,
            'selected': working_scp.c.selected,
            'cherry_pick_request_id': working_scp.c.cherry_pick_request_id,
        }
        custom_columns['searched_well_id'] = working_scp.c.searched_well_id
            
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)
        
        j = join(working_scp,_cpr, 
            working_scp.c.cherry_pick_request_id == _cpr.c.cherry_pick_request_id)
        j = j.join(_screen,
            _cpr.c.screen_id == _screen.c.screen_id)
        j = j.join(_well, _well.c.well_id==working_scp.c.screened_well_id)
        j = j.join(_library, _well.c.library_id==_library.c.library_id)
        j = j.join(_reagent, working_scp.c.screened_well_id==_reagent.c.well_id)
           
        stmt = select(columns.values()).select_from(j)
        stmt = stmt.where(_cpr.c.cherry_pick_request_id==cherry_pick_request_id)
        
        if show_alternates is True:
            with get_engine().connect() as conn:
                _alternates = (
                    select([distinct(_scp.c.searched_well_id)])
                    .select_from(_scp)
                    .where(_scp.c.cherry_pick_request_id==cherry_pick_request_id)
                    .where(_scp.c.searched_well_id!=_scp.c.screened_well_id)
                    .where(_scp.c.selected == True)
                    )
                alternate_searched_well_ids = set([x[0] for x in 
                    conn.execute(_alternates)])
                stmt = stmt.where(working_scp.c.searched_well_id.in_(alternate_searched_well_ids))
        
        if show_other_reagents is False and show_alternates is False:
            stmt = stmt.where(working_scp.c.selected==True)
        
        # general setup
            
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        
        if not order_clauses:
            # Ordering for well_id must be alphanumeric
            # For string field ordering, double sort as numeric and text
            order_clause = text(
                "(substring({field_name}, '^[0-9]+'))::int asc " # cast to integer
                ",substring({field_name}, ':(.*$)') asc  "  # works as text
                .format(field_name='searched_well_id'))
            if show_other_reagents is True or show_alternates is True:
                
                stmt = stmt.order_by(
                    order_clause,
                    desc(column('searched_well_id')==column('screened_well_id')),
                    desc(column('library_plate')),
                    desc(column('screened_well_id')),
                )
            else:
                stmt = stmt.order_by(
                    order_clause,
                    asc(column('library_plate')),
                    asc(column('screened_well_name'))
                    )
           
        # compiled_stmt = str(stmt.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)
        
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
        
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None))
        
             
class LabCherryPickResource(DbApiResource):        

    LCP_COPYWELL_KEY = '{source_copy_name}/{source_well_id}'

    class Meta:

        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'labcherrypick'
        authorization = CherryPickRequestAuthorization(resource_name)
        
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        always_return_data = True 

    def __init__(self, **kwargs):

        super(LabCherryPickResource, self).__init__(**kwargs)
        self.sr_resource = None
        self.smr_resource = None
        self.reagent_resource = None
        self.cpr_resource = None
        self.copywell_resource = None
    
    def get_copywell_resource(self):
        if self.copywell_resource is None:
            self.copywell_resource = CopyWellResource()
        return self.copywell_resource
        
    def get_sr_resource(self):
        if self.sr_resource is None:
            self.sr_resource = SilencingReagentResource()
        return self.sr_resource
    
    def get_smr_resource(self):
        if not self.smr_resource:
            self.smr_resource = SmallMoleculeReagentResource()
        return self.smr_resource
        
    def get_cpr_resource(self):
        if self.cpr_resource is None:
            self.cpr_resource = CherryPickRequestResource()
        return self.cpr_resource
    
    def clear_cache(self, request, **kwargs):
        DbApiResource.clear_cache(self, request, **kwargs)
        self.get_cpr_resource().clear_cache(request, **kwargs)
    
    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),

            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)"
                r"/(?P<source_well_id>[\d]+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
        ]
        
    @write_authorization
    @un_cache
    @transaction.atomic
    def patch_list(self, request, **kwargs):
        
        DEBUG_LCP = False or logger.isEnabledFor(logging.DEBUG)
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        cw_formatter = LabCherryPickResource.LCP_COPYWELL_KEY
        
        # TODO: should be handled by IccblBaseResource.dispatch ?
        convert_request_method_to_put(request)
        
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        param_override = parse_val(
            param_hash.get(API_PARAM_OVERRIDE, False),
                API_PARAM_OVERRIDE, 'boolean')
        set_deselected_to_zero = parse_val(
            param_hash.get(API_PARAM_SET_DESELECTED_TO_ZERO, False),
                API_PARAM_SET_DESELECTED_TO_ZERO, 'boolean')
#         id_attribute = schema['id_attribute']

        if 'cherry_pick_request_id' not in kwargs:
            raise BadRequestError(key='cherry_pick_request_id', msg='required')
        cpr_id = kwargs['cherry_pick_request_id']
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=cpr_id)
        logger.info(
            'patch_list: cpr: %r, screen: %r...', cpr, cpr.screen.facility_id)
        original_cpr = \
            self.get_cpr_resource()._get_detail_response_internal(
                **{'cherry_pick_request_id': cpr_id })
        # NOTE: not logging all LCP updates at this time, copywell logs can be used
        # original_lab_cherry_pick_copywells = \
        #     self._get_list_response_internal(
        #         **{
        #             'cherry_pick_request_id': cpr_id,
        #             'source_copy_name__is_null': False,
        #             'includes': '*'
        #         })
         
        deserialized, deserialize_meta = self.deserialize(request)
        if self._meta.collection_name in deserialized:
            deserialized = deserialized[self._meta.collection_name]
        
        self.get_cpr_resource().validate_cpr_for_plating(cpr) 

        if not cpr.lab_cherry_picks.all().exists():
            raise ValidationError(
                key='lcp_selection_updates', 
                msg='No Lab Cherry Picks have been created')
        current_lcps = { 
            lcp.source_well_id:lcp for lcp in cpr.lab_cherry_picks.all() }
        parent_log = self.get_cpr_resource().make_log(request)
        parent_log.key = str(cpr.cherry_pick_request_id)
        # NOTE: make the log uri more robust, with screen id as well
        parent_log.uri = '/'.join([
            parent_log.ref_resource_name,'screen',cpr.screen.facility_id,
            parent_log.key])        
        parent_log.save()

        is_mapped = cpr.lab_cherry_picks.filter(
            cherry_pick_assay_plate__isnull=False).exists()
        if is_mapped is True:
            plated_assay_plates_query = \
                cpr.cherry_pick_assay_plates.filter(
                    plating_date__isnull=False)                
            if plated_assay_plates_query.exists():
                raise ValidationError({
                    SCHEMA.API_MSG_NOT_ALLOWED: API_MSG_CPR_PLATED_CANCEL_DISALLOWED, 
                    API_MSG_LCP_PLATES_ASSIGNED: cpr.cherry_pick_assay_plates.count(),
                    API_MSG_LCP_ASSAY_PLATES_PLATED: plated_assay_plates_query.count()
                })
            if param_override is not True:
                raise ValidationError({
                    API_PARAM_OVERRIDE: 'required',
                    SCHEMA.API_MSG_WARNING: 
                        ('Lab cherry pick plates already assigned; '
                        'cancel reservation to deallocate plates, '
                        'or choose override to keep plating and change assignments'),
                    API_MSG_LCP_PLATES_ASSIGNED: cpr.cherry_pick_assay_plates.count()
                })
                
        # Validate selections are valid and allowed:
        required_for_patch = set([
            'source_well_id', 'selected'])
        copies = {}
        plates = {}
        selection_updates = {}
        selections_per_well = defaultdict(list)
        for selection_update in deserialized:
            if DEBUG_LCP:
                logger.info('selection update: %r', selection_update)
            if not required_for_patch.issubset(set(selection_update.keys())):
                raise ValidationError(
                    key='required_fields',
                    msg=(
                        'record: %r, missing fields: %r'
                        % (selection_update,
                        required_for_patch-set(selection_update.keys())))
                    )
            copy_id_fields = ['source_copy_id','source_copy_name']
            if 'source_copy_id' not in selection_update.keys():
                if 'source_copy_name' not in selection_update.keys():
                    raise ValidationError(
                        key='required_fields',
                        msg=('record: %r, must contain either: %r'
                            % (selection_update,copy_id_fields)))
            if set(copy_id_fields).issubset(set(selection_updates.keys())):
                raise ValidationError(
                    key='copy_id_fields',
                    msg='May only include one of: %r' % copy_id_fields)

            source_well_id = selection_update.get('source_well_id')
            if source_well_id not in current_lcps:
                error_dict = {
                    'source_well_id': 
                        'not a current lab cherry pick: %r' % source_well_id }
                if is_mapped:
                    error_dict[SCHEMA.API_MSG_WARNING] = (
                        'lab cherry pick reservation must be canceled in '
                        'order to add new selections')
                raise ValidationError(error_dict)
            # locate copies:
            copy = None
            plate = None
            plate_number = lims_utils.well_id_plate_number(source_well_id)
            source_copy_id = selection_update.get('source_copy_id', None)
            if source_copy_id:
                # TODO: Raise a nice Validation Error
                copy = Copy.objects.get(copy_id=source_copy_id)
                plate = Plate.objects.get(
                    plate_number=plate_number,copy=copy)
            else:
                source_copy_name = selection_update.get('source_copy_name')
                plate = Plate.objects.get(
                    plate_number=plate_number,copy__name=source_copy_name)
                copy = plate.copy
            
            selection_update['copy'] = copy
            selection_update['plate'] = plate # cache the plate for later use
            selection_update['source_copy_name'] = copy.name
            selection_update['library_short_name'] = copy.library.short_name
            selection_copy_name = cw_formatter.format(**selection_update)
            selection_updates[selection_copy_name] = selection_update
            
            if selection_update['selected'] is True:
                selections_per_well[source_well_id].append(copy.name)
                
        logger.info('selection_updates: %r', selection_updates.keys())
        # Check that only one copy is selected per well
        errors = []
        for source_well_id, copies in selections_per_well.items():
            if len(copies) > 1:
                errors.append('%s: %s' %(source_well_id,','.join(copies)))
        if errors:
            logger.info('errors: %r', errors)
            raise ValidationError(
                key = API_MSG_LCP_MULTIPLE_SELECTIONS_SUBMITTED,
                msg = '\n'.join(errors))
        logger.debug('selection_updates: %r', selection_updates.keys())
        
        lcps_to_deselect = set()
        # First, find all of the deselections
        for selection_copy_name, selection_update in selection_updates.items():
            source_well_id = selection_update['source_well_id']
            current_lcp = current_lcps[source_well_id]
            current_copy_name = None
            if current_lcp.copy is not None:
                current_copy_name = cw_formatter.format(
                    library_short_name=current_lcp.copy.library.short_name,
                    source_copy_name=current_lcp.copy.name,
                    source_well_id=source_well_id)
            if selection_update['selected'] is True: 
                if selection_copy_name != current_copy_name:
                    logger.info('lcp to change: %r change %r to %r', 
                        source_well_id, current_copy_name, selection_copy_name)
                    lcps_to_deselect.add(current_lcp)
                else:
                    logger.info('lcp is already selected: %r, %r', 
                        current_copy_name, selection_update)
            else:
                if selection_copy_name == current_copy_name:
                    logger.info('lcp to deselect: %r', current_lcp)
                    lcps_to_deselect.add(current_lcp)
                else:
                    logger.info('lcp is already unselected: %r, %r', 
                        current_copy_name, selection_update)
        
        # Second, if mapped and deselections exist, deallocate
        result_meta_allocate = {}
        if is_mapped is True and len(lcps_to_deselect) > 0:
            logger.info('Already mapped, lcps to deallocate: %r', 
                lcps_to_deselect)
            # Note: signal update_screening_count=False
            # do not adjust the plate.cplt_screening_count or the 
            # copywell.cherry_pick_screening_count (see deallocate_cherry_pick_volumes for
            # rational.
            result_meta_allocate = \
                self.get_copywell_resource().deallocate_cherry_pick_volumes(
                    cpr, lcps_to_deselect, parent_log,
                    set_deselected_to_zero=set_deselected_to_zero,
                    update_screening_count=False)

        # Third, update the LCPS
        changed = []
        deselected = []
        selected = []
        lcps_to_allocate = set()
        for selection_copy_name, selection_update in selection_updates.items():
            source_well_id = selection_update['source_well_id']
            current_lcp = current_lcps[source_well_id]
            current_copy_name = None
            if current_lcp.copy is not None:
                current_copy_name = cw_formatter.format(
                    library_short_name=current_lcp.copy.library.short_name,
                    source_copy_name=current_lcp.copy.name,
                    source_well_id=source_well_id)
            logger.info('current lcp: %r, %r', current_copy_name, current_lcp)
            logger.info('selection_update: %r to %r', 
                selection_copy_name, selection_update['selected'])
            if selection_update['selected'] is True: 
                if selection_copy_name != current_copy_name:
                    if current_copy_name is None:
                        selected.append(selection_copy_name)
                    else:
                        changed.append([
                            current_copy_name, selection_copy_name])
                    current_lcp.copy = selection_update['copy']
                    current_lcp.is_manually_selected = True
                    current_lcp.save()
                    lcps_to_allocate.add(current_lcp)
                else:
                    logger.info('lcp is already selected: %r, %r', 
                        current_copy_name, selection_update)
            else:
                if selection_copy_name == current_copy_name:
                    deselected.append(current_copy_name)
                    current_lcp.copy = None
                    current_lcp.is_manually_selected = False
                    current_lcp.save()
                else:
                    logger.info('lcp is already unselected: %r, %r', 
                        current_copy_name, selection_update)

        # Fourth, if selection updates exist, and already is_mapped, allocate
        # NOTE: volume will be taken without requiring overrides 
        # for insufficient volume (User has already sent an override for 
        # mapping changes)
        if is_mapped is True and len(lcps_to_allocate) > 0:
            logger.info('Already mapped, extra lcps to allocate: %r', 
                lcps_to_deselect)
            # Plate cplt_screening_count should only be updated if this is the 
            # first lcp for the plate. If lcps already exist for the plate,
            # then the cplt_screening_count has already been adjusted.
            new_plate_assignments = \
                set([lcp['plate'] for lcp in selection_updates.values()])
            current_plate_assignments = set()
            for lcp in [lcp for 
                lcp in current_lcps.values() if lcp.copy is not None]:
                copy = lcp.copy
                try:
                    plate = Plate.objects.get(
                        plate_number=lcp.source_well.plate_number, 
                        copy=lcp.copy)
                except ObjectDoesNotExist:
                    logger.exception('Can not find plate: %r, copy: %r', 
                        lcp.source_well.plate_number, lcp.copy)
                    raise
                new_plate_assignments.add(plate)
            plates_to_ignore = current_plate_assignments-new_plate_assignments
            result_meta = \
                self.get_copywell_resource().reserve_cherry_pick_volumes(
                    cpr, lcps_to_allocate, parent_log, 
                    plates_to_ignore=plates_to_ignore)
            result_meta_allocate.update(result_meta)
            
            cpr.date_volume_reserved = _now().date() 
            cpr.save()
        
        changed = sorted(changed)
        deselected = sorted(deselected)
        selected = sorted(selected)
        
        # Check for insufficient well volumes
        unfulfillable_wells = []
        new_lab_cherry_pick_copywells = \
            self._get_list_response_internal(
                **{
                    'cherry_pick_request_id': cpr.cherry_pick_request_id,
                    'source_copy_name__is_null': False,
                    'includes': [
                        'source_plate_type','destination_plate_type',
                        'source_copywell_id','source_copy_well_volume',
                        'volume_approved',
                        '-structure_image','-molfile', 
                        '-library_plate_comment_array'],
                })
        for lcp_cw in new_lab_cherry_pick_copywells:
            name = cw_formatter.format(**lcp_cw)
            if ( Decimal(lcp_cw['source_copy_well_volume'])
                    < cpr.transfer_volume_per_well_approved ):
                logger.info(
                    'vol requires override: lcp_cw: %r, approved: %r, available: %r', 
                    name, cpr.transfer_volume_per_well_approved, 
                    Decimal(lcp_cw['source_copy_well_volume']))
                unfulfillable_wells.append(cw_formatter.format(**lcp_cw))
            if DEBUG_LCP:
                logger.info('checking: %r < %r',
                    Decimal(lcp_cw['source_copy_well_volume']),
                    cpr.transfer_volume_per_well_approved)
        warning_messages = {}
        if unfulfillable_wells:
            warning_messages[API_MSG_LCPS_INSUFFICIENT_VOLUME] = unfulfillable_wells
        if DEBUG_LCP:
            logger.info('warning msg: %r', warning_messages)
        # final tally:
        _meta = {
            API_MSG_LCP_CHANGED: changed,
            API_MSG_LCP_DESELECTED: deselected,
            API_MSG_LCP_SELECTED: selected,
            SCHEMA.API_MSG_WARNING: warning_messages,
        }
        if result_meta_allocate:
            _meta.update(result_meta_allocate)
        if deserialize_meta:
            _meta.update(deserialize_meta)
        
        logger.info('result_meta: %r', _meta)
        
        self.get_cpr_resource().clear_cache(request)
        new_cpr = self.get_cpr_resource()._get_detail_response_internal(**{
            'cherry_pick_request_id': cpr_id })
        self.get_cpr_resource().log_patch(
            request, original_cpr, new_cpr, log=parent_log)
        # Store the cpr.date_volume_reserved as a marker even if not changed
        if cpr.date_volume_reserved and 'date_volume_reserved' not in parent_log.diffs:
            parent_log.diffs['date_volume_reserved'] = [
                cpr.date_volume_reserved,cpr.date_volume_reserved]
        parent_log.json_field = json.dumps(_meta, cls=LimsJSONEncoder)
        parent_log.save()

        # TODO: not logging all lcp changes at this time; copywell child logs 
        # can be used 
        # original_lab_cherry_pick_copywells = {
        #     lcp['source_well_id']:lcp for lcp in original_lab_cherry_pick_copywells}
        # new_lab_cherry_pick_copywells = {
        #     lcp['source_well_id']:lcp for lcp in new_lab_cherry_pick_copywells}

        return self.build_response(
            request, { API_RESULT_META: _meta }, 
            response_class=HttpResponse, **kwargs)

    def get_schema(self, request, **kwargs):
        ''' Generate an HttpResponse for the schema '''
        
        return self.build_response(
            request, self.build_schema(user=request.user,**kwargs))

    def build_schema(self, library_classification=None, user=None, **kwargs):

        if library_classification is None:
            cpr_id = kwargs.get('cherry_pick_request_id', None)
            if cpr_id is not None:
                try:
                    cpr = CherryPickRequest.objects.get(pk=cpr_id)
                    library_classification=cpr.screen.screen_type
                except:
                    logger.exception('Note: building generic lab cherry pick schema'
                        '(no cherry_pick_request_id provided)')
        try:
            schema = deepcopy(
                super(LabCherryPickResource, self).build_schema(user=user, **kwargs))
            original_fields = schema['fields']
            # 20170516 - keep the well_id field; required for the structure_image field
            # omit_redundant_fields = [
            #     'well_id','plate_number','well_name','library_well_type',
            #     'library_short_name','library_name']
            omit_redundant_fields = [
                'plate_number','well_name','library_well_type',
                'library_short_name','library_name']
            if library_classification:
                # Add in reagent fields
                sub_data = self.get_reagent_resource(library_classification)\
                    .build_schema(user, **kwargs)
                newfields = {}
                sub_fields = {key:field for key,field 
                    in sub_data['fields'].items() 
                        if key not in omit_redundant_fields}
                newfields.update(sub_fields)
                schema['fields'] = newfields
            
            # Add in well fields    
            well_schema = WellResource().build_schema(user, **kwargs)
            sub_fields = {key:field for key,field 
                in well_schema['fields'].items() 
                    if key not in omit_redundant_fields}
            schema['fields'].update(sub_fields)
            
            # Turn off the visibility of all inherited fields
            for key,field in schema['fields'].items():
                if not set(VOCAB.field.visibility.hidden_fields) & set(field['visibility']):
                    field['visibility'] = []
            
            # Overlay the original lcp fields on the top
            schema['fields'].update(original_fields)
            
            
            logger.debug('new lcp fields: %r',
                [(field['key'],field['scope']) 
                    for field in schema['fields'].values()])
            return schema
        except Exception, e:
            logger.exception('xxx: %r', e)
            raise

    @read_authorization
    def get_lab_cherry_pick_plating_schema(self, request, **kwargs):
        ''' Generate an HttpResponse for the plate_mapping specific schema '''
        
        if not 'cherry_pick_request_id' in kwargs:
            raise BadRequestError(key='cherry_pick_request_id', msg='required')
        
        return self.build_response(
            request, 
            self.build_lab_cherry_pick_plating_schema(request.user, **kwargs),
            **kwargs)

    def build_lab_cherry_pick_plating_schema(self, user, **kwargs):

        # Note: build schema for each request to use the subtype
        cherry_pick_request_id = kwargs.get('cherry_pick_request_id', None)
        if cherry_pick_request_id is None:
            raise BadRequestError(key='cherry_pick_request_id', msg='required')
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=cherry_pick_request_id)
        schema = self.build_schema(
            user=user,
            library_classification=cpr.screen.screen_type, **kwargs)
        
        # modify the schema for the plate mapping view
        
        visible_fields = [
            'cherry_pick_plate_number',
            'destination_well',
            'destination_plate_type',
            'library_short_name',
            'source_copy_name',
            'library_plate',
            'source_well_name',
            'source_copy_well_volume',
            'volume_approved',            
            'source_plate_status',
            'source_copy_usage_type',
            'source_plate_type',
            'location',               
           ]
        fields = schema['fields']
        for key,field in fields.items():
            if key in visible_fields:
                field['visibility'] = ['l']
                # NOTE: don't alter ordinal if possible, 
                # this makes it problematic included fields later
                # field['ordinal'] = visible_fields.index(key)
            else:
                if 'l' in field['visibility']:
                    field['visibility'].remove('l')
        fields['cherry_pick_plate_number']['ordinal'] = -10
        fields['destination_well']['ordinal'] = -9
        fields['source_copy_well_volume']['title'] = \
            'Source CopyWell Volume (after transfer)'
        fields['source_copy_well_volume']['description'] = \
            'Source CopyWell Volume (after transfer of '\
            'cherry pick volume to the destination well)'
            
        logger.debug('plate mapping visible fields: %r', 
            {k:{'key': v['key'], 'scope':v['scope'], 'title':v['title']}
                for k,v in fields.items()  })
        return schema
                            
    @read_authorization
    def get_detail(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        # Remove the default, user specific schema; lab cherry pick schema will
        # be specific to the CPR #
        kwargs['schema'] = None
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        # Remove the default, user specific schema; lab cherry pick schema will
        # be specific to the CPR #
        kwargs['schema'] = None
        return self.build_list_response(request, **kwargs)

    def build_sqlalchemy_columns(self, fields, base_query_tables=None, 
            custom_columns=None):
        sub_columns = self.get_sr_resource().build_sqlalchemy_columns(fields)
        sub_columns.update(self.get_smr_resource().build_sqlalchemy_columns(fields))
#         sub_columns['plate_number'] = (literal_column(
#             "to_char(well.plate_number,'%s')" % PLATE_NUMBER_SQL_FORMAT)
#             .label('plate_number'))
        if custom_columns is not None:
            sub_columns.update(custom_columns)
        return DbApiResource.build_sqlalchemy_columns(self, 
            fields, base_query_tables=base_query_tables, 
            custom_columns=sub_columns)
        
    def build_list_response(self, request, schema=None, **kwargs):
        '''
        Optimized to show the lcp's for one cherry pick request at a time
        
        Options:
        - Default (no options): show only the LCPs that have been created. Do 
            not show the copy assignments.
        - API_PARAM_SHOW_COPY_WELLS:
            show copy wells for the LCPs where: 
            copy type is cherry_pick_source_plates and, 
            plate status available
        - API_PARAM_SHOW_RETIRED_COPY_WELlS:
            show copy wells for the LCPs where:
            copy type is cherry_pick_source_plates (available or retired) or 
            copy type is library_screening_plates (retired)
        - API_PARAM_SHOW_UNFULFILLED: show only LCPs with no copy assigned
        - API_PARAM_SHOW_INSUFFICIENT: 
            for LCPs that already have a copy assigned: show those LCPs where
            the copy well volume is less than the approved request transfer volume 
        - API_PARAM_SHOW_MANUAL: show manually selected LCPs
        
        '''

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False

        is_for_detail = kwargs.pop('is_for_detail', False)
        extra_params = {}
        cherry_pick_request_id = param_hash.pop('cherry_pick_request_id', None)
        if cherry_pick_request_id is None:
            raise BadRequestError(key='cherry_pick_request_id', msg='required')
        extra_params['CPR'] = cherry_pick_request_id
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=cherry_pick_request_id)
        show_copy_wells = parse_val(
            param_hash.get(API_PARAM_SHOW_COPY_WELLS, False),
            API_PARAM_SHOW_COPY_WELLS, 'boolean')
        if show_copy_wells is True:
            # NOTE: add a marker to the file_name extra_params
            extra_params[API_PARAM_SHOW_COPY_WELLS] = None
        show_available_and_retired_copy_wells = parse_val(
            param_hash.get(API_PARAM_SHOW_RETIRED_COPY_WELlS, False),
            API_PARAM_SHOW_RETIRED_COPY_WELlS, 'boolean')
        if show_available_and_retired_copy_wells is True:
            extra_params[API_PARAM_SHOW_RETIRED_COPY_WELlS] = None
        show_unfulfilled = parse_val(
            param_hash.get(API_PARAM_SHOW_UNFULFILLED, False),
            API_PARAM_SHOW_UNFULFILLED, 'boolean')
        if show_unfulfilled is True:
            extra_params[API_PARAM_SHOW_UNFULFILLED] = None
        show_insufficient = parse_val(
            param_hash.get(API_PARAM_SHOW_INSUFFICIENT, False),
            API_PARAM_SHOW_INSUFFICIENT, 'boolean')
        if show_insufficient is True:
            extra_params[API_PARAM_SHOW_INSUFFICIENT] = None
        show_manual = parse_val(
            param_hash.get(API_PARAM_SHOW_MANUAL, False),
            API_PARAM_SHOW_MANUAL, 'boolean')
        if show_manual is True:
            extra_params[API_PARAM_SHOW_MANUAL] = None
        
        try:
            
            # Note: build schema for each request to use the subtype
            schema = kwargs.get('plating_schema', None)
            if schema is None: 
                schema = self.build_schema(
                    user=request.user, 
                    library_classification=cpr.screen.screen_type)
            # general setup
          
            manual_field_includes = set(param_hash.get('includes', []))
            manual_field_includes.add('source_copy_id')
            manual_field_includes.add('source_copy_comments')
            manual_field_includes.add('selected_copy_name')
            manual_field_includes.add('selected')
            manual_field_includes.add('source_copy_usage_type')
            manual_field_includes.add('cherry_pick_request_id')
            
            # FIXME: only add the comment array if selecting alternate copies
            if '-library_plate_comment_array' not in manual_field_includes:
                manual_field_includes.add('library_plate_comment_array')
            manual_field_includes.add('library_comment_array')
            
            if show_insufficient is True:
                manual_field_includes.add('volume_approved')
                manual_field_includes.add('source_copy_well_volume')

            (filter_expression, filter_hash, readable_filter_hash) = \
                SqlAlchemyResource.build_sqlalchemy_filters(
                    schema, param_hash=param_hash)
            filename = self._get_filename(
                readable_filter_hash, schema, **extra_params)
            filter_expression = \
                self._meta.authorization.filter(request.user,filter_expression)
            
            order_params = param_hash.get('order_by', [])
            field_hash = self.get_visible_fields(
                schema['fields'], filter_hash.keys(), manual_field_includes,
                param_hash.get('visibilities'),
                exact_fields=set(param_hash.get('exact_fields', [])),
                order_params=order_params)
            order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
                order_params, field_hash)
             
            rowproxy_generator = None
            if use_vocab is True:
                rowproxy_generator = \
                    DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
                # use "use_vocab" as a proxy to also adjust siunits for viewing
                rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
                    field_hash, rowproxy_generator)
            rowproxy_generator = \
                self._meta.authorization.get_row_property_generator(
                    request.user, field_hash, rowproxy_generator)

            # specific setup 
            base_query_tables = [
                'screen',
                'cherry_pick_request',
                'lab_cherry_pick',
                'well',
                'copy',
                'plate',
                'plate_location',
                'reagent',
                'library',
                'cherry_pick_assay_plate'
                ]
            # build the query statement
            _cpr = self.bridge['cherry_pick_request']
            _scp = self.bridge['screener_cherry_pick']
            _lcp = self.bridge['lab_cherry_pick']
            _p = self.bridge['plate']
            _pl = self.bridge['plate_location']
            _copy = self.bridge['copy']
            _screen = self.bridge['screen']
            _well = self.bridge['well']
            _reagent = self.bridge['reagent']
            _library = self.bridge['library']
            _cpap = self.bridge['cherry_pick_assay_plate']
            _cw = self.bridge['copy_well']
            _apilog = self.bridge['reports_apilog']
            _logdiff = self.bridge['reports_logdiff']
            
            _plate_comment_apilogs = \
                ApiLogResource.get_resource_comment_subquery('librarycopyplate')
            
            _library_comment_apilogs = \
                ApiLogResource.get_resource_comment_subquery('library')
            _library_comment_apilogs = _library_comment_apilogs.cte('library_comment_apilogs')

            j = join(_lcp,_cpr, 
                _lcp.c.cherry_pick_request_id == _cpr.c.cherry_pick_request_id)
            j = j.join(_scp, _lcp.c.screener_cherry_pick_id
                ==_scp.c.screener_cherry_pick_id)
            j = j.join(_screen,
                _cpr.c.screen_id == _screen.c.screen_id)
            j = j.join(_well, _well.c.well_id==_lcp.c.source_well_id)
            j = j.join(_library, _well.c.library_id==_library.c.library_id)
            j = j.join(_reagent, _lcp.c.source_well_id==_reagent.c.well_id)
            j = j.join(_cpap, _lcp.c.cherry_pick_assay_plate_id
                ==_cpap.c.cherry_pick_assay_plate_id, isouter=True)
            
            custom_columns = {
                'status': case([
                    (and_(_lcp.c.copy_id==_copy.c.copy_id,
                          _lcp.c.cherry_pick_assay_plate_id==None,), 
                        text("'%s'"%LCP_STATUS.SELECTED) ),
                    (and_(_lcp.c.copy_id==_copy.c.copy_id,
                          _lcp.c.cherry_pick_assay_plate_id!=None,), 
                        text("'%s'" % LCP_STATUS.PLATED ) )],
                    else_=text("'%s'" % LCP_STATUS.UNFULFILLED)),
                'destination_well': (
                    case([
                        (_lcp.c.assay_plate_row!=None,
                            _concat(
                                func.chr(_lcp.c.assay_plate_row+65),
                                func.trim(func.to_char(
                                    _lcp.c.assay_plate_column+1, '00')) )
                            )],
                        else_=None)),
                'selected': case([
                    (_lcp.c.copy_id==_copy.c.copy_id, text('true') )],
                        else_=text('false')),
                # 'source_copywell_id': (
                #     case([(_lcp.c.copy_id!=None,
                #             _concat(_library.c.short_name,'/',_copy.c.name,'/',
                #                 _lcp.c.source_well_id)
                #         )],
                #         else_=None )),
                'source_copywell_id': (
                    case([(_lcp.c.copy_id!=None,
                            _concat(_copy.c.name, '/', _lcp.c.source_well_id)
                        )],
                        else_=None )),
                'source_copy_well_volume': case([
                    (_well.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                         func.coalesce(_cw.c.volume, 
                             _p.c.remaining_well_volume, _p.c.well_volume) )],
                    else_=None),
                'source_copy_well_initial_volume': case([
                    (_well.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                         func.coalesce(_cw.c.initial_volume,_p.c.well_volume) )],
                     else_=None),
                'source_copy_well_consumed_volume': case([
                    (_well.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                        func.coalesce(_cw.c.initial_volume,_p.c.well_volume)-
                            func.coalesce(_cw.c.volume, _p.c.remaining_well_volume) )],
                    else_=None),
                'library_comment_array': (
                    select([func.array_to_string(
                        func.array_agg(
                            _concat(                            
                                cast(_library_comment_apilogs.c.name,
                                    sqlalchemy.sql.sqltypes.Text),
                                LIST_DELIMITER_SUB_ARRAY,
                                cast(_library_comment_apilogs.c.date_time,
                                    sqlalchemy.sql.sqltypes.Text),
                                LIST_DELIMITER_SUB_ARRAY,
                                _library_comment_apilogs.c.comment)
                        ), 
                        LIST_DELIMITER_SQL_ARRAY) ])
                    .select_from(_library_comment_apilogs)
                    .where(_library_comment_apilogs.c.key==_library.c.short_name)
                ),
            }
            
            # show screener selections (may be different if from pool wells)
            if ( 'screener_well_id' in field_hash
                or 'screener_library_short_name' in field_hash):
                _screener_well = _well.alias('screener_well')
                _screener_library = _library.alias('screener_library')
                j = j.join(_screener_well, 
                    _screener_well.c.well_id==_scp.c.screened_well_id)
                j = j.join(_screener_library, 
                    _screener_well.c.library_id==_screener_library.c.library_id)
                custom_columns['screener_well_id'] = _scp.c.screened_well_id
                custom_columns['screener_library_short_name'] = _screener_library.c.short_name
            
            # pull in the copy well volume
            if ( show_copy_wells is not True 
                 and show_available_and_retired_copy_wells is not True):
                j = j.join(_copy, _lcp.c.copy_id==_copy.c.copy_id, isouter=True)
                j = j.join(_p, and_(
                    _copy.c.copy_id == _p.c.copy_id,
                    _well.c.plate_number == _p.c.plate_number), isouter=True)
                j = j.join(
                    _pl, _p.c.plate_location_id == _pl.c.plate_location_id,
                    isouter=True)
                j = j.join(_cw, and_(
                    _cw.c.well_id == _well.c.well_id,
                    _cw.c.copy_id == _copy.c.copy_id), isouter=True )

                if 'library_plate_comment_array' in field_hash:
                    ### Performance hack: limit the apilogs for the query
                    with get_engine().connect() as conn:
                        temp = (
                            select([distinct(_concat(
                                _library.c.short_name,'/',_copy.c.name,'/', 
                                cast(_p.c.plate_number,sqlalchemy.sql.sqltypes.Text))
                                )])
                            .select_from(j)
                            .where(_lcp.c.cherry_pick_request_id==cherry_pick_request_id))
                        plate_keys = set([x[0] for x in 
                            conn.execute(temp)])
                        _plate_comment_apilogs = _plate_comment_apilogs.\
                            where(_apilog.c.key.in_(plate_keys))
                        _plate_comment_apilogs = \
                            _plate_comment_apilogs.cte('plate_comment_apilogs')

                custom_columns.update({
                    'selected_copy_name': case([
                        (_lcp.c.copy_id==_copy.c.copy_id, _copy.c.name )],
                            else_=None),
                })
            else: # show all / retired copies
                _original_copy = _copy.alias('c1')
                _copyplates_available = (
                    select([
                        _lcp.c.lab_cherry_pick_id,
                        _copy.c.copy_id,
                        _p.c.plate_id,
                        _concat(
                            _library.c.short_name,'/',_copy.c.name,'/', 
                            cast(_p.c.plate_number,sqlalchemy.sql.sqltypes.Text)).label('key')])
                    .select_from(
                        _lcp.join(_well,_well.c.well_id==_lcp.c.source_well_id)
                            .join(_p, _well.c.plate_number==_p.c.plate_number)
                            .join(_copy, _p.c.copy_id==_copy.c.copy_id)
                            .join(_library, _copy.c.library_id==_library.c.library_id))
                    .where(_lcp.c.cherry_pick_request_id==cherry_pick_request_id))
                if show_available_and_retired_copy_wells is not True:
                    _copyplates_available = \
                        _copyplates_available.where(and_(
                            _copy.c.usage_type=='cherry_pick_source_plates',
                            _p.c.status==SCHEMA.VOCAB.plate.status.AVAILABLE))
                else:
                    # NOTE: show retired cherry pick plates: make sure not to
                    # consider these when automatically mapping
                    # _copyplates_available = \
                    #     _copyplates_available.where(and_(
                    #         _copy.c.usage_type=='cherry_pick_source_plates',
                    #         _p.c.status=='available'))
                    _copyplates_available = \
                        _copyplates_available.where(or_(
                            _copy.c.usage_type=='cherry_pick_source_plates',
                            and_(_copy.c.usage_type=='library_screening_plates',
                                _p.c.status==SCHEMA.VOCAB.plate.status.RETIRED)))
                    
                _copyplates_available = _copyplates_available.where(
                    _p.c.status.in_([SCHEMA.VOCAB.plate.status.AVAILABLE,SCHEMA.VOCAB.plate.status.RETIRED]))
                _copyplates_available = _copyplates_available.cte('copy_plates_available')
                
                j = j.join(_copyplates_available,
                    _lcp.c.lab_cherry_pick_id
                    ==_copyplates_available.c.lab_cherry_pick_id)
                j = j.join(_original_copy, _lcp.c.copy_id
                    ==_original_copy.c.copy_id, isouter=True)
                j = j.join(_copy, _copy.c.copy_id
                    ==_copyplates_available.c.copy_id)
                j = j.join(_p, _p.c.plate_id==_copyplates_available.c.plate_id)
                j = j.join(
                    _pl, _p.c.plate_location_id == _pl.c.plate_location_id,
                    isouter=True)
                j = j.join(_cw, and_(
                    _cw.c.well_id == _well.c.well_id,
                    _cw.c.copy_id == _copy.c.copy_id), isouter=True )

                if 'library_plate_comment_array' in field_hash:
                    ### Performance hack: limit the apilogs for the query
                    with get_engine().connect() as conn:
                        temp = (
                            select([distinct(_concat(
                                _library.c.short_name,'/',_copy.c.name,'/', 
                                cast(_p.c.plate_number,sqlalchemy.sql.sqltypes.Text))
                                )])
                            .select_from(j)
                            .where(_lcp.c.cherry_pick_request_id==cherry_pick_request_id))
                        plate_keys = set([x[0] for x in 
                            conn.execute(temp)])
                        _plate_comment_apilogs = _plate_comment_apilogs.\
                            where(_apilog.c.key.in_(plate_keys))
                        _plate_comment_apilogs = \
                            _plate_comment_apilogs.cte('plate_comment_apilogs')
                    
                custom_columns.update({
                    'selected_copy_name': case([
                        (_lcp.c.copy_id==_copy.c.copy_id, _copy.c.name )],
                            else_=_original_copy.c.name),
                    'source_copywell_id': (
                        _concat(_copy.c.name, '/', _lcp.c.source_well_id)
                            ),
                    # 'source_copywell_id': (
                    #     _concat(_library.c.short_name,'/',_copy.c.name,'/',
                    #         _lcp.c.source_well_id)
                    #         ),
                    'status': case([
                        (and_(_lcp.c.copy_id==_copy.c.copy_id,
                              _lcp.c.cherry_pick_assay_plate_id==None,), 
                            text("'%s'" % LCP_STATUS.SELECTED) ),
                        (and_(_lcp.c.copy_id==_copy.c.copy_id,
                              _lcp.c.cherry_pick_assay_plate_id!=None,), 
                            text("'%s'" % LCP_STATUS.PLATED) )],
                        else_=text("'%s'" % LCP_STATUS.NOT_SELECTED)),
                })

            custom_columns['library_plate_comment_array'] = (
                select([func.array_to_string(
                    func.array_agg(
                        _concat(                            
                            cast(_plate_comment_apilogs.c.name,
                                sqlalchemy.sql.sqltypes.Text),
                            LIST_DELIMITER_SUB_ARRAY,
                            cast(_plate_comment_apilogs.c.date_time,
                                sqlalchemy.sql.sqltypes.Text),
                            LIST_DELIMITER_SUB_ARRAY,
                            _plate_comment_apilogs.c.comment)
                    ), 
                    LIST_DELIMITER_SQL_ARRAY) ])
                .select_from(_plate_comment_apilogs)
                .where(_plate_comment_apilogs.c.key== _concat(
                    _library.c.short_name,'/',_copy.c.name,'/', 
                    cast(_p.c.plate_number,sqlalchemy.sql.sqltypes.Text)))
                )
            
            columns = self.build_sqlalchemy_columns(
                field_hash.values(), base_query_tables=base_query_tables,
                custom_columns=custom_columns)

            
            stmt = select(columns.values()).select_from(j)
            stmt = stmt.where(_cpr.c.cherry_pick_request_id==cherry_pick_request_id)

            if show_insufficient is True:
                with get_engine().connect() as conn:
                    _lcps_insufficient = (
                        select([_lcp.c.lab_cherry_pick_id,
                            func.coalesce(_cw.c.volume, 
                                    _p.c.remaining_well_volume, _p.c.well_volume)])
                        .select_from(
                            _lcp.join(_copy, _lcp.c.copy_id==_copy.c.copy_id)
                                .join(_well, _lcp.c.source_well_id==_well.c.well_id)
                                .join(_p, and_(
                                    _p.c.plate_number==_well.c.plate_number,
                                    _p.c.copy_id==_copy.c.copy_id))
                                .join(_cw, and_(
                                    _cw.c.well_id==_well.c.well_id,
                                    _cw.c.copy_id==_copy.c.copy_id),isouter=True)
                                .join(_cpr, _lcp.c.cherry_pick_request_id
                                    ==_cpr.c.cherry_pick_request_id))
                        .where(_cpr.c.cherry_pick_request_id==cherry_pick_request_id)
                        )
                    if cpr.cherry_pick_assay_plates.exists():
                        _lcps_insufficient = _lcps_insufficient.where(_cw.c.volume < 0)
                    else:
                        _lcps_insufficient = _lcps_insufficient.where(
                            _cpr.c.transfer_volume_per_well_approved 
                                > func.coalesce(_cw.c.volume, 
                                    _p.c.remaining_well_volume, _p.c.well_volume))
                    lcp_ids = set([(x[0],x[1]) for x in 
                            conn.execute(_lcps_insufficient)])
                    logger.info('insufficient lcps: %r', lcp_ids)
                    lcp_ids = set([x[0] for x in lcp_ids])
                    stmt = stmt.where(_lcp.c.lab_cherry_pick_id.in_(lcp_ids))

            if show_manual is True:
                stmt = stmt.where(_lcp.c.is_manually_selected==True)
            
            if show_unfulfilled is True:
                if ( show_copy_wells is not True 
                     and show_available_and_retired_copy_wells is not True ):
                    stmt = stmt.where(_copy.c.name==None)
                else:
                    stmt = stmt.where(_original_copy.c.name==None)
            # Ordering for well_id must be alphanumeric
            # For string field ordering, double sort as numeric and text
            order_clauses.append(text(
                "(substring({field_name}, '^[0-9]+'))::int asc " # cast to integer
                ",substring({field_name}, ':(.*$)') asc  "  # works as text
                .format(field_name='source_well_id')))
            if ( show_copy_wells is True 
                 or show_available_and_retired_copy_wells is True):
                order_clauses.extend((
                    desc('selected'),asc('source_copy_usage_type'),
                    asc('source_copy_name'),))
            
            # general setup
             
            (stmt, count_stmt) = self.wrap_statement(
                stmt, order_clauses, filter_expression)
           
            # compiled_stmt = str(stmt.compile(
            #     dialect=postgresql.dialect(),
            #     compile_kwargs={"literal_binds": True}))
            # logger.info('compiled_stmt %s', compiled_stmt)
            
            title_function = None
            if use_titles is True:
                def title_function(key):
                    return field_hash[key]['title']
            if is_data_interchange:
                title_function = DbApiResource.datainterchange_title_function(
                    field_hash,schema['id_attribute'])
            
            return self.stream_response_from_statement(
                request, stmt, count_stmt, filename,
                field_hash=field_hash,
                param_hash=param_hash,
                is_for_detail=is_for_detail,
                rowproxy_generator=rowproxy_generator,
                title_function=title_function, meta=kwargs.get('meta', None))
             
        except Exception, e:
            logger.exception('on get list')
            raise e  


class CherryPickPlateResource(DbApiResource):        

    class Meta:

        queryset = CherryPickAssayPlate.objects.all()
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'cherrypickassayplate'
        authorization = CherryPickRequestAuthorization(resource_name)
        
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        always_return_data = True 

    def __init__(self, **kwargs):

        super(CherryPickPlateResource, self).__init__(**kwargs)
        self.cherry_pick_resource = None
        
    def get_cherry_pick_resource(self):
        if self.cherry_pick_resource is None:
            self.cherry_pick_resource = CherryPickRequestResource()
        return self.cherry_pick_resource
    
    def clear_cache(self, request, **kwargs):
        DbApiResource.clear_cache(self, request, **kwargs)
        self.get_cherry_pick_resource().clear_cache(request, **kwargs)
        
    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),

            url(r"^(?P<resource_name>%s)"
                r"/(?P<cherry_pick_request_id>[\d]+)"
                r"/(?P<plate_ordinal>[\d]+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
        ]

    @read_authorization
    def get_detail(self, request, **kwargs):

        cherry_pick_request_id = kwargs.get('cherry_pick_request_id', None)
        if not cherry_pick_request_id:
            raise MissingParam('cherry_pick_request_id')
        plate_ordinal = kwargs.get('plate_ordinal', None)
        if not plate_ordinal:
            raise MissingParam('plate_ordinal')

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, schema=None, **kwargs):

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        if schema is None:
            raise Exception('schema not initialized')

        is_for_detail = kwargs.pop('is_for_detail', False)
        cherry_pick_request_id = param_hash.pop('cherry_pick_request_id', None)
        if cherry_pick_request_id:
            param_hash['cherry_pick_request_id__eq'] = cherry_pick_request_id
        plate_ordinal = param_hash.pop('plate_ordinal', None)
        if plate_ordinal:
            param_hash['plate_ordinal__eq'] = plate_ordinal

        try:
            
            # general setup
          
            manual_field_includes = set(param_hash.get('includes', []))
  
            (filter_expression, filter_hash, readable_filter_hash) = \
                SqlAlchemyResource.build_sqlalchemy_filters(
                    schema, param_hash=param_hash)
            filename = self._get_filename(
                readable_filter_hash, schema, is_for_detail)                                  
            filter_expression = \
                self._meta.authorization.filter(request.user,filter_expression)
            
            order_params = param_hash.get('order_by', [])
            field_hash = self.get_visible_fields(
                schema['fields'], filter_hash.keys(), manual_field_includes,
                param_hash.get('visibilities'),
                exact_fields=set(param_hash.get('exact_fields', [])),
                order_params=order_params)
            order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
                order_params, field_hash)
             
            rowproxy_generator = None
            if use_vocab is True:
                rowproxy_generator = \
                    DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            rowproxy_generator = \
                self._meta.authorization.get_row_property_generator(
                    request.user, field_hash, rowproxy_generator)
 
            _cpap = self.bridge['cherry_pick_assay_plate']
            _cpr = self.bridge['cherry_pick_request']
            _lcp = self.bridge['lab_cherry_pick']
            _screen = self.bridge['screen']
            _user_cte = ScreensaverUserResource.get_user_cte().cte('users_cpr')
            _plated_by = _user_cte.alias('plated_by')
            _screened_by = _user_cte.alias('screened_by')


            _apilog = self.bridge['reports_apilog']
            _diff = self.bridge['reports_logdiff']


            # specific setup 
            base_query_tables = [
                'cherry_pick_assay_plate',
                'cherry_pick_request',
                'screen'
            ]
        
            custom_columns = {
                'plated_by_username': _plated_by.c.username,
                'plated_by_id': _plated_by.c.screensaver_user_id,
                'plated_by_name': _plated_by.c.name,
                'screened_by_id': _screened_by.c.screensaver_user_id,
                'screened_by_username': _screened_by.c.username,
                'screened_by_name': _screened_by.c.name,
                'lab_cherry_pick_count': (
                    select([func.count(None)])
                    .select_from(_lcp)
                    .where(_lcp.c.cherry_pick_request_id
                        ==_cpr.c.cherry_pick_request_id)
                    .where(_lcp.c.cherry_pick_assay_plate_id
                        ==_cpap.c.cherry_pick_assay_plate_id)),
                'plating_comments':(
                    select([_apilog.c.comment])
                        .select_from(
                            _apilog.join(_diff, _apilog.c.id==_diff.c.log_id))
                        .where(_apilog.c.ref_resource_name=='cherrypickassayplate')
                        .where(_apilog.c.key==
                            _concat(
                                cast(_cpap.c.cherry_pick_request_id,
                                     sqlalchemy.sql.sqltypes.Text),
                                '/',
                                cast(_cpap.c.plate_ordinal,sqlalchemy.sql.sqltypes.Text)))
                        .where(_diff.c.field_key.in_(['plating_date', 'plated_by_name']))
                        .order_by(desc(_apilog.c.date_time))
                        .limit(1)
                    ),
                'screening_comments':(
                    select([_apilog.c.comment])
                        .select_from(
                            _apilog.join(_diff, _apilog.c.id==_diff.c.log_id))
                        .where(_apilog.c.ref_resource_name
                            =='cherrypickassayplate')
                        .where(_apilog.c.key==
                            _concat(
                                cast(_cpap.c.cherry_pick_request_id,
                                    sqlalchemy.sql.sqltypes.Text),
                                '/',
                                cast(_cpap.c.plate_ordinal,
                                    sqlalchemy.sql.sqltypes.Text)))
                        .where(_diff.c.field_key.in_(['screening_date', 'screened_by_name']))
                        .order_by(desc(_apilog.c.date_time))
                        .limit(1)
                    ),
            }
            
            columns = self.build_sqlalchemy_columns(
                field_hash.values(), base_query_tables=base_query_tables,
                custom_columns=custom_columns)

            # build the query statement
            j = join(_cpap, _cpr,
                _cpap.c.cherry_pick_request_id == _cpr.c.cherry_pick_request_id)
            j = j.join(_screen, _screen.c.screen_id==_cpr.c.screen_id)
            j = j.join(_plated_by, _plated_by.c.screensaver_user_id
                ==_cpap.c.plated_by_id, isouter=True)
            j = j.join(_screened_by, _screened_by.c.screensaver_user_id
                ==_cpap.c.screened_by_id, isouter=True)
            stmt = select(columns.values()).select_from(j)

            # general setup
             
            (stmt, count_stmt) = self.wrap_statement(
                stmt, order_clauses, filter_expression)
            
            # compiled_stmt = str(stmt.compile(
            # dialect=postgresql.dialect(),
            # compile_kwargs={"literal_binds": True}))
            # logger.info('compiled_stmt %s', compiled_stmt)
            
#             if not order_clauses:
            stmt = stmt.order_by(
                nullslast(asc(column('cherry_pick_request_id'))),
                nullslast(asc(column('plate_ordinal'))),)
            
            title_function = None
            if use_titles is True:
                def title_function(key):
                    return field_hash[key]['title']
            if is_data_interchange:
                title_function = DbApiResource.datainterchange_title_function(
                    field_hash,schema['id_attribute'])
            
            return self.stream_response_from_statement(
                request, stmt, count_stmt, filename,
                field_hash=field_hash,
                param_hash=param_hash,
                is_for_detail=is_for_detail,
                rowproxy_generator=rowproxy_generator,
                title_function=title_function, meta=kwargs.get('meta', None))
             
        except Exception, e:
            logger.exception('on get list')
            raise e  

    @write_authorization
    @un_cache
    @transaction.atomic
    def patch_list(self, request, **kwargs):
        '''
        Note: For now, the only update fields in the cherry pick plates are for
        updating the plating and screening activities.
        20180925 - TODO: remove plating fields on the cherry_pick_assay_plate; 
            track the values on the activity classes.
        '''
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        if 'cherry_pick_request_id' not in kwargs:
            raise BadRequestError(key='cherry_pick_request_id', msg='required')
        cpr_id = kwargs['cherry_pick_request_id']
        cpr = CherryPickRequest.objects.get(
            cherry_pick_request_id=cpr_id)
        logger.info(
            'patch_list: cpr: %r, screen: %r...', cpr, cpr.screen.facility_id)
         
        deserialized,deserialize_meta = self.deserialize(request)
        if self._meta.collection_name in deserialized:
            deserialized = deserialized[self._meta.collection_name]
        logger.debug('patch cpaps: %r', deserialized)
        
        assay_plates = { cpap.plate_ordinal:cpap 
            for cpap in cpr.cherry_pick_assay_plates.all() }
        
        if not assay_plates:
            raise ValidationError(
                key='Cherry Pick Assay Plates',
                msg='plates have not been created')
        
        parent_log = self.make_log(request)
        parent_log.ref_resource_name = 'cherrypickrequest'
        parent_log.key = str(cpr.cherry_pick_request_id)
        # NOTE: make the log uri more robust, with screen id as well
        parent_log.uri = '/'.join([
            parent_log.ref_resource_name,'screen',cpr.screen.facility_id,
            parent_log.key])        
        parent_log.save()
        
        original_cpr = self.get_cherry_pick_resource()\
            ._get_detail_response_internal(**{
                'cherry_pick_request_id': cpr_id })
        original_cpap_data = self._get_list_response_internal(**{
            'includes': '*',
            'cherry_pick_request_id': cpr_id })
        
        plated_cpaps = []
        screened_cpaps = []
        for update_cpap in deserialized:
            initializer_dict = self.parse(update_cpap, schema=schema)
            logger.info('initializer_dict: %r', initializer_dict)
            plate_ordinal = parse_val(
                update_cpap.get('plate_ordinal', None), 'plate_ordinal', 'integer')
            if plate_ordinal is None:
                raise ValidationError(key='plate_ordinal', msg='required')
            
            cpap = assay_plates.get(plate_ordinal, None)
            if cpap is None:
                raise ValidationError(
                    key='plate_ordinal', 
                    msg='%d not found'%plate_ordinal)
            
            plating_date = initializer_dict.get('plating_date', None)
            screening_date = initializer_dict.get('screening_date', None)
            
            if plating_date is None and screening_date is None:
                msg = 'must submit either "plating_date" or "screening_date"'
                raise ValidationError({
                    'plating_date': msg, 'screening_date': msg })
            
            if plating_date is not None:
                plated_by_username = initializer_dict.get('plated_by_username', None)
                plated_by_id = initializer_dict.get('plated_by_id', None)
                if plated_by_username is None and plated_by_id is None:
                    raise ValidationError({
                        'plated_by_username': 'required',
                        'plated_by_id': 'required',
                    })
                try:
                    if plated_by_id is not None:
                        user = ScreensaverUser.objects.get(screensaver_user_id=plated_by_id)
                    else:
                        user = ScreensaverUser.objects.get(username=plated_by_username)
                    
                    # FIXME: check that user has cherrypickrequest/write permission
                    logger.info('set cpap user: %r', user)
                    cpap.plated_by = user
                    cpap.plating_date = plating_date
                    
                    cpap.save()
                    plated_cpaps.append(cpap)
                except ObjectDoesNotExist:
                    if plated_by_id is not None:
                        raise ValidationError(
                            key='plated_by_id', 
                            msg='User: "%s" not found' % plated_by_id)
                    else:
                        raise ValidationError(
                            key='plated_by_username', 
                            msg='User: "%s" not found' % plated_by_username)

                parent_log.diffs['last_plating_activity_date'] = [
                    original_cpr.get('last_plating_activity_date', None),
                    plating_date] 
                
            if screening_date is not None:
                if cpap.plating_date is None:
                    raise ValidationError(
                        key='screening_date',
                        msg='"plating_date" must be set before "screening_date" can be set')
                screened_by_username = initializer_dict.get('screened_by_username', None)
                screened_by_id = initializer_dict.get('screened_by_id', None)
                if screened_by_username is None and screened_by_id is None:
                    raise ValidationError({
                        'screened_by_username': 'required',
                        'screened_by_id': 'required',
                    })
                try:
                    if screened_by_id is not None:
                        user = ScreensaverUser.objects.get(screensaver_user_id=screened_by_id)
                    else:
                        user = ScreensaverUser.objects.get(username=screened_by_username)
                    
                    if user not in cpr.screen.get_screen_users():
                        raise ValidationError(
                            key='screened_by_username',
                            msg='Not a member of this screen')
                    
                    cpap.screened_by = user
                    cpap.screening_date = screening_date
                    cpap.save()
                    screened_cpaps.append(cpap)
                except ObjectDoesNotExist:
                    if screened_by_id is not None:
                        raise ValidationError(
                            key='screened_by_id', 
                            msg='User: "%s" not found' % screened_by_id)
                    else:
                        raise ValidationError(
                            key='screened_by_username', 
                            msg='User: "%s" not found' % screened_by_username)

                parent_log.diffs['last_screening_activity_date'] = [
                    original_cpr.get('last_screening_activity_date', None),
                    screening_date] 
        if plated_cpaps:
            # TODO: 20180925: activity_refactor:
            # CPLT(activity) mirrors the plating_date and plated_by fields:
            # rework when req's are clear:
            # - consider removing plating_date and plated_by and using the 
            # activity to track information.
            # - address orphaned CPLT issue
            cplt = CherryPickLiquidTransfer.objects.create(
                screen=cpr.screen,
                cherry_pick_request = cpr,
                
                # TODO: deprecate vol (not entered)
                volume_transferred_per_well_from_library_plates
                    =cpr.transfer_volume_per_well_approved,
                
                apilog_uri=parent_log.log_uri,
                # Activity fields
                comments=parent_log.comment,
                date_of_activity=plated_cpaps[0].plating_date,
                performed_by = plated_cpaps[0].plated_by,
                created_by = self.get_request_user(request),
                classification = SCHEMA.VOCAB.activity.classification.SCREENING,
                type = SCHEMA.VOCAB.activity.type.CHERRY_PICK_TRANSFER
            )

            # TODO: 20180925 - remove: associate plates with plating activity:
            # Alternate is to use the apilog_uri to generate a report of child 
            # (plate) logs
            for cpap in plated_cpaps:
                cpap.cherry_pick_liquid_transfer = cplt
                cpap.save()
            
        if screened_cpaps:
            # TODO: deprecate/rework CherryPickScreening: 
            # 20180925: not used at ICCB-L; revisit when req's are clear
            # - if assay_plate tracking is needed, asoc. with cpap's
            # - consider removing screened_by, screening_date; as these mirror
            # current activity fields
            # - address orphaned CherryPickScreening issue
            cp_screening = CherryPickScreening.objects.create(
                screen = cpr.screen,
                cherry_pick_request = cpr,
                
                # TODO: deprecate protocol, type
                assay_protocol = '',
                assay_protocol_type = '',
                # TODO: deprecate vol xfer (not entered)
                volume_transferred_per_well_to_assay_plates
                    =cpr.transfer_volume_per_well_approved,

                apilog_uri=parent_log.log_uri,
                # Activity fields
                comments=parent_log.comment,
                date_of_activity=screened_cpaps[0].screening_date,
                performed_by = screened_cpaps[0].screened_by,
                created_by = self.get_request_user(request),
                classification = SCHEMA.VOCAB.activity.classification.SCREENING,
                type = SCHEMA.VOCAB.activity.type.CHERRY_PICK_SCREENING
                )
            
            # TODO: 20180925 - associate plates with screening activity:
            # Alternate is to use the apilog_uri to generate a report of child 
            # (plate) logs
            # for cpap in screened_cpaps:
            #     cpap.cherry_pick_screening = cp_screening
            #     cpap.save()

        # Log
        self.get_cherry_pick_resource().clear_cache(request)
        new_cpr = self.get_cherry_pick_resource()._get_detail_response_internal(**{
            'cherry_pick_request_id': cpr_id })
        new_cpap_data = self._get_list_response_internal(**{
            'includes': '*',
            'cherry_pick_request_id': cpr_id })
        self.get_cherry_pick_resource().log_patch(
            request, original_cpr, new_cpr, log=parent_log)
        patch_logs = self.log_patches(
            request, original_cpap_data, new_cpap_data, schema=schema, 
            parent_log=parent_log)
        meta = {
            SCHEMA.API_MSG_RESULT: SCHEMA.API_MSG_SUCCESS }
        if patch_logs:
            plating_fields = set(['plating_date', 'plated_by_username', 'plating_comments'])
            screening_fields = set(['screening_date', 'screened_by_username', 'screening_comments'])
            plated_changed_count = len([
                x for x in patch_logs if plating_fields & set(x.diffs.keys()) ])
            if plated_changed_count > 0:
                meta[API_MSG_CPR_PLATES_PLATED] = plated_changed_count
            screened_changed_count = len([
                x for x in patch_logs if screening_fields & set(x.diffs.keys()) ])
            if screened_changed_count > 0:
                meta[API_MSG_CPR_PLATES_SCREENED] = screened_changed_count
            
        logger.info('meta: %r', meta)
        if deserialize_meta:
            meta.update(deserialize_meta)
        parent_log.json_field = json.dumps(meta, cls=LimsJSONEncoder)
        parent_log.save()
        
        return self.build_response(
            request, { API_RESULT_META: meta }, 
            response_class=HttpResponse, **kwargs)
                
class LibraryCopyResource(DbApiResource):

    class Meta:

        queryset = Copy.objects.all().order_by('name')
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'librarycopy'
        authorization = UserGroupAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        always_return_data = True 

    def __init__(self, **kwargs):

        super(LibraryCopyResource, self).__init__(**kwargs)
        self.librarycopyplate_resource = None
        
    def get_plate_resource(self):
        if self.librarycopyplate_resource is None:
            self.librarycopyplate_resource = LibraryCopyPlateResource()
        return self.librarycopyplate_resource

    def clear_cache(self, request, **kwargs):
        logger.info('clear copy caches')
        DbApiResource.clear_cache(self, request, **kwargs)

    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
           url((r"^(?P<resource_name>%s)"
                 r"/(?P<library_short_name>[\w.\-\+: ]+)"
                 r"/(?P<copy_name>[^/]+)%s$")  
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)/%s/(?P<%s>[\d]+)%s$" 
                % (self._meta.resource_name, SCHEMA.URI_PATH_COMPLEX_SEARCH, 
                    SCHEMA.API_PARAM_COMPLEX_SEARCH_ID, TRAILING_SLASH),
                self.wrap_view('search'), name="api_search"),
            url((r"^(?P<resource_name>%s)"
                 r"/(?P<library_short_name>[\w.\-\+: ]+)"
                 r"/(?P<copy_name>[^/]+)"
                 r"/plate%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_librarycopyplateview'),
                name="api_dispatch_librarycopy_plateview"),
        ]    

    def dispatch_librarycopyplateview(self, request, **kwargs):

        return LibraryCopyPlateResource().dispatch('list', request, **kwargs)    
        
    @read_authorization
    def get_detail(self, request, **kwargs):

        library_short_name = kwargs.get('library_short_name', None)
        if not library_short_name:
            raise MissingParam('library_short_name')
        copy_name = kwargs.get('copy_name', None)
        if not copy_name:
            raise MissingParam('copy_name')
        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)
    
    def build_list_response(self, request, schema=None, **kwargs):

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        if schema is None:
            raise Exception('schema not initialized')
        
        is_for_detail = kwargs.pop('is_for_detail', False)
        library_short_name = param_hash.pop('library_short_name',
            param_hash.get('library_short_name__eq', None))
        if not library_short_name:
            logger.info('no library_short_name provided')
        else:
            param_hash['library_short_name__eq'] = library_short_name
        name = param_hash.pop('name', param_hash.get('name', None))
        if name:
            param_hash['name__eq'] = name
            
        try:
            
            # general setup
          
            manual_field_includes = set(param_hash.get('includes', []))
            if is_for_detail:
#                 manual_field_includes.add('has_copywell_concentrations')
                manual_field_includes.add('has_copywell_volumes')

            (filter_expression, filter_hash, readable_filter_hash) = \
                SqlAlchemyResource.build_sqlalchemy_filters(
                    schema, param_hash=param_hash)
            filename = self._get_filename(
                readable_filter_hash, schema, is_for_detail)
            filter_expression = \
                self._meta.authorization.filter(request.user,filter_expression)

            # if filter_expression is None:
            #     raise InformationError(
            #         key='Input filters ',
            #         msg='Please enter a filter expression to begin')

            order_params = param_hash.get('order_by', [])
            field_hash = self.get_visible_fields(
                schema['fields'], filter_hash.keys(), manual_field_includes,
                param_hash.get('visibilities'),
                exact_fields=set(param_hash.get('exact_fields', [])),
                order_params=order_params)
            order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
                order_params, field_hash)
             
            rowproxy_generator = None
            if use_vocab is True:
                rowproxy_generator = \
                    DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
                # use "use_vocab" as a proxy to also adjust siunits for viewing
                rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
                    field_hash, rowproxy_generator)
            rowproxy_generator = \
               self._meta.authorization.get_row_property_generator(
                   request.user, field_hash, rowproxy_generator)

            # specific setup 

            _c = self.bridge['copy']
            _l = self.bridge['library']
            _ap = self.bridge['assay_plate']
            _p = self.bridge['plate']
            _cw = self.bridge['copy_well']
            _ls = self.bridge['library_screening']
            _pl = self.bridge['plate_location']
            _plate_statistics = (
                LibraryCopyPlateResource.get_plate_copywell_statistics_cte(
                    filter_hash=filter_hash)
                    .cte('plate_statistics'))
            _plate_screening_statistics = (
                LibraryCopyPlateResource.get_plate_screening_statistics_cte(
                    filter_hash=filter_hash)
                    .cte('plate_screening_statistics'))
            _copy_statistics = (
                select([
                    _plate_statistics.c.copy_id,
                    case([
                        ( func.avg(_plate_statistics.c.avg_well_remaining_volume) != None,
                           func.avg(_plate_statistics.c.avg_well_remaining_volume))], 
                        else_=func.avg(_plate_statistics.c.remaining_well_volume))
                        .label('avg_plate_volume'),
                    case([
                        ( func.min(_plate_statistics.c.min_well_remaining_volume) != None,
                           func.min(_plate_statistics.c.min_well_remaining_volume))
                        ], else_=func.min(_plate_statistics.c.remaining_well_volume))
                        .label('min_plate_volume'),
                    case([
                        ( func.max(_plate_statistics.c.max_well_remaining_volume) != None,
                           func.max(_plate_statistics.c.max_well_remaining_volume))
                        ],else_=func.max(_plate_statistics.c.remaining_well_volume))
                        .label('max_plate_volume'),
                    func.min(_plate_statistics.c.min_mg_ml_concentration)
                        .label('min_mg_ml_concentration'),
                    func.max(_plate_statistics.c.max_mg_ml_concentration)
                        .label('max_mg_ml_concentration'),
                    func.min(_plate_statistics.c.min_molar_concentration)
                        .label('min_molar_concentration'),
                    func.max(_plate_statistics.c.max_molar_concentration)
                        .label('max_molar_concentration'),
                     ])
                 .select_from(_plate_statistics)
                 .group_by(_plate_statistics.c.copy_id)
             ).cte('copy_statistics')

            _copy_screening_statistics = (
                select([
                    _plate_screening_statistics.c.copy_id,
                    func.max(_plate_screening_statistics.c.last_date_screened)
                        .label('last_date_screened'),
                    func.min(_plate_screening_statistics.c.first_date_screened)
                        .label('first_date_screened')])
                .select_from(_plate_screening_statistics)
                .group_by(_plate_screening_statistics.c.copy_id)
                ).cte('copy_screening_statistics')
            
            _plate_status_counts = (
                select([_p.c.copy_id, _p.c.status, func.count(None).label('count')])
                .select_from(_p)
                .group_by(_p.c.copy_id, _p.c.status)
                .order_by(nullslast(desc(literal_column('count'))))
                ).cte('plate_status_counts')

            _plate_location_counts = (
                select([
                    _p.c.copy_id, 
                    _concat_with_sep(
                        (_pl.c.room,_pl.c.freezer,_pl.c.shelf,_pl.c.bin),'-').label('location'),
                    func.count(None).label('count')])
                .select_from(_pl.join(
                    _p, _pl.c.plate_location_id==_p.c.plate_location_id))
                .group_by(_p.c.copy_id, _pl.c.room,_pl.c.freezer,_pl.c.shelf,_pl.c.bin)
                .order_by(nullslast(desc(literal_column('count'))))
                ).cte('plate_location_counts')
                
            custom_columns = {
                'copy_plate_count': (
                    select([func.count(None)])
                    .select_from(_p).where(_p.c.copy_id==text('copy.copy_id'))),
                    
                'avg_plate_volume': _copy_statistics.c.avg_plate_volume,
                'min_plate_volume': _copy_statistics.c.min_plate_volume,
                'max_plate_volume': _copy_statistics.c.max_plate_volume,
                'avg_plate_screening_count': (
                    select([func.avg(_p.c.screening_count)])
                    .select_from(_p)
                    .where(_p.c.copy_id == literal_column('copy.copy_id'))),
                'avg_plate_cp_screening_count': (
                    select([func.avg(_p.c.cplt_screening_count)])
                    .select_from(_p)
                    .where(_p.c.copy_id == literal_column('copy.copy_id'))),
                'min_mg_ml_concentration': _copy_statistics.c.min_mg_ml_concentration,
                'max_mg_ml_concentration': _copy_statistics.c.max_mg_ml_concentration,
                'min_molar_concentration': _copy_statistics.c.min_molar_concentration,
                'max_molar_concentration': _copy_statistics.c.min_molar_concentration,
                
                'first_date_screened': _copy_screening_statistics.c.first_date_screened,
                'last_date_screened': _copy_screening_statistics.c.last_date_screened,
                'primary_plate_status': (
                    select([_plate_status_counts.c.status])
                    .where(_plate_status_counts.c.copy_id == _c.c.copy_id)
                    .limit(1)),
                'primary_plate_location': (
                    select([_plate_location_counts.c.location])
                    .where(_plate_location_counts.c.copy_id == _c.c.copy_id)
                    .limit(1)),
                'plate_locations': literal_column('\n'.join([
                    '(select count(distinct(plate_location_id)) ',
                    '    from plate p',
                    '    where p.copy_id = copy.copy_id ) '])).\
                    label('plate_locations'),
                'plates_available': literal_column('\n'.join([
                    '(select count(p)',
                    '    from plate p ',
                    '    where p.copy_id=copy.copy_id',
                    "    and p.status in ('available') ) "])).\
                    label('plates_available'),
                'has_copywell_volumes': (
                    exists(select([None])
                        .select_from(_cw)
                        .where(_cw.c.copy_id==_c.c.copy_id)
                        .where(_cw.c.initial_volume!=None)
                        )),
                'has_copywell_concentrations':
                    exists(select([None])
                        .select_from(_cw)
                        .where(_cw.c.copy_id==_c.c.copy_id)
                        .where(or_(
                            _cw.c.mg_ml_concentration!=None,
                            _cw.c.molar_concentration!=None)
                        )),
                }
            
            base_query_tables = ['copy', 'library']
 
            columns = self.build_sqlalchemy_columns(
                field_hash.values(), base_query_tables=base_query_tables,
                custom_columns=custom_columns)

            # build the query statement

            
            j = join(_c, _l, _c.c.library_id == _l.c.library_id)
            if set(['avg_remaining_volume','min_remaining_volume',
                'max_remaining_volume']) | set(field_hash.keys()):
                j = j.outerjoin(
                    _copy_statistics,_c.c.copy_id==_copy_statistics.c.copy_id)     
            j = j.outerjoin(_copy_screening_statistics,
                _c.c.copy_id==_copy_screening_statistics.c.copy_id)
            stmt = select(columns.values()).select_from(j)
            
            # general setup
             
            (stmt, count_stmt) = self.wrap_statement(
                stmt, order_clauses, filter_expression)
            
            if not order_clauses:
                stmt = stmt.order_by("library_short_name", "copy_name")
 
            title_function = None
            if use_titles is True:
                def title_function(key):
                    return field_hash[key]['title']
            if is_data_interchange:
                title_function = DbApiResource.datainterchange_title_function(
                    field_hash,schema['id_attribute'])

            # compiled_stmt = str(stmt.compile(
            #     dialect=postgresql.dialect(),
            #     compile_kwargs={"literal_binds": True}))
            # logger.info('compiled_stmt %s', compiled_stmt)

            return self.stream_response_from_statement(
                request, stmt, count_stmt, filename,
                field_hash=field_hash, param_hash=param_hash,
                is_for_detail=is_for_detail,
                rowproxy_generator=rowproxy_generator,
                title_function=title_function, meta=kwargs.get('meta', None),
                use_caching=True)
            
        except Exception, e:
            logger.exception('on get list')
            raise e   

    def put_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_detail')
                
    @write_authorization
    @un_cache
    @transaction.atomic    
    def delete_obj(self, request, deserialized, **kwargs):
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')

        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
        ScreensaverUser.objects.get(**id_kwargs).delete()
    
    @write_authorization
    @transaction.atomic    
    def patch_obj(self, request, deserialized, **kwargs):

        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        fields = schema['fields']
        initializer_dict = {}

        # TODO: wrapper for parsing
        logger.debug('fields: %r, deserialized: %r', fields.keys(), deserialized)
        for key in fields.keys():
            if deserialized.get(key, None) is not None:
                initializer_dict[key] = parse_val(
                    deserialized.get(key, None), key, fields[key]['data_type']) 

        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
        
        warnings = []
        meta = {}
        try:
            short_name = id_kwargs['library_short_name']
            try:
                library = Library.objects.get(short_name=short_name)
            except ObjectDoesNotExist:
                msg = 'library_short_name not found: %r' % short_name
                logger.info(msg)
                raise Http404(msg)
            
            patch = False
            librarycopy = None
            try:
                librarycopy = Copy.objects.get(
                    name=id_kwargs['copy_name'], library=library)
                patch = True
                errors = self.validate(deserialized, schema=schema, patch=True)
                if errors:
                    raise ValidationError(errors)
            except ObjectDoesNotExist:
                librarycopy = Copy.objects.create(
                    name=id_kwargs['copy_name'], library=library)
                errors = self.validate(deserialized, schema=schema, patch=False)
                if errors:
                    raise ValidationError(errors)
                librarycopy.save()
            initializer_dict = {}
            for key in fields.keys():
                if key in deserialized:
                    initializer_dict[key] = parse_val(
                        deserialized.get(key, None), key,
                        fields[key]['data_type']) 
            if initializer_dict:
                logger.debug('initializer dict: %s', initializer_dict)
                for key, val in initializer_dict.items():
                    if hasattr(librarycopy, key):
                        setattr(librarycopy, key, val)
            else:
                logger.info(
                    'no (basic) library copy fields to update %s', deserialized)
            
            librarycopy.save()
            logger.info('librarycopy saved: %r', librarycopy)

            if patch is False:
            
                logger.info('create librarycopyplates for range: %d-%d, copy: %s',
                    librarycopy.library.start_plate, 
                    librarycopy.library.end_plate, librarycopy.name )
    
                initial_plate_status = deserialized.get(
                    'initial_plate_status', None)
                if initial_plate_status:
                    self.get_plate_resource().validate({
                        'status': initial_plate_status}, schema=schema)
            
                initial_plate_well_volume = deserialized.get(
                    'initial_plate_well_volume', None)
                if initial_plate_well_volume:
                    initial_plate_well_volume = parse_val(
                        initial_plate_well_volume, 
                        'initial_plate_well_volume', 'decimal')
                elif patch is False:
                    raise ValidationError(
                        key='initial_plate_well_volume',
                        msg='required')

                # Validation of concentration settings
                
                initial_plate_mg_ml_concentration = \
                    deserialized.get('initial_plate_mg_ml_concentration', None)
                if initial_plate_mg_ml_concentration:
                    initial_plate_mg_ml_concentration = parse_val(
                        initial_plate_mg_ml_concentration, 
                        'initial_plate_mg_ml_concentration', 'decimal')
                initial_plate_molar_concentration = \
                    deserialized.get('initial_plate_molar_concentration', None)
                if initial_plate_molar_concentration:
                    initial_plate_molar_concentration = parse_val(
                        initial_plate_mg_ml_concentration, 
                        'initial_plate_molar_concentration', 'decimal')
                if ( initial_plate_molar_concentration is not None and
                     initial_plate_mg_ml_concentration is not None ):
                    msg = ('May only set one of '
                        '["initial_plate_mg_ml_concentration",'
                        '"initial_plate_molar_concentration"]')
                    raise ValidationError({
                        'initial_plate_mg_ml_concentration': msg,
                        'initial_plate_molar_concentration': msg })     
                
                library = librarycopy.library
                mg_mls = ( 
                    library.well_set.all()
                        .filter(library_well_type=WELL_TYPE.EXPERIMENTAL)
                        .distinct('mg_ml_concentration')
                        .values_list('mg_ml_concentration', flat=True))
                molars = ( 
                    library.well_set.all()
                        .filter(library_well_type=WELL_TYPE.EXPERIMENTAL)
                        .distinct('molar_concentration')
                        .values_list('molar_concentration', flat=True))
                
                
                if ( initial_plate_molar_concentration is not None and
                     initial_plate_mg_ml_concentration is not None ):
                    msg = 'Can not set both: molar and mg/ml concentration'
                    raise ValidationError({
                        'initial_plate_molar_concentration': msg,
                        'initial_plate_mg_ml_concentration': msg
                        })
                if len(mg_mls)>1 or len(molars)>1 or \
                    (len(mg_mls)==1 and len(molars)==1):
                    msg = ('May not set plate concentration if the library '
                           'definition contains multiple well concentrations: '
                           '(Use the copy well interface to set individual '
                           'copy well concentrations)')
                    if initial_plate_mg_ml_concentration is not None:
                        raise ValidationError(
                            key='initial_plate_mg_ml_concentration',
                            msg=msg)
                    if initial_plate_molar_concentration is not None:
                        raise ValidationError(
                            key='initial_plate_molar_concentration',
                            msg=msg)
                if len(mg_mls)==0 and len(molars) == 0:
                    if initial_plate_mg_ml_concentration is None \
                        and initial_plate_molar_concentration is None:
                        warnings.append(
                            'No concentration specified')
                        
                logger.debug('create plates start: %d, end: %d',
                    library.start_plate, library.end_plate)
                plates_created = 0
                for x in range(library.start_plate, library.end_plate + 1):
                    p = Plate.objects.create(
                        copy=librarycopy, 
                        plate_number=x,
                        screening_count=0,
                        status='not_specified',
                        well_volume=initial_plate_well_volume,
                        remaining_well_volume=initial_plate_well_volume,
                        mg_ml_concentration=initial_plate_mg_ml_concentration,
                        molar_concentration=initial_plate_molar_concentration )
                    
                    if initial_plate_status:
                        p.status = initial_plate_status
                        if p.status == SCHEMA.VOCAB.plate.status.AVAILABLE:
                            p.date_plated = _now()
                            # TODO: set retired if retired status
                    p.save()
                    plates_created += 1
                    logger.debug('saved plate: %r', p.plate_number)
                logger.info('created %d plates', plates_created)
                logger.info('patch_obj done for librarycopy: %r', librarycopy)
                meta[SCHEMA.API_MSG_RESULT] = {
                    'plates created': plates_created,
                }
                if warnings:
                    meta[SCHEMA.API_MSG_WARNING] = warnings
            
            return { API_RESULT_META: meta, API_RESULT_OBJ: librarycopy }
            
        except Exception, e:
            logger.exception('on patch detail')
            raise e  

class PublicationAuthorization(ScreenAuthorization):
    # If the user can see screens, they can then see publications
    
    def filter(self, user, filter_expression):
        return UserGroupAuthorization.filter(self, user, filter_expression)
    def get_row_property_generator(self, user, fields, extant_generator):
        return extant_generator
    
    def has_publication_read_authorization(self, user, publication_id):
        return True
    
    # def filter(self, user, filter_expression):
    #     
    #     if self.is_restricted_view(user):
    #     
    #         screen_access_dict = self.get_screen_access_level_table(user.username)
    # 
    #         allowed_screens = { facility_id:_dict for facility_id,_dict
    #             in screen_access_dict if _dict['user_access_level_granted'] in [2,3] }    
    # 
    #         auth_filter = column('screen_facility_id').in_(allowed_screens.keys())
    #         if filter_expression is not None:
    #             filter_expression = and_(filter_expression, auth_filter)
    #         else:
    #             filter_expression = auth_filter
    #         
    #     return filter_expression
    
class PublicationResource(DbApiResource):

    class Meta:

        queryset = Publication.objects.all()
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'publication'
        authorization = PublicationAuthorization(resource_name)
        serializer = LimsSerializer()
        always_return_data = True

    def __init__(self, **kwargs):
        
        self.attached_file_resource = None
        self.screen_resource = None
        super(PublicationResource, self).__init__(**kwargs)
        
    def get_attached_file_resource(self):
        if not self.attached_file_resource:
            self.attached_file_resource = AttachedFileResource()
        return self.attached_file_resource

    def get_screen_resource(self):
        if not self.screen_resource:
            self.screen_resource = ScreenResource()
        return self.screen_resource

    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url((r"^(?P<resource_name>%s)/" 
                 r"(?P<publication_id>([\d]+))%s$")
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
        ] 
        
    @read_authorization
    def get_detail(self, request, **kwargs):

        publication_id = kwargs.get('publication_id', None)
        if not publication_id:
            raise MissingParam('publication_id')
        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, **kwargs):
        
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        
        logger.info('params: %r', param_hash.keys())
        
        is_for_detail = kwargs.pop('is_for_detail', False)
        screen_facility_id = param_hash.pop('screen_facility_id', None)
        if screen_facility_id:
            param_hash['screen_facility_id__eq'] = screen_facility_id
        publication_id = param_hash.pop('publication_id', None)
        if publication_id:
            param_hash['publication_id__eq'] = publication_id
        
        # general setup
      
        manual_field_includes = set(param_hash.get('includes', []))
        
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filename = self._get_filename(
            readable_filter_hash, schema, is_for_detail)

        filter_expression = \
            self._meta.authorization.filter(request.user,filter_expression)

              
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
             
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
        rowproxy_generator = \
           self._meta.authorization.get_row_property_generator(
               request.user, field_hash, rowproxy_generator)
 
        # specific setup
        _publication = self.bridge['publication']
        _af = self.bridge['attached_file']
        _screen = self.bridge['screen']
        
        j = _publication
        j = j.join(
            _af, _publication.c.publication_id==_af.c.publication_id, 
            isouter=True)
        j = j.join(
            _screen, _publication.c.screen_id==_screen.c.screen_id,
            isouter=True)
            
        custom_columns = {
            'lookup_pmid': literal_column("'lookup_pmid'"),
            }

        base_query_tables = ['publication', 'attached_file', 'screen' ] 
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)
            
        stmt = select(columns.values()).select_from(j)
        # general setup
         
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        if not order_clauses and filter_expression is None:
            _alias = Alias(stmt)
            stmt = select([text('*')]).select_from(_alias)
        stmt = stmt.order_by('-publication_id')
            
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
        
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None),
            use_caching=True)
             
    def put_list(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_list')
    
    def put_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_detail')
    
    def patch_list(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'patch_list')
    
    def patch_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'patch_detail')
    
    @write_authorization
    @un_cache        
    @transaction.atomic
    def post_detail(self, request, **kwargs):
        
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
                
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        fields = schema['fields']

        initializer_dict = {}
        for key in fields.keys():
            if key in param_hash:
                initializer_dict[key] = parse_val(
                    param_hash.get(key, None), key, fields[key]['data_type']) 

        parent_fields = set(['screen_facility_id', 'reagent_id'])
        initializer_keys = set(initializer_dict.keys())
        if ( parent_fields.isdisjoint(initializer_keys) ):
            msg='must provide one of: %r' % parent_fields
            raise ValidationError({
                'screen_facility_id': msg,
                'reagent_id': msg 
            })
        if (len(parent_fields & initializer_keys)>1):
            logger.warn('too many parent_fields: %r', 
                (parent_fields & initializer_keys))
            msg='Only one of parent fields allowed: %r' % parent_fields
            raise ValidationError({
                'screen_facility_id': msg,
                'reagent_id': msg
            })
        
        screen_facility_id = initializer_dict.pop('screen_facility_id', None)
        if screen_facility_id:
            try:
                screen = Screen.objects.get(
                    facility_id=screen_facility_id)
                initializer_dict['screen'] = screen
            except ObjectDoesNotExist:
                raise Http404('screen_facility_id %r does not exist' 
                    % screen_facility_id)
        reagent_id = initializer_dict.pop('reagent_id', None)
        if reagent_id: 
            try: 
                reagent = Reagent.objects.get(
                    reagent_id=reagent_id)
                initializer_dict['reagent'] = reagent
            except ObjectDoesNotExist:
                raise Http404('reagent_id does not exist: %s' % reagent_id)
        
        if ( 'pubmed_id' not in initializer_dict 
                and 'pubmed_central_id' not in initializer_dict):
            msg = 'must specify either "Pubmed ID" or "Pubmed CID"'
            raise ValidationError({
                'pubmed_id': msg,
                'pubmed_central_id': msg })

        publication = Publication.objects.create()
        for key, val in initializer_dict.items():
            if hasattr(publication, key):
                setattr(publication, key, val)
            else:
                logger.warn(
                    'no such attribute on publication: %s:%r' % (key, val))

        parent_log = kwargs.get('parent_log',None)
        if parent_log is None and publication.screen is not None:
            parent_log = self.log_to_screen(
                publication.screen, publication, request)
            kwargs['parent_log'] = parent_log
            
        publication.save()

        attached_file = request.FILES.get('attached_file', None)
        if attached_file:
            logger.info('create attached file for publication: %r', publication)
            
            # af_request = RequestFactory().generic('POST', '.', HTTP_ACCEPT=JSON_MIMETYPE )
            af_request = HttpRequest()
            af_request.META['HTTP_ACCEPT'] = JSON_MIMETYPE
            af_request.user = request.user
            self.get_attached_file_resource().post_detail(
                af_request, **{ 
                    'attached_file': attached_file,
                    'publication_id': publication.publication_id,
                    'type': 'publication',
                    'parent_log': parent_log })
            
        kwargs_for_log = { 'parent_log': parent_log }
        for id_field in schema['id_attribute']:
            val = getattr(publication, id_field,None)
            if val:
                kwargs_for_log['%s' % id_field] = val
        logger.info('get new data: %r', kwargs_for_log)
        new_data = self._get_detail_response_internal(**kwargs_for_log)
        log = self.log_patch(
            request, None, new_data, full_create_log=True, **kwargs_for_log)
        if log:
            log.save()
        logger.info('created log: %r, %r', log, log.diffs)
        return self.build_response(request, new_data, status_code=201)

    def log_to_screen(self, screen, publication, request, is_delete=False):
        screen_resource = self.get_screen_resource()
        _screen_data = \
            screen_resource._get_detail_response_internal(**{
                'limit': 10,
                'facility_id': screen.facility_id,
                'exact_fields': ['publications'],
                })
        parent_log = screen_resource.make_log(request)
        parent_log.api_action = API_ACTION.PATCH
        parent_log.key = screen.facility_id
        parent_log.uri = '/'.join([parent_log.ref_resource_name,parent_log.key])
        current_pubs = _screen_data['publications'] or []
        if is_delete:
            new_pubs = [x for x in current_pubs 
                if x != str(publication.title)]
        else:
            new_pubs = list(current_pubs)
            new_pubs.append(publication.title)
        parent_log.diffs = { 'publications': [current_pubs,new_pubs]}
        parent_log.save()
        logger.info('created parent log: %r', parent_log)
        return parent_log
    
    @write_authorization
    @un_cache   
    @transaction.atomic     
    def delete_detail(self, request, **kwargs):

        logger.info('delete publication...')
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
    
        publication_id = kwargs.get('publication_id', None)
        if not publication_id:
            raise MissingParam('publication_id')
        
        publication = Publication.objects.get(publication_id=publication_id)

        parent_log = kwargs.get('parent_log',None)
        if parent_log is None and publication.screen is not None:
            parent_log = self.log_to_screen(
                publication.screen, publication, request, is_delete=True)

        kwargs_for_log = {}
        for id_field in schema['id_attribute']:
            if kwargs.get(id_field,None):
                kwargs_for_log[id_field] = kwargs[id_field]
        logger.debug('delete detail: %s' %(kwargs_for_log))
        if not kwargs_for_log:
            raise Exception('required id keys %s' % schema['id_attribute'])
        else:
            try:
                original_data = self._get_detail_response_internal(**kwargs_for_log)
            except Exception as e:
                logger.exception('original state not obtained')
                raise
#                 original_data = {}

        publication.delete()

        logger.info('deleted: %s' %kwargs_for_log)
        log_comment = None
        if HEADER_APILOG_COMMENT in request.META:
            log_comment = request.META[HEADER_APILOG_COMMENT]
        
        log = self.make_log(request)
        self.make_log_key(log, original_data, schema=schema)
        log.parent_log = parent_log
        log.api_action = API_ACTION.DELETE
        log.diffs = { k:[v,None] for k,v in original_data.items()}
        log.save()
        logger.info('delete, api log: %r', log)

        return HttpResponse(status=204)

class AttachedFileAuthorization(ScreenAuthorization):
    '''
    Extend ScreenAuthorization for convenient access to sharing levels
    '''
    
    def filter(self, user, filter_expression):
        if self.is_restricted_view(user) is False:
            return filter_expression

        screensaver_user = ScreensaverUser.objects.get(username=user.username)
        my_screens = self.get_user_screens(screensaver_user)
        
        auth_filter = or_(
            column('screen_facility_id').in_(
                [screen.facility_id for screen in my_screens]),
            column('username') == user.username)
            
        if filter_expression is not None:
            filter_expression = and_(filter_expression, auth_filter)
        else:
            filter_expression = auth_filter

        return filter_expression
        
    def get_row_property_generator(self, user, fields, extant_generator):
        return extant_generator
    
    def has_file_read_authorization(self, user, attached_file_id):
        logger.info('has_file_read_authorization: %r, %r', user, attached_file_id)
        is_restricted = self.is_restricted_view(user)
        if is_restricted is not True:
            return True
        screensaver_user = ScreensaverUser.objects.get(username=user.username)
        attached_file = AttachedFile.objects.get(attached_file_id=attached_file_id)
        
        if attached_file.screensaver_user == screensaver_user:
            return True
        elif attached_file.screen:
            authorized_screens = self.get_read_authorized_screens(screensaver_user)
            return attached_file.screen in authorized_screens
        if attached_file.screensaver_user is None and attached_file.screen is None:
            return True
        logger.info('return False')
        return False
        
class AttachedFileResource(DbApiResource):

    class Meta:

        queryset = AttachedFile.objects.all()
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'attachedfile'
        authorization = AttachedFileAuthorization(resource_name)
        serializer = LimsSerializer()
        always_return_data = True

    def __init__(self, **kwargs):
        
        super(AttachedFileResource, self).__init__(**kwargs)
        self.screen_resource = None
        self.user_resource = None
        
    def get_screen_resource(self):
        if self.screen_resource is None:
            self.screen_resource = ScreenResource()
        return self.screen_resource

    def get_user_resource(self):
        if self.user_resource is None:
            self.user_resource = ScreensaverUserResource()
        return self.user_resource
    
    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url((r"^(?P<resource_name>%s)/" 
                 r"(?P<attached_file_id>([\d]+))%s$")
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)/user/(?P<screensaver_user_id>([\d]+))%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_list'), name="api_dispatch_list"),
            url(r"^(?P<resource_name>%s)/user/(?P<username>([\w]+))%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_list'), name="api_dispatch_list"),
            url((r"^(?P<resource_name>%s)/user/(?P<screensaver_user_id>([\d]+))" 
                 r"/(?P<attached_file_id>([\d]+))%s$")
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url((r"^(?P<resource_name>%s)/user/(?P<username>([\w]+))" 
                 r"/(?P<attached_file_id>([\d]+))%s$")
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
        ] 
        
    def put_list(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_list')
    
    def put_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_detail')
    
    def patch_list(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'patch_list')
    
    def patch_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'patch_detail')
    
    def log_to_screen(self, screen, af, request, is_delete=False):
        '''
        Creates a parent log for attached_files for screens
        '''
        screen_resource = self.get_screen_resource()
        _screen_data = \
            screen_resource._get_detail_response_internal(**{
                'limit': 10,
                'facility_id': screen.facility_id,
                'exact_fields': ['attached_files'],
                })
        parent_log = screen_resource.make_log(request)
        parent_log.api_action = API_ACTION.PATCH
        parent_log.key = str(screen.facility_id)
        parent_log.uri = '/'.join([parent_log.ref_resource_name,parent_log.key])
        current_files = _screen_data['attached_files'] or []
        if is_delete:
            new_files = [x for x in current_files 
                if x != str(af.filename)]
        else:
            new_files = list(current_files)
            new_files.append(af.filename)
        parent_log.diffs = { 'attached_files': [current_files,new_files]}
        parent_log.save()
        return parent_log

    def log_to_user(self, user, af, request, is_delete=False):
        '''
        Creates a parent log for attached_files for screens
        '''
        user_resource = self.get_user_resource()
        parent_log = user_resource.make_log(request)
        parent_log.api_action = API_ACTION.PATCH
        parent_log.key = str(user.screensaver_user_id)
        parent_log.uri = '/'.join([parent_log.ref_resource_name,parent_log.key])
        if is_delete:
            parent_log.diffs =  { 'attached_fiels': [af.filename,None] }
        else:
            parent_log.diffs =  { 'attached_fiels': [None, af.filename] }
        parent_log.save()
        return parent_log

    @write_authorization
    @un_cache        
    @transaction.atomic
    def post_detail(self, request, **kwargs):
        ''' 
        Custom post_detail: 
            - attached file is not editable, so no logging of former state
            - custom deserialization of the attached file from form parameters
        '''
        logger.info('post attached file')
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        fields = schema['fields']
#         id_attribute = schema['id_attribute']
        initializer_dict = {}
        for key in fields.keys():
            if param_hash.get(key, None) is not None:
                initializer_dict[key] = parse_val(
                    param_hash.get(key, None), key, fields[key]['data_type']) 

        attached_file = request.FILES.get('attached_file', None)
        if not attached_file:
            attached_file = param_hash.get('attached_file', None)
        if not attached_file:
            # FIXME: "contents" has been eliminated
            contents = param_hash.get('contents', None)
            filename = param_hash.get('filename', None)
            if not (contents and filename):
                raise ValidationError(
                    key = 'attached_file',
                    msg = ('must provide either "attached_file" '
                           'or "contents+filename" parameters'))
            contents = contents.encode('utf-8')
            initializer_dict['contents'] = contents
            initializer_dict['filename'] = filename
        else:
            contents = attached_file.read()
            filename = attached_file.name
            if param_hash.get('filename', None):
                filename = param_hash.get('filename', None)
            initializer_dict['contents'] = contents
            initializer_dict['filename'] = filename

        if not 'created_by_username' in initializer_dict:
            initializer_dict['created_by_username'] = request.user.username
        try:
            admin_user = ScreensaverUser.objects.get(
                username=initializer_dict['created_by_username'])
            logger.debug('using admin_user %s' % admin_user)
            initializer_dict['created_by'] = admin_user
        except ObjectDoesNotExist:
            logger.exception('created_by_username does not exist: %s',
                initializer_dict['created_by_username'])
            raise ValidationError(
                key='created_by_username',
                msg='user: %r does not exist' % initializer_dict['created_by_username'])
        
        user = None
        if 'username' in initializer_dict:
            try:
                user = ScreensaverUser.objects.get(
                    username=initializer_dict['username'])
                initializer_dict['screensaver_user'] = user
            except ObjectDoesNotExist:
                raise Http404('username %r does not exist' 
                    % initializer_dict['username'])
        if 'screensaver_user_id' in initializer_dict:
            try:
                user1 = ScreensaverUser.objects.get(
                    screensaver_user_id=initializer_dict['screensaver_user_id'])
                if user is not None:
                    if user.screensaver_user_id != user1.screensaver_user_id:
                        raise ValidationError({
                            'username': 'does not match the specified screensaver_user_id',
                            'screensaver_user_id': 'does not match the specified username'
                        })
                user = user1
                initializer_dict['screensaver_user'] = user
            except ObjectDoesNotExist:
                raise Http404('screensaver_user_id %r does not exist' 
                    % initializer_dict['screensaver_user_id'])
            
        parent_fields = set(['screensaver_user', 'publication_id', 
            'screen_facility_id', 'reagent_id'])
        initializer_keys = set(initializer_dict.keys())
        if ( parent_fields.isdisjoint(initializer_keys) ):
            msg='must provide one of: %r' % parent_fields
            raise ValidationError({
                'username': msg, 'screen_facility_id': msg,
                'publication_id': msg, 'reagent_id': msg 
            })
        if (len(parent_fields & initializer_keys)>1):
            logger.warn('too many parent_fields: %r', 
                (parent_fields & initializer_keys))
            msg='Only one of parent fields allowed: %r' % parent_fields
            raise ValidationError({
                'username': msg, 'screensaver_user_id': msg, 'screen_facility_id': msg,
                'publication_id': msg, 'reagent_id': msg
            })
        elif 'screen_facility_id' in initializer_dict:
            try:
                screen = Screen.objects.get(
                    facility_id=initializer_dict['screen_facility_id'])
                initializer_dict['screen'] = screen
            except ObjectDoesNotExist:
                raise Http404('screen_facility_id %r does not exist' 
                    % initializer_dict['screen_facility_id'])
        elif 'publication_id' in initializer_dict: 
            try: 
                publication = Publication.objects.get(
                    publication_id=initializer_dict['publication_id'])
                initializer_dict['publication'] = publication
            except ObjectDoesNotExist:
                raise Http404('publication_id does not exist: %s' 
                    % initializer_dict['publication_id'])
        elif 'reagent_id' in initializer_dict: 
            try: 
                reagent = Reagent.objects.get(
                    reagent_id=initializer_dict['reagent_id'])
                initializer_dict['reagent'] = reagent
            except ObjectDoesNotExist:
                raise Http404('reagent_id does not exist: %s' 
                    % initializer_dict['reagent_id'])
        
        af = AttachedFile()
        for key, val in initializer_dict.items():
            if hasattr(af, key):
                setattr(af, key, val)
            else:
                logger.debug('no such attribute on attached_file: %s:%r' 
                    % (key, val))

        parent_log = kwargs.get('parent_log', None)
        if parent_log is None and af.screen is not None:
            parent_log = \
                self.log_to_screen(af.screen, af, request, is_delete=False)
        if parent_log is None and af.screensaver_user is not None:
            parent_log = \
                self.log_to_user(af.screensaver_user, af, request, is_delete=False)
        # TODO: log_to_reagent
        
        af.save()

        # get new state, for logging
        kwargs_for_log = { 
            'attached_file_id': af.attached_file_id, 
            'parent_log': parent_log 
        }
        new_data = self._get_detail_response_internal(**kwargs_for_log)
        log = self.log_patch(
            request, None,new_data, 
            full_create_log=True,**kwargs_for_log)
        if log:
            log.save()

        if not self._meta.always_return_data:
            return HttpResponse(status=200)
        else:
            return self.build_response(request, new_data, status_code=201)
        
    @write_authorization
    @un_cache        
    @transaction.atomic
    def delete_detail(self, request, **kwargs):
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        attached_file_id = kwargs.pop('attached_file_id', None)
        if not attached_file_id:
            raise MissingParam('attached_file_id')
        
        af = AttachedFile.objects.get(attached_file_id=attached_file_id)
        
        try:
            ua_query = UserAgreement.objects.all().filter(file=af)
            if ua_query.exists():
                raise ValidationError(
                    key='%s' % af.filename,
                    msg='File is attached to the current (%s) User Agreement' % ua_query[0].type)
        except ObjectDoesNotExist:
            logger.info('ok to delete attached file, no associated user agreement')
        
        
        _dict = model_to_dict(af)

        # Create parent log
        parent_log = kwargs.get('parent_log', None)
        if parent_log is None and af.screen is not None:
            parent_log = \
                self.log_to_screen(af.screen, af, request, is_delete=True)
        if parent_log is None and af.screensaver_user is not None:
            parent_log = \
                self.log_to_user(af.screensaver_user, af, request, is_delete=True)

        af.delete()
        
        log = self.make_log(request, **kwargs)
        self.make_log_key(log, _dict, schema=schema)
        log.parent_log = parent_log
        log.api_action = API_ACTION.DELETE
        log.diffs = { k: [v,None] for k,v in _dict.items() }
        log.save()
        logger.debug('delete, api log: %r', log)

        return HttpResponse(status=204)

    @read_authorization
    def get_detail(self, request, **kwargs):

        attached_file_id = kwargs.get('attached_file_id', None)
        if not attached_file_id:
            raise MissingParam('attached_file_id')
        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, **kwargs):
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        
        is_for_detail = kwargs.pop('is_for_detail', False)
        username = param_hash.pop('username', None)
        if username:
            param_hash['username__eq'] = username
        screensaver_user_id = param_hash.pop('screensaver_user_id', None)
        if screensaver_user_id:
            param_hash['screensaver_user_id__eq'] = screensaver_user_id
        screen_facility_id = param_hash.pop('screen_facility_id', None)
        if screen_facility_id:
            param_hash['screen_facility_id__eq'] = screen_facility_id
        attached_file_id = param_hash.pop('attached_file_id', None)
        if attached_file_id:
            param_hash['attached_file_id__eq'] = attached_file_id
        
        try:
            
            # general setup
          
            manual_field_includes = set(param_hash.get('includes', []))
            
            # add fields for authorization filters
            manual_field_includes.add('screen_facility_id')
            manual_field_includes.add('username')
            
            (filter_expression, filter_hash, readable_filter_hash) = \
                SqlAlchemyResource.build_sqlalchemy_filters(
                    schema, param_hash=param_hash)
            filename = self._get_filename(
                readable_filter_hash, schema, is_for_detail)
            filter_expression = \
                self._meta.authorization.filter(request.user,filter_expression)

                  
            order_params = param_hash.get('order_by', [])
            field_hash = self.get_visible_fields(
                schema['fields'], filter_hash.keys(), manual_field_includes,
                param_hash.get('visibilities'),
                exact_fields=set(param_hash.get('exact_fields', [])),
                order_params=order_params)
            order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
                order_params, field_hash)
             
            rowproxy_generator = None
            if use_vocab is True:
                rowproxy_generator = \
                    DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            rowproxy_generator = \
               self._meta.authorization.get_row_property_generator(
                   request.user, field_hash, rowproxy_generator)
 
            # specific setup
            _af = self.bridge['attached_file']
            _su = self.bridge['screensaver_user']
            _screen = self.bridge['screen']
            _publication = self.bridge['publication']
            _reagent = self.bridge['reagent']
            _up = self.bridge['reports_userprofile']
            _comment_apilogs = ApiLogResource.get_resource_comment_subquery(
                self._meta.resource_name, without_log_diffs=False)
            _comment_apilogs = _comment_apilogs.cte('_comment_apilogs')
            lab_head_table = ScreensaverUserResource.get_lab_head_cte().cte('lab_heads')
            
            j = _af
            j = j.join(
                _su, _af.c.screensaver_user_id == _su.c.screensaver_user_id,
                isouter=True)
            j = j.join(
                lab_head_table, _su.c.lab_head_id==lab_head_table.c.screensaver_user_id,
                isouter=True)
            j = j.join(
                _screen, _af.c.screen_id == _screen.c.screen_id,
                isouter=True)
            j = j.join(
                _publication, _af.c.publication_id == _publication.c.publication_id,
                isouter=True)
            j = j.join(
                _reagent, _af.c.reagent_id == _reagent.c.reagent_id,
                isouter=True)
            
            # This entire query doesn't fit the pattern, construct it manually
            # bleah
            custom_columns = {
                'user_fullname': literal_column(
                    "screensaver_user.last_name || ', ' || screensaver_user.first_name"),
                'created_by_username': literal_column(
                    '(Select au.username '
                    ' from screensaver_user au '
                    ' where au.screensaver_user_id=attached_file.created_by_id )'),
                'comment_array': (
                    select([func.array_to_string(
                        func.array_agg(
                            _concat(                            
                                cast(_comment_apilogs.c.name,
                                    sqlalchemy.sql.sqltypes.Text),
                                LIST_DELIMITER_SUB_ARRAY,
                                cast(_comment_apilogs.c.date_time,
                                    sqlalchemy.sql.sqltypes.Text),
                                LIST_DELIMITER_SUB_ARRAY,
                                _comment_apilogs.c.comment)
                        ), 
                        LIST_DELIMITER_SQL_ARRAY) ])
                    .select_from(_comment_apilogs)
                    .where(
                        _comment_apilogs.c.key == 
                        cast(_af.c.attached_file_id,sqlalchemy.sql.sqltypes.TEXT))),
                'lab_name': lab_head_table.c.lab_name_full,
                'lab_head_id': lab_head_table.c.screensaver_user_id,
                'lab_head_username': lab_head_table.c.username,
                'lab_affiliation_name': lab_head_table.c.lab_affiliation_name,
                'lab_affiliation_category': lab_head_table.c.lab_affiliation_category,
                'lab_affiliation_id': lab_head_table.c.lab_affiliation_id,
               }

            base_query_tables = [
                'attached_file', 'screensaver_user', 'screen','publication','reagent'] 
            columns = self.build_sqlalchemy_columns(
                field_hash.values(), base_query_tables=base_query_tables,
                custom_columns=custom_columns)
            
            stmt = select(columns.values()).select_from(j)
            if username:
                stmt = stmt.where(_su.c.username == username)
            if screensaver_user_id:
                stmt = stmt.where(_su.c.screensaver_user_id == screensaver_user_id)
                
            # general setup
             
            (stmt, count_stmt) = self.wrap_statement(
                stmt, order_clauses, filter_expression)
            stmt = stmt.order_by('-attached_file_id')
            
            # compiled_stmt = str(stmt.compile(
            #     dialect=postgresql.dialect(),
            #     compile_kwargs={"literal_binds": True}))
            # logger.info('compiled_stmt %s', compiled_stmt)
            
            title_function = None
            if use_titles is True:
                def title_function(key):
                    return field_hash[key]['title']
            if is_data_interchange:
                title_function = DbApiResource.datainterchange_title_function(
                    field_hash,schema['id_attribute'])
            
            return self.stream_response_from_statement(
                request, stmt, count_stmt, filename,
                field_hash=field_hash,
                param_hash=param_hash,
                is_for_detail=is_for_detail,
                rowproxy_generator=rowproxy_generator,
                title_function=title_function, meta=kwargs.get('meta', None),
                use_caching=True)
             
        except Exception, e:
            logger.exception('on get_list %s' % self._meta.resource_name)
            raise e  


class ActivityResourceAuthorization(ScreenAuthorization):        

    def _is_resource_authorized(
        self, user, permission_type, **kwargs):
        authorized = \
            super(ActivityResourceAuthorization, self)\
                ._is_resource_authorized(user, permission_type, **kwargs)
        if authorized is True:
            return True
        
        return user.is_active
    
    def filter_in_sql(self, user, query, screen_table=None, serviced_user_table=None):
        return query
        # # NOTE: replaces filter for performance
        # if self.is_restricted_view(user) is False:
        #     return query
        # 
        # screensaver_user = ScreensaverUser.objects.get(username=user.username)
        # my_screens = self.get_user_screens(screensaver_user)
        # # Can only see own activities
        # 
        # if serviced_user_table is not None and screen_table is not None:
        #     query = query.where(or_(
        #         screen_table.c.screen_id.in_(
        #             [screen.screen_id for screen in my_screens]),
        #         serviced_user_table.c.username == user.username ))
        # elif serviced_user_table is not None:
        #     query = query.where(
        #         serviced_user_table.c.username == user.username )
        # elif screen_table is not None:
        #     query = query.where(
        #         screen_table.c.screen_id.in_(
        #             [screen.screen_id for screen in my_screens]))
        # else:
        #     raise ProgrammingError('must specify screen_table or serviced_user_table')
        # return query
    
    def filter(self, user, filter_expression):
        # TODO testing: use filter_in_sql for performance
        if self.is_restricted_view(user) is False:
            return filter_expression
         
        screensaver_user = ScreensaverUser.objects.get(username=user.username)
        my_screens = self.get_user_screens(screensaver_user)
        # Can only see own activities
         
        auth_filter = or_(
            column('screen_facility_id').in_(
                [screen.facility_id for screen in my_screens]),
            column('serviced_user_id') == screensaver_user.screensaver_user_id )
             
        if filter_expression is not None:
            filter_expression = and_(filter_expression, auth_filter)
        else:
            filter_expression = auth_filter
         
        return filter_expression
    
    def get_row_property_generator(self, user, fields, extant_generator):
        # If the user may see the Activity, there are no property restrictions
        return extant_generator
    
    def has_activity_read_authorization(self, user, activity_id):

        if self.is_restricted_view(user) is False:
            return True
        else:
            screensaver_user = ScreensaverUser.objects.get(username=user.username)
            my_screens = self.get_user_screens(screensaver_user)
            
            activity = Activity.objects.get(activity_id=activity_id)
            result = activity.screen.facility_id in set([
                screen.facility_id for screen in my_screens])
            if not result:
                result = activity.serviced_user.username == user.username
            
            return False        

class ActivityResource(DbApiResource):

    class Meta:

        queryset = Activity.objects.all()
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'activity'
        authorization = ActivityResourceAuthorization(resource_name)
        serializer = LimsSerializer()
        
    def __init__(self, **kwargs):

        super(ActivityResource, self).__init__(**kwargs)
        self.su_resource = None
        self.screen_resource = None
        
    def get_screen_resource(self):
        if self.screen_resource is None:
            self.screen_resource = ScreenResource()
        return self.screen_resource
    
    def get_su_resource(self):
        if self.su_resource is None:
            self.su_resource = ScreensaverUserResource()
        return self.su_resource

    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)"
                r"/(?P<activity_id>[\d]+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
        ]
    
    def build_schema(self, user=None, **kwargs):
         
        schema = super(ActivityResource, self).build_schema(user=user, **kwargs)
        return schema

    @read_authorization
    def get_detail(self, request, **kwargs):
 
        activity_id = kwargs.pop('activity_id', None)
        if not activity_id:
            raise Http404('must provide an activity_id parameter')
        else:
            kwargs['activity_id__eq'] = activity_id
 
        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, **kwargs):
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        is_for_detail = kwargs.pop('is_for_detail', False)
        
        (field_hash, columns, stmt, count_stmt, filename) = \
            self.get_query(param_hash, request.user)
        
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            # use "use_vocab" as a proxy to also adjust siunits for viewing
            rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
                field_hash, rowproxy_generator)
        rowproxy_generator = \
           self._meta.authorization.get_row_property_generator(
               request.user, field_hash, rowproxy_generator)

        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
        
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None),
            use_caching=True)
      
    def get_join_and_custom_columns(self):
        '''
        Motivation: expose the join and custom column defs for subclass (LibraryScreening)
        '''
        _a = self.bridge['activity']
        _screen_activity = _a.alias('screen_activity')
        _screen = self.bridge['screen']
        _user_cte = ScreensaverUserResource.get_user_cte().cte('activity_user')
        _serviced_user = _user_cte.alias('svcu')
        _performed_by = _user_cte.alias('pbu')
        _created_by = _user_cte.alias('cbu')
        _screen_lead = _user_cte.alias('s_ls')
        _lh = ScreensaverUserResource.get_lab_head_cte('lht').cte('lht')
        _sfs = self.bridge['screen_funding_supports']
                
        j = _a
        j = j.join(
            _serviced_user,
            _a.c.serviced_user_id == _serviced_user.c.screensaver_user_id, isouter=True)
        j = j.join(
            _performed_by,
            _a.c.performed_by_id == _performed_by.c.screensaver_user_id, isouter=True)
        j = j.join(
            _created_by,
            _a.c.created_by_id == _created_by.c.screensaver_user_id, isouter=True)
        j = j.join(
            _screen,
            _a.c.screen_id == _screen.c.screen_id, isouter=True)
        j = j.join(
            _lh, _screen.c.lab_head_id==_lh.c.screensaver_user_id, isouter=True)
        j = j.join(
            _screen_lead, 
            _screen.c.lead_screener_id==_screen_lead.c.screensaver_user_id, isouter=True)
        custom_columns = {
            'serviced_user': _serviced_user.c.name,
            'serviced_user_id': _serviced_user.c.screensaver_user_id,
            'serviced_username': _serviced_user.c.username,
            'performed_by_user_id': _performed_by.c.screensaver_user_id,
            'performed_by_username': _performed_by.c.username,
            'performed_by_name': _performed_by.c.name,
            'created_by_user_id': _created_by.c.screensaver_user_id,
            'created_by_username': _created_by.c.username,
            'created_by_name': _created_by.c.name,
            'screen_lab_name': _lh.c.lab_name_full,
            'screen_lab_head_id': _lh.c.screensaver_user_id,
            'screen_funding_supports':
                select([func.array_to_string(
                    func.array_agg(literal_column('funding_support')
                    ), LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(
                        select([_sfs.c.funding_support])
                            .select_from(_sfs)
                            .order_by(_sfs.c.funding_support)
                            .where(_sfs.c.screen_id 
                                == literal_column('screen.screen_id'))
                            .alias('inner')),
            'screen_lead_screener_id': _screen_lead.c.screensaver_user_id,
            'screen_lead_screener_name': _screen_lead.c.name,
            'screen_date_of_last_activity': (
                select([func.max(_screen_activity.c.date_of_activity)])
                .select_from(_screen_activity)
                .where(_screen_activity.c.screen_id
                    == literal_column('screen.screen_id'))
                ),
            'cherry_pick_request_id': cast(
                literal_column("null"),sqlalchemy.sql.sqltypes.Numeric),
        }
        
        return (j, custom_columns)

        
    # TODO: 20180924 - get_query was created for the LibraryScreening subclass,
    # but it is not needed, see get_join_and_custom_columns
    def get_query(self, param_hash, user):
        # general setup
        schema = self.build_schema(user=user)
        logger.info('activity get_query, schema: %r, %r', 
            schema['key'], schema['fields'].keys())
        
        manual_field_includes = set(param_hash.get('includes', []))
        # for join to screen query (TODO: only include if screen fields rqst'd)
        manual_field_includes.add('screen_id')
        manual_field_includes.add('classification')
        manual_field_includes.add('serviced_user_id') # req'd for authorization
        # for join to cherrypickrequest (TODO: req'd to complete activity id link)
        manual_field_includes.add('cherry_pick_request_id')
        param_hash['includes'] = list(manual_field_includes)
        
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        
        filename = self._get_filename(readable_filter_hash, schema)
        
        filter_expression = \
            self._meta.authorization.filter(user,filter_expression)
              
        order_params = param_hash.get('order_by', [])
        order_params.append('-date_of_activity')
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)

        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)

        # specific setup
        (join_clause, custom_columns) = self.get_join_and_custom_columns()
        
        base_query_tables = ['activity', 'screen'] 

        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)
        
        stmt = select(columns.values()).select_from(join_clause)
        # stmt = self._meta.authorization.filter_in_sql(
        #     user, stmt, serviced_user_table=_serviced, screen_table=_screen)
        # general setup
         
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)

        # compiled_stmt = str(stmt.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)
        
        return (field_hash, columns, stmt, count_stmt, filename)

    @write_authorization
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):

        schema = kwargs.pop('schema', None)
        if schema is None:
            raise Exception('schema not initialized')
        fields = schema['fields']

        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)

        logger.info('patch Activity: %r, %r', id_kwargs, deserialized)
        
        patch = bool(id_kwargs)
        initializer_dict = self.parse(deserialized, create=not patch, schema=schema)
        errors = self.validate(initializer_dict, schema=schema, patch=patch)
        if errors:
            raise ValidationError(errors)
        
        activity_type = deserialized.get('type')
        classification = deserialized.get('classification')
        serviced_user_id = deserialized.get('serviced_user_id', None)
        performed_by_user_id = deserialized.get('performed_by_user_id', None)
        serviced_screen_facility_id = deserialized.get('screen_facility_id', None)
        
        if not patch:
            if serviced_user_id is None and serviced_screen_facility_id is None:
                msg = 'Either serviced user or serviced screen_facility_id is required'
                raise ValidationError({
                    'serviced_user_id': msg,
                    'screen_facility_id': msg
                    })
            if not activity_type:
                raise ValidationError(
                    key='type',
                    msg='required')
            
            screening_types = SCHEMA.VOCAB.activity.type.SCREENING_TYPES
            if activity_type in screening_types:
                raise ValidationError(
                    key='type',
                    msg='Reserved for "screening" Activities: (%s)'
                        % ', '.join(screening_types))    
            
            if not classification:
                raise ValidationError(
                    key='classification', msg='required')
            
            service_classifications = \
                SCHEMA.VOCAB.activity.classification.SERVICE_CLASSIFICATIONS
            if classification not in service_classifications:
                raise ValidationError(
                    key='classification', 
                    msg='A service activity must be one of (%s)'
                        % ', '.join(service_classifications))
            
            if not performed_by_user_id:
                raise ValidationError(
                    key='performed_by_user_id',
                    msg='required')

        if serviced_user_id:
            try:
                serviced_user = ScreensaverUser.objects.get(
                    screensaver_user_id=serviced_user_id)
                initializer_dict['serviced_user'] = serviced_user
            except ObjectDoesNotExist:
                raise ValidationError(
                    key='serviced_user_id',
                    msg='id does not exist: %r' % serviced_user_id)
        if serviced_screen_facility_id:
            try:
                serviced_screen = Screen.objects.get(
                    facility_id=serviced_screen_facility_id)
                initializer_dict['screen'] = serviced_screen
            except ObjectDoesNotExist:
                raise ValidationError(
                    key='screen_facility_id',
                    msg='screen does not exist: %r' % serviced_screen_facility_id)
        if performed_by_user_id:
            performed_by = \
                self.get_su_resource()._get_detail_response_internal(
                exact_fields=['screensaver_user_id','is_staff'],
                screensaver_user_id=performed_by_user_id)
            if not performed_by: 
                raise ValidationError(
                    key='performed_by_user_id',
                    msg='No such screensaver_user_id: %r' % performed_by_user_id)
            if performed_by.get('is_staff',False) != True:
                raise ValidationError(
                    key='performed_by_user_id',
                    msg='Must be a staff user')
            initializer_dict['performed_by_id'] = performed_by['screensaver_user_id']
        activity = None
        if patch:
            try:
                activity = Activity.objects.get(
                    pk=id_kwargs['activity_id'])
            except ObjectDoesNotExist:
                raise Http404(
                    'Activity does not exist for: %r', id_kwargs)
        else:
            # Set the created_by field:
            # NOTE: deprecate for SS V2
            try:
                adminuser = ScreensaverUser.objects.get(username=request.user.username)
            except ObjectDoesNotExist as e:
                logger.error('admin user: %r does not exist', request.user.username )
                raise
            
            activity = Activity()
            activity.created_by = adminuser
        
        for key, val in initializer_dict.items():
            setattr(activity, key, val)

        # Final Validation:
        # TODO: use specific vocab for the activity.classification
        vocab_scope = schema['fields']['type']['vocabulary_scope_ref']
        vocab_scope = vocab_scope.replace('*', activity.classification)
        type_vocab = self.get_vocab_resource()._get_vocabularies_by_scope(vocab_scope)
        if not type_vocab:
            logger.warn('no vocabulary found for scope: %r, field: %r', 
                vocab_scope, 'activity.type')
        if activity.type not in type_vocab:
            logger.error('activity.type %r not found in vocab: %r',
                activity.type, type_vocab.keys())
            raise ValidationError(
                key='type',
                msg='choices: (%s)' % ', '.join(type_vocab.keys()))
        activity.save()
        logger.info('saved activity: %r', activity)
        return { API_RESULT_OBJ: activity }

    def make_log_key(self, log, attributes, schema=None, **kwargs):

        super(ActivityResource, self).make_log_key(
            log, attributes, schema=schema, **kwargs)
        serviced_user_id = attributes.get('serviced_user_id')
        screen_facility_id = attributes.get('screen_facility_id')
        if serviced_user_id is not None:
            # NOTE: make the log uri more robust, with SU id as well
            log.uri = '/'.join(map(str,[
                log.ref_resource_name,'screensaveruser', serviced_user_id, 
                log.key]))
        elif screen_facility_id is not None:
            # NOTE: make the log uri more robust, with Screen id as well
            log.uri = '/'.join(map(str,[
                log.ref_resource_name,'screen', screen_facility_id, log.key]))
        
        logger.info('log uri: %r, %r', log.uri, log)

    @write_authorization
    @un_cache
    @transaction.atomic
    def delete_obj(self, request, deserialized, **kwargs):

        activity_id = kwargs.get('activity_id', None)
        if activity_id:
            try:
                activity = Activity.objects.get(
                    activity_id=activity_id)
                activity.delete()
            except ObjectDoesNotExist:
                logger.warn('no such Activity: %s' % activity_id)
                raise Exception(
                    'Activity for activity_id: %s not found' % activity_id)
        else:
            raise Exception(
                'Activity delete action requires an activity_id %s' 
                % kwargs)
        
# class ActivityResourceOld(DbApiResource):
#     '''
#     Activity Resource is a combination of the LabActivity and the ServiceActivity
#     
#     NOTE: 20170523
#     ActivityResource needs to be reworked; the current design is from the 
#     legacy Screensaver 1; 
#     Activity is both a logging facility and a join class between User, Screen, 
#     and CherryPickRequest.
#     Refactor to make Activity a Reporting resource that logs:
#     - performed by
#     - serviced resource: Screen, User
#     - serviced context: Screen, User, CherryPickRequest
#     - referenced resources: Equipment
#     - 
#     '''
# 
#     class Meta:
# 
#         queryset = Activity.objects.all()
#         authentication = MultiAuthentication(IccblBasicAuthentication(),
#                                              IccblSessionAuthentication())
#         resource_name = 'activity'
#         authorization = ActivityResourceAuthorization(resource_name)
#         serializer = LimsSerializer()
#         ordering = []
#         filtering = {}
#         always_return_data = True 
#         
#     def __init__(self, **kwargs):
# 
#         self.service_activity_resource = None
#         self.screen_resource = None
#         self.su_resource = None
#         super(ActivityResource, self).__init__(**kwargs)
# 
#     def prepend_urls(self):
#         
#         return [
#             url(r"^(?P<resource_name>%s)/schema%s$" 
#                 % (self._meta.resource_name, TRAILING_SLASH),
#                 self.wrap_view('get_schema'), name="api_get_schema"),
#             url(r"^(?P<resource_name>%s)"
#                 r"/(?P<activity_id>[\d]+)%s$" 
#                     % (self._meta.resource_name, TRAILING_SLASH),
#                 self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
#         ]
#     
#     def get_su_resource(self):
#         if self.su_resource is None:
#             self.su_resource = ScreensaverUserResource()
#         return self.su_resource
# 
#     def get_service_activity_resource(self):
#         if self.service_activity_resource is None:
#             self.service_activity_resource = ServiceActivityResource()
#         return self.service_activity_resource
# 
#     def get_screen_resource(self):
#         if self.screen_resource is None:
#             self.screen_resource = ScreenResource()
#         return self.screen_resource
# 
#     def build_schema(self, user=None, **kwargs):
#          
#         schema = super(ActivityResource, self).build_schema(user=user, **kwargs)
#         return schema
# 
#     @read_authorization
#     def get_detail(self, request, **kwargs):
#  
#         activity_id = kwargs.pop('activity_id', None)
#         if not activity_id:
#             raise Http404('must provide an activity_id parameter')
#         else:
#             kwargs['activity_id__eq'] = activity_id
#  
#         kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
#         kwargs['is_for_detail'] = True
#         return self.build_list_response(request, **kwargs)
#         
#     @read_authorization
#     def get_list(self, request, **kwargs):
# 
#         kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
#         return self.build_list_response(request, **kwargs)
# 
#     def get_custom_columns(self, alias_qualifier):
#         '''
#         Convenience method for subclasses: reusable custom columns
#         @param alias_qualifier a sql compatible string used to name subqueries
#             so that this method may be called multiple times to compose a query
#         '''
#         _screen = self.bridge['screen']
#         _activity = self.bridge['activity']
#         _su = self.bridge['screensaver_user']
#         # perform some hacks to speed up the query
#         _performed_by_users_cte = \
#             ScreensaverUserResource.get_user_cte()\
#             .where(
#                 exists(select([None]).select_from(_activity)
#                     .where(_su.c.screensaver_user_id==_activity.c.performed_by_id)))\
#             .cte('performers_%s' % alias_qualifier)
#         _performed_by = _performed_by_users_cte.alias('performed_by_%s' % alias_qualifier)
#         _performed_by1 = _performed_by_users_cte.alias('performed_by1_%s' % alias_qualifier)
#         _performed_by2 = _performed_by_users_cte.alias('performed_by2_%s' % alias_qualifier)
#         _created_by_users_cte = \
#             ScreensaverUserResource.get_user_cte()\
#             .where(
#                 exists(select([None]).select_from(_activity)
#                     .where(_su.c.screensaver_user_id==_activity.c.created_by_id)))\
#             .cte('creators_%s' % alias_qualifier)
#         _created_by = _created_by_users_cte.alias('created_by_%s' % alias_qualifier)
#         _created_by1 = _created_by_users_cte.alias('created_by1_%s' % alias_qualifier)
#         _lhsu = _su.alias('lhsu_%s' % alias_qualifier)
#         _sfs = self.bridge['screen_funding_supports']
#         
#         lab_head_table = \
#             ScreensaverUserResource.get_lab_head_cte(alias_qualifier)\
#                 .cte('lab_heads_%s' % alias_qualifier)
#         return {
#             'performed_by_name': (
#                 select([_performed_by1.c.name])
#                     .select_from(_performed_by1)
#                     .where(_performed_by1.c.screensaver_user_id 
#                         == _activity.c.performed_by_id)
#                 ),
#             'performed_by_username': (
#                 select([_performed_by.c.username])
#                     .select_from(_performed_by)
#                     .where(_performed_by.c.screensaver_user_id 
#                         == _activity.c.performed_by_id)
#                 ),
#             'performed_by_user_id': (
#                 select([_performed_by2.c.screensaver_user_id])
#                     .select_from(_performed_by2)
#                     .where(_performed_by2.c.screensaver_user_id 
#                         == _activity.c.performed_by_id)
#                 ),
#             'created_by_name': (
#                 select([_created_by1.c.name])
#                     .select_from(_created_by1)
#                     .where(_created_by1.c.screensaver_user_id 
#                         == _activity.c.created_by_id)
#                 ),
#             'created_by_username': (
#                 select([_created_by.c.username])
#                     .select_from(_created_by)
#                     .where(_created_by.c.screensaver_user_id 
#                         == _activity.c.created_by_id)
#                 ),
#             'screen_lab_affiliation': (
#                 select([lab_head_table.c.lab_affiliation])
#                 .select_from(lab_head_table)
#                 .where(lab_head_table.c.screensaver_user_id==_screen.c.lab_head_id)),
#             'screen_lab_name': (
#                 select([lab_head_table.c.lab_name_full])
#                 .select_from(lab_head_table)
#                 .where(lab_head_table.c.screensaver_user_id==_screen.c.lab_head_id)),
#             'screen_lab_head_id': (
#                 select([lab_head_table.c.screensaver_user_id])
#                 .select_from(lab_head_table)
#                 .where(lab_head_table.c.screensaver_user_id==_screen.c.lab_head_id)),
#             'screen_funding_supports':
#                 select([func.array_to_string(
#                     func.array_agg(literal_column('funding_support')
#                     ), LIST_DELIMITER_SQL_ARRAY)])
#                     .select_from(
#                         select([_sfs.c.funding_support])
#                             .select_from(_sfs)
#                             .order_by(_sfs.c.funding_support)
#                             .where(_sfs.c.screen_id 
#                                 == literal_column('screen.screen_id'))
#                             .alias('inner')),
#             'screen_lead_screener_name': (
#                 select([_concat(_su.c.first_name, ' ', _su.c.last_name)])
#                     .select_from(_su)
#                     .where(_su.c.screensaver_user_id 
#                         == _screen.c.lead_screener_id)),
#             'screen_lead_screener_id': (
#                 select([_su.c.screensaver_user_id])
#                     .select_from(_su)
#                     .where(_su.c.screensaver_user_id 
#                         == _screen.c.lead_screener_id)),
#             'screen_date_of_last_activity': literal_column(
#                 '( (select date_of_activity '
#                 '  from activity '
#                 '  join lab_activity la using(activity_id) '
#                 '  where la.screen_id=screen.screen_id '
#                 '  and not exists(select null from cherry_pick_liquid_transfer cplt'
#                 '     where cplt.activity_id = activity.activity_id) '
#                 '  UNION ALL'
#                 '  select date_of_activity '
#                 '  from activity '
#                 '  join service_activity sa using(activity_id) '
#                 '  where sa.serviced_screen_id=screen.screen_id )'
#                 '  order by date_of_activity desc LIMIT 1 )'),
#         }
# 
#     def get_query(self, param_hash, user):
#         # general setup
#         schema = self.build_schema(user=user)
#         logger.info('activity get_query, schema: %r, %r', 
#             schema['key'], schema['fields'].keys())
#         
#         manual_field_includes = set(param_hash.get('includes', []))
#         # for join to screen query (TODO: only include if screen fields rqst'd)
#         manual_field_includes.add('screen_id')
#         manual_field_includes.add('activity_class')
#         # for join to cherrypickrequest (TODO: req'd to complete activity id link)
#         manual_field_includes.add('cherry_pick_request_id')
#         param_hash['includes'] = list(manual_field_includes)
#         
#         (filter_expression, filter_hash, readable_filter_hash) = \
#             SqlAlchemyResource.build_sqlalchemy_filters(
#                 schema, param_hash=param_hash)
#         filename = self._get_filename(readable_filter_hash, schema)
#         
#         # NOTE: try "filter_in_sql" for performance
#         # NOTE: filters are done in the subquery clauses
#         # filter_expression = \
#         #     self._meta.authorization.filter(user,filter_expression)
#               
#         order_params = param_hash.get('order_by', [])
#         order_params.append('-date_of_activity')
#         field_hash = self.get_visible_fields(
#             schema['fields'], filter_hash.keys(), manual_field_includes,
#             param_hash.get('visibilities'),
#             exact_fields=set(param_hash.get('exact_fields', [])),
#             order_params=order_params)
#         order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
#             order_params, field_hash)
# 
#         # specific setup
#         
#         _activity = self.bridge['activity']
#         # Create a UNION query of each subclass query:
#         (field_hash_sa, columns_sa, stmt_sa, count_stmt_sa, filenamesa) = (
#             self.get_service_activity_resource().get_query(param_hash, user))
#         # if a field is not present in the subquery, create an empty field            
#         sa_columns = []
#         for key, field in field_hash.items():
#             if field['scope'] == 'fields.activity':
#                 if key not in columns_sa:
#                     if field['data_type'] == 'string':
#                         sa_columns.append(cast(literal_column("null"), 
#                             sqlalchemy.sql.sqltypes.Text).label(key))
#                     else:
#                         sa_columns.append(literal_column("null").label(key))
#                 else:
#                     sa_columns.append(literal_column(key))
#         stmt_sa = stmt_sa.cte('serviceactivities')
#         stmt1 = select(sa_columns).select_from(stmt_sa)
# 
#         (field_hash_la, columns_la, stmt_la, count_stmt_la) = (
#             self.get_lab_activity_query(param_hash, user))
#         # if a field is not present in the subquery, create an empty field            
#         la_columns = []
#         for key, field in field_hash.items():
#             if field['scope'] == 'fields.activity':
#                 if key not in columns_la:
#                     logger.info('create dummy col for %r', key)
#                     if field['data_type'] == 'string':
#                         la_columns.append(cast(literal_column("null"), 
#                             sqlalchemy.sql.sqltypes.Text).label(key))
#                     else: 
#                         la_columns.append(literal_column("null").label(key))
#                 else:
#                     la_columns.append(literal_column(key))
#                     
#         stmt_la = stmt_la.cte('labactivities')
#         stmt2 = select(la_columns).select_from(stmt_la)
#         
#         stmt = stmt1.union_all(stmt2)
#         (stmt, count_stmt) = self.wrap_statement(
#             stmt, order_clauses, filter_expression)
# 
# #         if logger.isEnabledFor(logging.DEBUG):
#         compiled_stmt = str(stmt.compile(
#             dialect=postgresql.dialect(),
#             compile_kwargs={"literal_binds": True}))
#         logger.info('compiled_stmt %s', compiled_stmt)
# 
# 
#         columns = { key:literal_column(key) for key in field_hash.keys()}
#         return (field_hash, columns, stmt, count_stmt, filename)
#     
#     def get_custom_lab_activity_columns(self, alias_qualifier):
# 
#         _library_screening = self.bridge['library_screening']
#         _cps = self.bridge['cherry_pick_screening']
#         # _cplt = self.bridge['cherry_pick_liquid_transfer']
#         _screen = self.bridge['screen']
#         
#         activity_type_column = cast(case([
#             (_library_screening.c.activity_id != None,
#                case([(_library_screening.c.is_for_external_library_plates,
#                         'externallibraryscreening')],
#                     else_='libraryscreening')),
#             (_cps.c.activity_id != None,
#                 'cherrypickscreening')
#             ],
#             else_='cplt'), sqlalchemy.sql.sqltypes.Text)
# 
#         return { 
#             'serviced_user_id': cast(
#                 literal_column("null"),sqlalchemy.sql.sqltypes.Numeric),
#             'serviced_user': cast(
#                 literal_column("null"),sqlalchemy.sql.sqltypes.Text),
#             'serviced_username': cast(
#                 literal_column("null"),sqlalchemy.sql.sqltypes.Text),
#             'funding_support': cast(
#                 literal_column("null"),sqlalchemy.sql.sqltypes.Text),
#             'cherry_pick_request_id': _cps.c.cherry_pick_request_id,
#             # 20171108 - Do not show CPLT activities in the general activity report
#             # 'cherry_pick_request_id': 
#             #     cast(func.coalesce(
#             #             _cps.c.cherry_pick_request_id, 
#             #             _cplt.c.cherry_pick_request_id, None), 
#             #         sqlalchemy.sql.sqltypes.Text),
#             'type': activity_type_column,
#             'activity_class': activity_type_column,
#             }
#         
#     def get_lab_activity_query(self, param_hash, user):
# 
#         # general setup
#         # schema for labactivity part of the query should only be the activity fields
#         # (exclude the screen.fields)
#         schema = deepcopy(self.build_schema(user=user))
#         field_hash = schema['fields']
#         field_hash = { key:val for key, val in field_hash.items() 
#             if val['scope'] == 'fields.activity'}  
#         schema['fields'] = field_hash
#         
#         manual_field_includes = set(param_hash.get('includes', []))
#         manual_field_includes.add('screen_id')
#         
#         (filter_expression, filter_hash, readable_filter_hash) = \
#             SqlAlchemyResource.build_sqlalchemy_filters(
#                 schema, param_hash=param_hash)
#         filter_expression = \
#             self._meta.authorization.filter(user,filter_expression)
#               
#         order_params = param_hash.get('order_by', [])
#         field_hash = self.get_visible_fields(
#             schema['fields'], filter_hash.keys(), manual_field_includes,
#             param_hash.get('visibilities'),
#             exact_fields=set(param_hash.get('exact_fields', [])),
#             order_params=order_params)
#         order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
#             order_params, field_hash)
#          
#         # specific setup
#         _sfs = self.bridge['screen_funding_supports']
#         _a = self.bridge['activity']
#         _la = self.bridge['lab_activity']
#         _screening = self.bridge['screening']
#         _screen = self.bridge['screen']
# 
#         j = _a
#         j = j.join(_la, _a.c.activity_id == _la.c.activity_id)
#         j = j.join(_screen, _la.c.screen_id == _screen.c.screen_id)
# 
#         # TODO: delegate to sub_classes (when built)
#         _library_screening = self.bridge['library_screening']
#         _cps = self.bridge['cherry_pick_screening']
#         # 20171108 - Do not show CPLT activities in general report
#         _cplt = self.bridge['cherry_pick_liquid_transfer']
#         j = j.join(
#             _library_screening,
#             _la.c.activity_id == _library_screening.c.activity_id, isouter=True)
#         j = j.join(_cps, _la.c.activity_id == _cps.c.activity_id, isouter=True)
#         # j = j.join(_cplt, _la.c.activity_id == _cplt.c.activity_id, isouter=True)
#                 
#         custom_columns = self.get_custom_columns('la')
#         custom_columns.update(self.get_custom_lab_activity_columns('lab_activity'))
#         
#         base_query_tables = ['activity', 'lab_activity', 'screening', 'screen'] 
#         columns = self.build_sqlalchemy_columns(
#             field_hash.values(), base_query_tables=base_query_tables,
#             custom_columns=custom_columns)
#         
#         stmt = select(columns.values()).select_from(j)
#         
#         stmt = stmt.where(~exists(
#             select([None]).select_from(_cplt).where(_cplt.c.activity_id==_a.c.activity_id)))
#         # stmt = self._meta.authorization.filter_in_sql(
#         #     user, stmt, screen_table=_screen)
#         # general setup
#          
#         (stmt, count_stmt) = self.wrap_statement(
#             stmt, order_clauses, filter_expression)
#         
#         if logger.isEnabledFor(logging.DEBUG):
#             compiled_stmt = str(stmt.compile(
#                 dialect=postgresql.dialect(),
#                 compile_kwargs={"literal_binds": True}))
#             logger.info('compiled_stmt %s', compiled_stmt)
#         
#         return (field_hash, columns, stmt, count_stmt)
# 
#     def build_list_response(self, request, **kwargs):
#         schema = kwargs.pop('schema', None)
#         if not schema:
#             raise Exception('schema not initialized')
# 
#         param_hash = self._convert_request_to_dict(request)
#         param_hash.update(kwargs)
#         is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
#         use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
#         use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
#         if is_data_interchange:
#             use_vocab = False
#             use_titles = False
#         is_for_detail = kwargs.pop('is_for_detail', False)
#         
#         (field_hash, columns, stmt, count_stmt, filename) = \
#             self.get_query(param_hash, request.user)
#         
#         rowproxy_generator = None
#         if use_vocab is True:
#             rowproxy_generator = \
#                 DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
#             # use "use_vocab" as a proxy to also adjust siunits for viewing
#             rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
#                 field_hash, rowproxy_generator)
#         rowproxy_generator = \
#            self._meta.authorization.get_row_property_generator(
#                request.user, field_hash, rowproxy_generator)
# 
#         title_function = None
#         if use_titles is True:
#             def title_function(key):
#                 return field_hash[key]['title']
#         if is_data_interchange:
#             title_function = DbApiResource.datainterchange_title_function(
#                 field_hash,schema['id_attribute'])
#         
#         return self.stream_response_from_statement(
#             request, stmt, count_stmt, filename,
#             field_hash=field_hash,
#             param_hash=param_hash,
#             is_for_detail=is_for_detail,
#             rowproxy_generator=rowproxy_generator,
#             title_function=title_function, meta=kwargs.get('meta', None),
#             use_caching=True)
             

# class ServiceActivityResource(ActivityResource):    
# 
#     class Meta:
# 
#         queryset = ServiceActivity.objects.all()
#         authentication = MultiAuthentication(IccblBasicAuthentication(),
#                                              IccblSessionAuthentication())
#         resource_name = 'serviceactivity'
#         authorization = ActivityResourceAuthorization(resource_name)
#         ordering = []
#         filtering = {}
#         serializer = LimsSerializer()
#         max_limit = 10000
#         always_return_data = True
# 
#     def __init__(self, **kwargs):
#         super(ServiceActivityResource, self).__init__(**kwargs)
# 
#     def prepend_urls(self):
# 
#         return [
#             url(r"^(?P<resource_name>%s)/schema%s$" 
#                 % (self._meta.resource_name, TRAILING_SLASH),
#                 self.wrap_view('get_schema'), name="api_get_schema"),
#             url((r"^(?P<resource_name>%s)/" 
#                  r"(?P<activity_id>([\d]+))%s$")
#                     % (self._meta.resource_name, TRAILING_SLASH),
#                 self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
#             url(r"^(?P<resource_name>%s)/for_user/(?P<serviced_user_id>([\d]+))%s$" 
#                     % (self._meta.resource_name, TRAILING_SLASH),
#                 self.wrap_view('dispatch_list'), name="api_dispatch_list"),
#             url(r"^(?P<resource_name>%s)/for_user/(?P<serviced_user_id>([\d]+))%s$" 
#                     % (self._meta.resource_name, TRAILING_SLASH),
#                 self.wrap_view('dispatch_list'), name="api_dispatch_list"),
#         ]    
# 
#     @write_authorization
#     @transaction.atomic
#     def patch_obj(self, request, deserialized, **kwargs):
# 
#         schema = kwargs.pop('schema', None)
#         if schema is None:
#             raise Exception('schema not initialized')
#         fields = schema['fields']
# 
#         # NOTE: parse params only if needed for client overrides
#         # param_hash = self._convert_request_to_dict(request)
#         # param_hash.update(kwargs)
#         # logger.debug('param_hash: %r', param_hash)
# 
#         id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
# 
#         logger.info('patch ServiceActivity: %r', deserialized)
#         
#         patch = bool(id_kwargs)
#         initializer_dict = self.parse(deserialized, create=not patch, schema=schema)
#         errors = self.validate(initializer_dict, schema=schema, patch=patch)
#         if errors:
#             raise ValidationError(errors)
#         
#         activity_type = deserialized.get('type', None)
#         if activity_type:
#             initializer_dict['service_activity_type'] = activity_type
# 
#         serviced_user_id = deserialized.get('serviced_user_id', None)
#         performed_by_user_id = deserialized.get('performed_by_user_id', None)
#         serviced_screen_facility_id = deserialized.get('screen_facility_id', None)
#         
#         if not patch:
#             if serviced_user_id is None and serviced_screen_facility_id is None:
#                 msg = 'Either serviced user or serviced screen_facility_id is required'
#                 raise ValidationError({
#                     'serviced_user_id': msg,
#                     'screen_facility_id': msg
#                     })
#             if not activity_type:
#                 raise ValidationError(
#                     key='type',
#                     msg='required')
#             if not performed_by_user_id:
#                 raise ValidationError(
#                     key='performed_by_user_id',
#                     msg='required')
# 
#         if serviced_user_id:
#             try:
#                 serviced_user = ScreensaverUser.objects.get(
#                     screensaver_user_id=serviced_user_id)
#                 initializer_dict['serviced_user'] = serviced_user
#             except ObjectDoesNotExist:
#                 raise ValidationError(
#                     key='serviced_user_id',
#                     msg='id does not exist: %r' % serviced_user_id)
#         if serviced_screen_facility_id:
#             try:
#                 serviced_screen = Screen.objects.get(
#                     facility_id=serviced_screen_facility_id)
#                 initializer_dict['serviced_screen'] = serviced_screen
#             except ObjectDoesNotExist:
#                 raise ValidationError(
#                     key='screen_facility_id',
#                     msg='screen does not exist: %r' % serviced_screen_facility_id)
#         if performed_by_user_id:
#             performed_by = \
#                 self.get_su_resource()._get_detail_response_internal(
#                 exact_fields=['screensaver_user_id','is_staff'],
#                 screensaver_user_id=performed_by_user_id)
#             if not performed_by: 
#                 raise ValidationError(
#                     key='performed_by_user_id',
#                     msg='No such screensaver_user_id: %r' % performed_by_user_id)
#             if performed_by.get('is_staff',False) != True:
#                 raise ValidationError(
#                     key='performed_by_user_id',
#                     msg='Must be a staff user')
# 
#         service_activity = None
#         if patch:
#             try:
#                 service_activity = ServiceActivity.objects.get(
#                     pk=id_kwargs['activity_id'])
#             except ObjectDoesNotExist:
#                 raise Http404(
#                     'ServiceActivity does not exist for: %r', id_kwargs)
#         else:
#             # Set the created_by field:
#             # NOTE: deprecate for SS V2
#             try:
#                 adminuser = ScreensaverUser.objects.get(username=request.user.username)
#             except ObjectDoesNotExist as e:
#                 logger.error('admin user: %r does not exist', request.user.username )
#                 raise
#             
#             service_activity = ServiceActivity()
#             service_activity.created_by = adminuser
#         
#         service_activity.performed_by_id = performed_by_user_id
#         
#         model_field_names = [
#             x.name for x in service_activity._meta.get_fields()]
#         for key, val in initializer_dict.items():
#             if key in model_field_names:
#                 setattr(service_activity, key, val)
# 
#         service_activity.save()
#         logger.info('saved service_activity: %r', service_activity)
#         return { API_RESULT_OBJ: service_activity }
# 
#     def make_log_key(self, log, attributes, id_attribute=None, schema=None, **kwargs):
# 
#         logger.debug('make_log_key: %r, %r, %r', attributes, id_attribute, kwargs)
#         
#         if attributes:
#             ActivityResource.make_log_key(
#                 self, log, attributes, id_attribute=id_attribute, 
#                 schema=schema, **kwargs)
#             logger.info('log key: %r, %r', log.key, log)
#     
#             keys = []
#             # Always create the service activity log for the user, if available 
#             if attributes.get('serviced_user_id', None):
#                 keys.append('screensaveruser')
#                 keys.append(str(attributes['serviced_user_id']))
#             elif attributes.get('screen_facility_id', None):
#                 keys.append('screen')
#                 keys.append(attributes['screen_facility_id'])
#             keys.append(self._meta.resource_name)
#             log.uri = '%s/%s' % ('/'.join(keys), log.key)
#         
#             logger.info('log uri: %r, %r', log.uri, log)
# 
#     @write_authorization
#     @un_cache
#     @transaction.atomic
#     def delete_obj(self, request, deserialized, **kwargs):
# 
#         activity_id = kwargs.get('activity_id', None)
#         if activity_id:
#             try:
#                 sa = ServiceActivity.objects.get(
#                     activity_id=activity_id)
#                 sa.delete()
#             except ObjectDoesNotExist:
#                 logger.warn('no such ServiceActivity: %s' % activity_id)
#                 raise Exception(
#                     'ServiceActivity for activity_id: %s not found' % activity_id)
#         else:
#             raise Exception(
#                 'ServiceActivity delete action requires an activity_id %s' 
#                 % kwargs)
#         
#     def get_query(self, param_hash, user):
# 
#         schema = self.build_schema(user=user)
#         logger.debug('serviceactivity query: %r', schema['fields'].keys())
#         # general setup
#         alias_qualifier = 'sa'
#         manual_field_includes = set(param_hash.get('includes', []))
#         # for join to screen query (TODO: only include if screen fields rqst'd)
#         manual_field_includes.add('screen_id')
#         
#         (filter_expression, filter_hash, readable_filter_hash) = \
#             SqlAlchemyResource.build_sqlalchemy_filters(
#                 schema, param_hash=param_hash)
#         filename = self._get_filename(readable_filter_hash, schema)
#         filter_expression = \
#             self._meta.authorization.filter(user,filter_expression)
# 
#               
#         order_params = param_hash.get('order_by', [])
#         order_params.append('-date_of_activity')
#         field_hash = self.get_visible_fields(
#             schema['fields'], filter_hash.keys(), manual_field_includes,
#             param_hash.get('visibilities'),
#             exact_fields=set(param_hash.get('exact_fields', [])),
#             order_params=order_params)
#         order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
#             order_params, field_hash)
#          
#         # specific setup
#         _a = self.bridge['activity']
#         _sa = self.bridge['service_activity']
#         _screen = self.bridge['screen']
#         _user_cte = ScreensaverUserResource.get_user_cte().cte('users_serviced_sa')
#         _serviced = _user_cte.alias('serviced_user')
#         
#         j = _a
#         j = j.join(_sa, _a.c.activity_id == _sa.c.activity_id)
#         j = j.join(
#             _serviced,
#             _sa.c.serviced_user_id == _serviced.c.screensaver_user_id, isouter=True)
#         j = j.join(
#             _screen,
#             _sa.c.serviced_screen_id == _screen.c.screen_id, isouter=True)
#         
#         # Get custom columns from the parent (ActivityResource); the
#         # ServiceActivity query rows contain the needed activity and screen ids
#         custom_columns = \
#             super(ServiceActivityResource, self).get_custom_columns('sa')
#         custom_columns.update({
#             'activity_class': cast(
#                 literal_column("'serviceactivity'"), 
#                 sqlalchemy.sql.sqltypes.Text),
#             'serviced_user': _serviced.c.name,
#             'serviced_username': _serviced.c.username,
#             'serviced_user_id': _serviced.c.screensaver_user_id,
#             })
# 
#         base_query_tables = ['activity', 'service_activity', 'screen'] 
#         columns = self.build_sqlalchemy_columns(
#             field_hash.values(), base_query_tables=base_query_tables,
#             custom_columns=custom_columns)
#         
#         stmt = select(columns.values()).select_from(j)
#         # stmt = self._meta.authorization.filter_in_sql(
#         #     user, stmt, serviced_user_table=_serviced, screen_table=_screen)
#         # general setup
#          
#         (stmt, count_stmt) = self.wrap_statement(
#             stmt, order_clauses, filter_expression)
# 
#         # compiled_stmt = str(stmt.compile(
#         #     dialect=postgresql.dialect(),
#         #     compile_kwargs={"literal_binds": True}))
#         # logger.info('compiled_stmt %s', compiled_stmt)
#         
#         return (field_hash, columns, stmt, count_stmt, filename)


class LibraryScreeningResource(ActivityResource):
    
    # Constants: may be overridden in the settings.py
    MIN_WELL_VOL_SMALL_MOLECULE = Decimal('0.0000069') # 6.9 uL
    MIN_WELL_VOL_RNAI = Decimal(0)
    
    ALLOWED_LIBRARY_SCREENING_STATUS = (SCHEMA.VOCAB.library.screening_status.ALLOWED,)
    WARN_LIBRARY_SCREENING_STATUS = (
        SCHEMA.VOCAB.library.screening_status.REQUIRES_PERMISSION,
        SCHEMA.VOCAB.library.screening_status.NOT_RECOMMENDED,
        SCHEMA.VOCAB.library.screening_status.RETIRED,)
    # NOTE: all other library screening status are error statuses
    # ERROR_LIBRARY_SCREENING_STATUS = (
    #     'not_allowed','discarded',)
    ALLOWED_PLATE_STATUS = (
        SCHEMA.VOCAB.plate.status.AVAILABLE,
        SCHEMA.VOCAB.plate.status.RETIRED,)
    WARN_PLATE_STATUS = (SCHEMA.VOCAB.plate.status.RETIRED,)
    # NOTE: all other status are error statuses:
    # ERROR_PLATE_STATUS = [
    #     'discarded', 'given_away','not_specified', 'not_available', 
    #     'not_created', 'discarded_volume_transferred', 'lost']
    ALLOWED_COPY_USAGE_TYPE = (SCHEMA.VOCAB.copy.usage_type.LIBRARY_SCREENING_PLATES,)
    # NOTE: all other copy usage types are errors
    SCREENING_COUNT_THRESHOLD = 12

    # Messages for Screening Inquiry errors
    MSG_ALREADY_SCREENED = 'Plates have already been screened'
    MSG_PLATE_STATUS_ERROR = 'Plate status'
    MSG_SCREENING_COUNT = 'Screening count > %d' % SCREENING_COUNT_THRESHOLD
    MSG_PLATES_WELLS_ADJUSTED = 'Plate well volumes have been adjusted'
    MSG_COPY_USAGE_TYPE = 'Copy usage type'
    MSG_INSUFFICIENT_VOL = 'Insufficient vol: %s uL'
    MSG_NO_PLATE_VOLUME = 'No plate volume recorded'
    MSG_LIBRARY_SCREENING_STATUS = 'Library screening status'
    MSG_LIBRARY_SCREENING_TYPE = 'Library screening type'

    class Meta:

        queryset = LibraryScreening.objects.all()
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'libraryscreening'
        alt_resource_name = 'externallibraryscreening'
        authorization = ActivityResourceAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        max_limit = 10000
        always_return_data = True

    def __init__(self, **kwargs):
        super(LibraryScreeningResource, self).__init__(**kwargs)
        
        self.plate_resource = None
        self.copywell_resource = None
        self.library_resource = None
        self.lcp_resource = None
        
    def get_librarycopyplate_resource(self):
        if self.lcp_resource is None:
            self.lcp_resource = LibraryCopyPlateResource()
        return self.lcp_resource
    
    def get_library_resource(self):
        if self.library_resource is None:
            self.library_resource = LibraryResource()
        return self.library_resource
        
    def get_plate_resource(self):
        if self.plate_resource is None:
            self.plate_resource = LibraryCopyPlateResource()
        return self.plate_resource

    def get_copywell_resource(self):
        if self.copywell_resource is None:
            self.copywell_resource = CopyWellResource()
        return self.copywell_resource
    
    def prepend_urls(self):

        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url((r"^(?P<resource_name>%s)/" 
                 r"(?P<activity_id>([\d]+))%s$")
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<activity_id>([\d]+))/plates%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_plates_screened_view'),
                name="api_dispatch_plates_screened_view"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<activity_id>([\d]+))/libraries%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_libraries_screened_view'),
                name="api_dispatch_libraries_screened_view"),
        ]  
        
    def dispatch_plates_screened_view(self, request, **kwargs):

        library_screening_id = kwargs.pop('activity_id')
        # NOTE: authorization is performed in LibraryCopyPlateResource
        return self.get_plate_resource()\
            .build_screened_plate_response(
                request, library_screening_id=library_screening_id, **kwargs)    
        
    def dispatch_libraries_screened_view(self, request, **kwargs):

        library_screening_id = kwargs.pop('activity_id')
        # add custom authorization
        if self._meta.authorization.has_activity_read_authorization(
                request.user, library_screening_id) is False:
            raise PermissionDenied

        with get_engine().connect() as conn:
            _l = self.bridge['library']
            _ap = self.bridge['assay_plate']
            _c = self.bridge['copy']
            _p = self.bridge['plate']
            
            library_names = [x[0] for x in 
                conn.execute(
                    select([distinct(_l.c.short_name)])
                    .select_from(
                        _l.join(_c, _l.c.library_id==_c.c.library_id)
                          .join(_p, _p.c.copy_id==_c.c.copy_id)
                          .join(_ap, _ap.c.plate_id==_p.c.plate_id))
                    .where(_ap.c.library_screening_id==library_screening_id))]
            
            kwargs[API_PARAM_SHOW_ARCHIVED] = True
            return self.get_library_resource().get_list(request,
                short_name__in=library_names, **kwargs)

    def dispatch_plate_range_search_view(self, request, **kwargs):
        ''' 
        Find: 
        - plates already asssociated with the libraryscreenings for the screen
        - plates matched by API_PARAM_SEARCH
        Note: bypasses the "dispatch" framework call
        -- must be authenticated and authorized
        '''
        logger.info('dispatch_plate_range_search_view')
        self.is_authenticated(request)
        resource_name = kwargs.pop('resource_name', self._meta.resource_name)
        if not self._meta.authorization._is_resource_authorized(
                request.user,'read'):
            raise PermissionDenied(
                'user: %s, permission: %s/%s not found' 
                    % (request.user,resource_name,'read'))
        if request.method.lower() not in ['get','post']:
            return self.dispatch('detail', request, **kwargs)

        # With POST, params may come in the request
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        logger.info('params: %r', param_hash)
        
        facility_id = param_hash.get('facility_id', None)
        activity_id = param_hash.get('activity_id', None)
        plate_search_data = param_hash.get(SCHEMA.API_PARAM_SEARCH, None)
        volume_required = parse_val(param_hash.get('volume_required', None),
            'volume_required', 'decimal')
        show_retired_plates = parse_val(
            param_hash.get('show_retired_plates'),
            'show_retired_plates','boolean')
        plate_status_types = [SCHEMA.VOCAB.plate.status.AVAILABLE]
        if show_retired_plates:
            plate_status_types.append(SCHEMA.VOCAB.plate.status.RETIRED)
        show_first_copy_only = parse_val(
            param_hash.get('show_first_copy_only',None),
            'show_first_copy_only','boolean')    
        hide_existing = parse_val(
            param_hash.get('hide_existing',None),
            'hide_existing','boolean')    
        
        if not facility_id:
            raise MissingParam('facility_id')
        if self.get_screen_resource()._meta.authorization\
            .has_screen_read_authorization(request.user, facility_id) is False:
            raise PermissionDenied
        try:
            screen = Screen.objects.get(facility_id=facility_id)
        except ObjectDoesNotExist:
            raise Http404(
                'screen does not exist for: %r', facility_id)
        filename = 'plate_search_for_%s' % screen.facility_id

        library_screening = None
        if activity_id is not None:
            filename += '_screening_' + activity_id
            if self._meta.authorization.has_activity_read_authorization(
                request.user, activity_id) is False:
                raise PermissionDenied
            library_screening = LibraryScreening.objects.get(activity_id=activity_id)
        
        searched_plate_ids = []
        plate_search_errors = []
        if plate_search_data:
            parsed_searches = self.get_librarycopyplate_resource()\
                .parse_plate_copy_search(plate_search_data)
            (plates,errors) = \
                self.get_librarycopyplate_resource().find_plates(parsed_searches)
            if errors:
                logger.info('errors: %r', errors)
                plate_search_errors.append(
                    'Plates not found: %s' % ', '.join(errors))
            # library screening plates only
            library_screening_plates_only = []
            wrong_type_errors = defaultdict(set)
            for plate in plates:
                if plate.copy.usage_type in self.ALLOWED_COPY_USAGE_TYPE:
                    library_screening_plates_only.append(plate)
                else:
                    # Only show error if explicitly searching for the copy-plate
                    copy_plate_key = '%s/%s' %(plate.copy.name, plate.plate_number)
                    for parsed_search in parsed_searches:
                        if copy_plate_key in parsed_search['plate_copy_keys_expected']:
                            wrong_type_errors[plate.copy.usage_type].add(copy_plate_key)
            plates = library_screening_plates_only
            if wrong_type_errors:
                plate_search_errors.append(
                    self.MSG_COPY_USAGE_TYPE + ': %s' % '; '.join(
                        ['%s: %s' % (k, ', '.join(v)) 
                            for k,v in wrong_type_errors.items()]))
            # status types
            allowed_status_plates_only = []
            wrong_status_errors = defaultdict(set)
            for plate in plates:
                if plate.status in plate_status_types:
                    allowed_status_plates_only.append(plate)
                else:
                    # Only show error if explicitly searching for the copy-plate
                    copy_plate_key = '%s/%s' %(plate.copy.name, plate.plate_number)
                    for parsed_search in parsed_searches:
                        if copy_plate_key in parsed_search['plate_copy_keys_expected']:
                            wrong_status_errors[plate.status].add(copy_plate_key)
            plates = allowed_status_plates_only
            if wrong_status_errors:
                plate_search_errors.append(
                    self.MSG_PLATE_STATUS_ERROR + ': %s' % '; '.join(
                        ['%s: %s' % (k, ', '.join(v)) 
                            for k,v in wrong_status_errors.items()]))
                        
                        
            logger.info('show_first_copy_only: %r',show_first_copy_only)
            if show_first_copy_only is not True:
                searched_plate_ids = [x.plate_id for x in plates]
            else:
                plate_copy_map = defaultdict(list)
                for plate in plates:
                    plate_copy_map[plate.plate_number].append(
                        (plate.plate_location.freezer, plate.copy.name,plate.plate_id))
                for plate_number,copy_list in plate_copy_map.items():
                    searched_plate_ids.append(sorted(copy_list)[0][2])
        _data = []
        _data = self.get_plate_range_search_table(
            screen, searched_plate_ids,
            volume_required=volume_required)
        
        # If the library_screening ID is provided, filter out the extra rows
        # for other screenings as a convenience
        if library_screening is not None:
            new_data = []
            for x in _data:
                logger.info('filter data: %r', x)
                if x['library_screening_id'] == 0:
                    new_data.append(x)
                elif library_screening is not None:
                    if x['library_screening_id'] == library_screening.activity_id:
                        new_data.append(x)
            _data = new_data
        if hide_existing is True:
            logger.info('hide_existing for plate range search...')
            new_data = []
            for x in _data:
                if x['library_screening_id'] == 0:
                    new_data.append(x)
            _data = new_data
        
        meta = { 'total_count': len(_data) }
        if plate_search_errors:
            meta[SCHEMA.API_MSG_WARNING] = plate_search_errors
        response_data = {
            API_RESULT_META: meta,
            API_RESULT_DATA: _data 
        }
        return self.build_response(request, response_data, filename=filename)

    def get_plate_range_search_table(
        self, screen, searched_plate_ids, volume_required=None):
        '''
        Generate a (screening inquiry) plate range table:
        - for the searched_plate_ids
        - find extant plate_ranges for the screen 
        - check for errors and warnings 
        '''    

        LSR = self
        logger.info('get_plate_range_search_table for ids: %r ...', searched_plate_ids)
        class ErrorDict():
            ''' Track errors and warnings for each plate range
            '''
            def __init__(self):
                self.errors = defaultdict(set)
                self.warnings = defaultdict(set)
                self.plate_errors = defaultdict(set)
                self.plate_warnings = defaultdict(set)
                
            def addPlateError(self, key, plate_number):
                self.plate_errors[key].add(plate_number)
            def addPlateWarning(self, key, plate_number):
                self.plate_warnings[key].add(plate_number)
            def addError(self, key, value):
                self.errors[key].add(value)
            def addWarning(self, key, value):
                self.warnings[key].add(value)
            
            def check_row(self,_row):
                logger.debug('check row: %r', _row)
                if  _row['activity_id'] == 0:
                    if _row['plate_number'] in extant_plate_numbers:
                        self.addPlateWarning(
                            LSR.MSG_ALREADY_SCREENED, plate_number)
                    if _row['status'] not in LSR.ALLOWED_PLATE_STATUS:
                        self.addPlateError(
                            LSR.MSG_PLATE_STATUS_ERROR + ': %s' % _row['status'],
                            plate_number)
                    if _row['status'] in LSR.WARN_PLATE_STATUS:
                        self.addPlateWarning(
                            LSR.MSG_PLATE_STATUS_ERROR + ': %s' % _row['status'],
                            plate_number)
                    if  _row['usage_type'] != 'library_screening_plates':
                        self.addError(LSR.MSG_COPY_USAGE_TYPE, _row['usage_type'])
                    if _row['remaining_well_volume']:
                        vol_min = LSR.MIN_WELL_VOL_RNAI
                        error_key = LSR.MSG_INSUFFICIENT_VOL
                        if screen.screen_type == SCREEN_TYPE.SMALL_MOLECULE:
                            vol_min = LSR.MIN_WELL_VOL_SMALL_MOLECULE
                            error_key += (' (req %s)' 
                                % si_unit.convert_decimal(vol_min,1e-6, 1))
                        if volume_required is not None:
                            vol_after_transfer = (
                                Decimal(_row['remaining_well_volume']) 
                                    - volume_required )
                            if vol_after_transfer < vol_min:
                                self.addPlateError(
                                    error_key % si_unit.convert_decimal(
                                        vol_after_transfer,1e-6, 1),
                                    plate_number)
                    else:
                        self.addPlateWarning(LSR.MSG_NO_PLATE_VOLUME,plate_number)
                    if _row.get('cplt_screening_count',0) > 0:
                        self.addPlateWarning(
                            LSR.MSG_PLATES_WELLS_ADJUSTED, plate_number)
                    if _row.get('screening_count',0) > LSR.SCREENING_COUNT_THRESHOLD:
                        self.addPlateWarning(
                            LSR.MSG_SCREENING_COUNT, plate_number)

                if (_row['library_screening_status'] 
                        in LSR.WARN_LIBRARY_SCREENING_STATUS):
                    self.addWarning(
                        LSR.MSG_LIBRARY_SCREENING_STATUS,
                        _row['library_screening_status'])
                elif (_row['library_screening_status'] 
                        not in LSR.ALLOWED_LIBRARY_SCREENING_STATUS):
                    self.addError(
                        LSR.MSG_LIBRARY_SCREENING_STATUS,
                        _row['library_screening_status'])
                if _row['library_screen_type'] != screen.screen_type:
                    self.addError(LSR.MSG_LIBRARY_SCREENING_TYPE,
                        _row['library_screen_type'])
            
            def showErrors(self):
                full_errors = [ '%s: %s' % (k,', '.join(v))
                    for k,v in self.errors.items()]
                for k,v in self.plate_errors.items():
                    full_errors.append(
                        '%s: %s' % (k, ', '.join(lims_utils.find_ranges(v))))
                return sorted(full_errors)

            def showWarnings(self):
                full_warnings = [ '%s: %s' % (k,', '.join(v))
                    for k,v in self.warnings.items()]
                for k,v in self.plate_warnings.items():
                    full_warnings.append(
                        '%s: %s' % (k, ', '.join(lims_utils.find_ranges(v))))
                return sorted(full_warnings)

        logger.info('plate range search screen: %r, vol: %r', 
            screen.facility_id, volume_required)
        with get_engine().connect() as conn:
            _a = self.bridge['activity']
            _c = self.bridge['copy']
            _p = self.bridge['plate']
            _pl = self.bridge['plate_location']
            _l = self.bridge['library']
            _ls = self.bridge['library_screening']
            _ap = self.bridge['assay_plate']
            _screen = self.bridge['screen']
            
            fields = [
                'activity_id', 'plate_key', 'library_short_name', 
                'library_screening_status', 'library_screen_type', 'copy_name', 
                'copy_comments', 'plate_location', 'usage_type', 
                'plate_number','cplt_screening_count', 'screening_count', 
                'remaining_well_volume','status']
            # 1. query for current library screening plates
            _assay_plates_query = (select([
                _ap.c.plate_id,
                _ap.c.plate_number,
                _ls.c.activity_id,
                _a.c.date_of_activity,
                _c.c.copy_id,
                _c.c.name.label('copy_name'),
                _l.c.short_name,
                _c.c.library_id,
                ])
                .select_from(
                    _ap.join(_ls,_ap.c.library_screening_id==_ls.c.activity_id)
                       .join(_a,_ls.c.activity_id==_a.c.activity_id)
                       .join(_p, _ap.c.plate_id==_p.c.plate_id)
                       .join(_c, _c.c.copy_id==_p.c.copy_id)
                       .join(_l, _c.c.library_id==_l.c.library_id)
                    )
                .where(_ap.c.replicate_ordinal==0))
            extant_plate_numbers = []
            if screen is not None:
                _assay_plates_query = _assay_plates_query.where(
                    _a.c.screen_id==screen.screen_id)
                extant_plate_numbers = [
                    x[1] for x in conn.execute(_assay_plates_query)]
            _assay_plates_query = _assay_plates_query.cte('assay_plates')
            j = _p
            j = j.join(_c, _c.c.copy_id==_p.c.copy_id)
            j = j.join(_l, _c.c.library_id==_l.c.library_id)
            j = j.join(_pl, _p.c.plate_location_id==_pl.c.plate_location_id,
                isouter=True)
            j = j.join(_assay_plates_query, _p.c.plate_id
                ==_assay_plates_query.c.plate_id)
            _extant_query = ( 
                select([
                    _assay_plates_query.c.activity_id,
                    _concat(
                        _l.c.short_name, '/', _c.c.name, '/', 
                        cast(_p.c.plate_number, sqlalchemy.sql.sqltypes.Text)
                    ).label('plate_key'),
                    _l.c.short_name.label('library_short_name'),
                    _l.c.screening_status.label('library_screening_status'),
                    _l.c.screen_type.label('library_screen_type'),
                    _c.c.name.label('copy_name'),
                    _c.c.comments.label('copy_comments'),
                    _concat_with_sep(
                        args=[_pl.c.room,_pl.c.freezer,_pl.c.shelf,_pl.c.bin],
                        sep='-').label('plate_location'),
                    _c.c.usage_type,
                    _p.c.plate_number,
                    _p.c.cplt_screening_count,
                    # TODO: consider using the cumulative_freeze_thaw_count
                    _p.c.screening_count,
                    _p.c.remaining_well_volume,
                    _p.c.status,
                ])
                .select_from(j)
                .order_by(
                    _assay_plates_query.c.activity_id,
                    _l.c.short_name, _c.c.name, _p.c.plate_number ))
            _extant_query = _extant_query.cte('extant')
            
            # 2. query for searched plates
            logger.info('searched_plate_ids: %r', searched_plate_ids)
            j = _p
            j = j.join(_c, _c.c.copy_id==_p.c.copy_id)
            j = j.join(_l, _c.c.library_id==_l.c.library_id)
            j = j.join(_pl, _p.c.plate_location_id==_pl.c.plate_location_id,
                isouter=True)
            _search_query = ( 
                select([
                    literal_column('0').label('activity_id'),
                    _concat(
                        _l.c.short_name, '/', _c.c.name, '/', 
                        cast(_p.c.plate_number, sqlalchemy.sql.sqltypes.Text)
                    ).label('plate_key'),
                    _l.c.short_name.label('library_short_name'),
                    _l.c.screening_status.label('library_screening_status'),
                    _l.c.screen_type.label('library_screen_type'),
                    _c.c.name.label('copy_name'),
                    _c.c.comments.label('copy_comments'),
                    _concat_with_sep(
                        args=[_pl.c.room,_pl.c.freezer,_pl.c.shelf,_pl.c.bin],
                        sep='-').label('plate_location'),
                    _c.c.usage_type,
                    _p.c.plate_number,
                    _p.c.cplt_screening_count,
                    # TODO: consider using the cumulative_freeze_thaw_count
                    _p.c.screening_count,
                    _p.c.remaining_well_volume,
                    _p.c.status,
                ])
                .select_from(j)
                .where(_p.c.plate_id.in_(searched_plate_ids))
                .order_by(_l.c.short_name, _c.c.name, _p.c.plate_number ))
            
            if extant_plate_numbers:
                _search_query = _search_query.cte('search_query')
                _combined_query = union(
                    select([literal_column(x) for x in fields])
                        .select_from(_extant_query),
                    select([literal_column(x) for x in fields])
                        .select_from(_search_query)
                    )
                _combined_query = _combined_query.order_by(
                    nullslast(desc(column('activity_id'))),
                    'library_short_name', 'copy_name','plate_number')    
            else:
                _combined_query = _search_query

            # compiled_stmt = str(_combined_query.compile(
            #     dialect=postgresql.dialect(),
            #     compile_kwargs={"literal_binds": True}))
            # logger.info('compiled_stmt %s', compiled_stmt)
                    
            _result = conn.execute(_combined_query)
            
            logger.info('executed, build table...')
            # Convert the query into plate-ranges
            _data = []
            librarycopy = None
            start_plate = 0
            end_plate = 0
            current_lc = None
            current_ls = None

            new_row = OrderedDict()
            errorDict = ErrorDict()
            plate_locations = set()
            plate_keys = set()
            plate_volumes = []
            for _row in cursor_generator(_result,fields):
                logger.debug('row: %r', _row)
                librarycopy = '{library_short_name}/{copy_name}'.format(**_row)
                plate_number = _row['plate_number']
                if ( librarycopy != current_lc 
                        or current_ls != _row['activity_id']
                        or end_plate < _row['plate_number']-1):
                    if new_row:
                        new_row['plate_locations'] = [x for x in plate_locations]
                        new_row['plate_keys'] = [x for x in plate_keys]
                        sum_vol = sum([Decimal(x) for x in plate_volumes])/Decimal(len(plate_volumes))
                        sum_vol = u'{} {}L'.format(
                                si_unit.convert_decimal(
                                    sum_vol,1e-6, 2),
                                si_unit.get_siunit_symbol(1e-6))
                        new_row['avg_plate_volume'] = sum_vol
                        new_row['end_plate'] = end_plate
                        new_row['errors'] = errorDict.showErrors()
                        new_row['warnings'] = errorDict.showWarnings()
                        _data.append(new_row)
                    errorDict = ErrorDict()
                    plate_locations = set()
                    plate_keys = set()
                    plate_volumes = []
                    new_row = OrderedDict((
                        ('library_screening_id', _row['activity_id']),
                        ('library_short_name', _row['library_short_name']),
                        ('library_screening_status', _row['library_screening_status']),
                        ('copy_name', _row['copy_name']),
                        ('copy_comments', _row['copy_comments']),
                        ('start_plate', _row['plate_number']),
                        ))
                    current_lc = librarycopy
                    current_ls = _row['activity_id']
                end_plate = _row['plate_number']
                errorDict.check_row(_row)
                plate_locations.add(_row['plate_location'])
                plate_keys.add(_row['plate_key'])
                if _row['remaining_well_volume']:
                    plate_volumes.append(_row['remaining_well_volume'])
                else:
                    plate_volumes.append(0)
            if librarycopy:
                new_row['plate_locations'] = [x for x in plate_locations]
                new_row['plate_keys'] = [x for x in plate_keys]
                sum_vol = sum([Decimal(x) for x in plate_volumes])/Decimal(len(plate_volumes))
                sum_vol = u'{} {}L'.format(
                        si_unit.convert_decimal(
                            sum_vol,1e-6, 2),
                        si_unit.get_siunit_symbol(1e-6))
                new_row['avg_plate_volume'] = sum_vol
                new_row['end_plate'] = end_plate
                new_row['errors'] = errorDict.showErrors()
                new_row['warnings'] = errorDict.showWarnings()
                _data.append(new_row)
        
        library_comments = self.get_library_comments(
            { _dict['library_short_name'] for _dict in _data})
        
        for _dict in _data:
            _dict['library_comment_array'] = \
                library_comments.get(_dict['library_short_name'],None)

        self.get_plate_comments_for_plate_range_data(_data, _combined_query)
        
        return _data

    def get_plate_comments_for_plate_range_data(self, _data, join_query):
        
        logger.info('get plate comments...')
        for _dict in _data:
            _dict['plate_comment_array'] = set()
        _comment_apilogs = \
            ApiLogResource.get_resource_comment_subquery('librarycopyplate')
        _apilogs = self.bridge['reports_apilog']
        _join_query = join_query.cte('join_query')
        _comment_apilogs = \
            _comment_apilogs.where(_apilogs.c.key.in_(
                select([_join_query.c.plate_key])
                .select_from(_join_query)
            ))
        _comment_apilogs = _comment_apilogs.cte('logs')
        query = (
            select([
                _comment_apilogs.c.key,
                func.array_agg(
                    _concat(                            
                        cast(_comment_apilogs.c.name,
                            sqlalchemy.sql.sqltypes.Text),
                        LIST_DELIMITER_SUB_ARRAY,
                        cast(_comment_apilogs.c.date_time,
                            sqlalchemy.sql.sqltypes.Text),
                        LIST_DELIMITER_SUB_ARRAY,
                        '(',_comment_apilogs.c.key, ') ',
                        _comment_apilogs.c.comment)
                    )
            ])
            .select_from(_comment_apilogs)
            .group_by(_comment_apilogs.c.key)
        )
        
        with get_engine().connect() as conn:
            for x in conn.execute(query):
                key = x[0]
                comment_array = x[1]
                for _dict in _data:
                    if key in _dict['plate_keys']:
                        _dict['plate_comment_array'].update(comment_array)
        for _dict in _data:
            _dict['plate_comment_array'] = list(_dict['plate_comment_array'])            

        logger.info('plate comments generated')

    def get_plate_comments_for_plate_range_data_bak(self, _data):
        
        logger.info('get plate comments...')
        cumulative_plate_keys = set()
        for _dict in _data:
            _dict['plate_comment_array'] = set()
            cumulative_plate_keys.update(_dict['plate_keys'])
        logger.info('cumulative_plate_keys: %r', cumulative_plate_keys)
        _comment_apilogs = \
            ApiLogResource.get_resource_comment_subquery('librarycopyplate').cte('logs')
        query = (
            select([
                _comment_apilogs.c.key,
                func.array_agg(
                    _concat(                            
                        cast(_comment_apilogs.c.name,
                            sqlalchemy.sql.sqltypes.Text),
                        LIST_DELIMITER_SUB_ARRAY,
                        cast(_comment_apilogs.c.date_time,
                            sqlalchemy.sql.sqltypes.Text),
                        LIST_DELIMITER_SUB_ARRAY,
                        '(',_comment_apilogs.c.key, ') ',
                        _comment_apilogs.c.comment)
                    )
            ])
            .select_from(_comment_apilogs)
            .group_by(_comment_apilogs.c.key)
            .where(_comment_apilogs.c.key.in_(cumulative_plate_keys))
        )
        
        with get_engine().connect() as conn:
            for x in conn.execute(query):
                key = x[0]
                comment_array = x[1]
                for _dict in _data:
                    if key in _dict['plate_keys']:
                        _dict['plate_comment_array'].update(comment_array)
        for _dict in _data:
            _dict['plate_comment_array'] = list(_dict['plate_comment_array'])            

        logger.info('plate comments generated')

    def get_library_comments(self, library_keys):

        logger.info('get library comments...')

        _comment_apilogs = \
            ApiLogResource.get_resource_comment_subquery('library').cte('logs')
        query = (
            select([
                _comment_apilogs.c.key,
                func.array_agg(
                    _concat(                            
                        cast(_comment_apilogs.c.name,
                            sqlalchemy.sql.sqltypes.Text),
                        LIST_DELIMITER_SUB_ARRAY,
                        cast(_comment_apilogs.c.date_time,
                            sqlalchemy.sql.sqltypes.Text),
                        LIST_DELIMITER_SUB_ARRAY,
                        _comment_apilogs.c.comment)
                    )
            ])
            .select_from(_comment_apilogs)
            .group_by(_comment_apilogs.c.key)
            .where(_comment_apilogs.c.key.in_(library_keys))
        )
        
        with get_engine().connect() as conn:
            comments = defaultdict(list)
            for x in conn.execute(query):
                comments[x[0]] = x[1]
            logger.info('library comments generated')
            return comments

    def get_query(self, schema, param_hash, user):
        '''  
        LibraryScreeningResource
        - NOTE: overrides the ActivityResource get_query:
        - TODO: LibraryScreeningResource does not need to extend ActivityResource
        
        special use case: 
        - use "library_plates_screened__contains" param to query for 
        LibraryScreenings that have screened a particular set of plate-copies
        '''
        logger.debug('libraryscreening get_query, schema: %r, %r', 
            schema['key'], schema['fields'].keys())

        manual_field_includes = set(param_hash.get('includes', []))
        manual_field_includes.add('serviced_user_id') # req'd for auth
        
        library_plates_screened_search = param_hash.pop(
            'library_plates_screened__contains', None)
        if library_plates_screened_search:
            manual_field_includes.add('library_plates_screened')
            
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filter_expression = \
            self._meta.authorization.filter(user,filter_expression)

        order_params = param_hash.get('order_by', [])
        order_params.append('-date_of_activity')
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
         
        # specific setup
        (join_clause, custom_columns) = \
            super(LibraryScreeningResource, self).get_join_and_custom_columns()
            
        _a = self.bridge['activity']
        _la = self.bridge['lab_activity']
        _screening = self.bridge['screening']
        _library_screening = self.bridge['library_screening']
        _ap = self.bridge['assay_plate']
        _c = self.bridge['copy']
        _p = self.bridge['plate']
        join_clause = join_clause.join(
            _la, _a.c.activity_id == _la.c.activity_id)    
        join_clause = join_clause.join(
            _screening, _screening.c.activity_id == _la.c.activity_id)    
        join_clause = join_clause.join(
            _library_screening,
            _library_screening.c.activity_id == _la.c.activity_id)
            
        custom_columns.update({
            'libraries_screened_count': literal_column(
                '(select count(distinct(l.*)) from library l '
                'join copy using(library_id) join plate using(copy_id) '
                'join assay_plate ap using(plate_id) '
                'where library_screening_id='
                'library_screening.activity_id)' ),
             'library_plates_screened_count': literal_column(
                 '(select count(distinct(ap.plate_id)) '
                 'from assay_plate ap where library_screening_id='
                 'library_screening.activity_id)' ),
            })

        base_query_tables = [
            'activity', 'lab_activity', 'screening', 'library_screening',
            'screen'] 
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)
        
        stmt = select(columns.values()).select_from(join_clause)

        extra_params = {}
        if library_plates_screened_search:
            extra_params['plates_screened'] = None
            copy_plate_query = (
                select([
                    _p.c.plate_id,_p.c.plate_number, _c.c.name, 
                    _ap.c.library_screening_id])
                .select_from(_p.join(_c,_p.c.copy_id==_c.c.copy_id)
                    .join(_ap,_p.c.plate_id==_ap.c.plate_id)))
            
            logger.info('try to grep library_plates_screened_search: %r', 
                library_plates_screened_search)
            if isinstance(library_plates_screened_search, six.types.StringTypes):
                # FIXME: lines should be split using the PLATE_SEARCH_LINE_SPLITTING_PATTERN
                library_plates_screened_search = \
                    re.split(r'[,;]+', library_plates_screened_search)
            if not isinstance(library_plates_screened_search, (list,tuple)):
                library_plates_screened_search = (library_plates_screened_search,)
            or_clause = []
            for lps_search in library_plates_screened_search:
                logger.info('try: %r', lps_search)
                copy_plate_pattern = re.compile(r'([^\/]+)\/(\d+)')
                match = copy_plate_pattern.match(lps_search)
                if match: 
                    logger.info('found match %r for %r', match, lps_search)
                    or_clause.append(and_(
                        _c.c.name==match.group(1),_p.c.plate_number==match.group(2)))
            if or_clause:
                logger.info('library_copy_plate pattern subquery')
                copy_plate_query = copy_plate_query.where(or_(*or_clause))
                copy_plate_query = copy_plate_query.alias('cp_query')
                stmt = stmt.where(exists(
                    select([None]).select_from(copy_plate_query)
                        .where(copy_plate_query.c.library_screening_id
                            ==_library_screening.c.activity_id) ))
        
        if logger.isEnabledFor(logging.DEBUG):
            compiled_stmt = str(stmt.compile(
                dialect=postgresql.dialect(),
                compile_kwargs={"literal_binds": True}))
            logger.info('compiled_stmt %s', compiled_stmt)
        # general setup
         
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        stmt = stmt.order_by('activity_id')

        filename = self._get_filename(
            readable_filter_hash, schema, **extra_params)
        
        return (field_hash, columns, stmt, count_stmt,filename)
        
    @read_authorization
    def build_list_response(self, request, **kwargs):
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
         
        is_for_detail = kwargs.pop('is_for_detail', False)
             
        (field_hash, columns, stmt, count_stmt,filename) = \
            self.get_query(schema, param_hash, request.user)
         
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            # use "use_vocab" as a proxy to also adjust siunits for viewing
            rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
                field_hash, rowproxy_generator)
        
        # create a generator to wrap the cursor and expand the library_plates_screened
        def create_lcp_gen(generator):
            bridge = self.bridge
            _library = self.bridge['library']
            _lcp = self.bridge['plate']
            _cp = self.bridge['copy']
            _ap = self.bridge['assay_plate']
            lcp_query = (
                select([ 
                    _library.c.short_name,
                    _cp.c.name,
                    _ap.c.plate_number,
                 ])
                .select_from(
                    _ap.join(_lcp, _ap.c.plate_id == _lcp.c.plate_id)
                        .join(_cp, _cp.c.copy_id == _lcp.c.copy_id)
                        .join(
                            _library,
                            _library.c.library_id == _cp.c.library_id))
                .where(_ap.c.library_screening_id == text(':activity_id'))
                .group_by(
                    _library.c.short_name, _cp.c.name, _ap.c.plate_number)
                .order_by(
                    _library.c.short_name, _cp.c.name, _ap.c.plate_number))
            logger.debug('lcp_query: %r', str(lcp_query.compile()))
            
            def library_copy_plates_screened_generator(cursor):
                if generator:
                    cursor = generator(cursor)
                class Row:
                    def __init__(self, row):
                        self.row = row
                        self.entries = []
                        activity_id = row['activity_id']
                        query = conn.execute(
                            lcp_query, activity_id=activity_id)
                        copy = None
                        start_plate = None
                        end_plate = None
                        for x in query:
                            if not copy:
                                copy = x[1]
                                library = x[0]
                            if not start_plate:
                                start_plate = end_plate = x[2]
                            if (x[0] != library 
                                or x[1] != copy 
                                or x[2] > end_plate + 1):
                                # start a new range, save old range
                                self.entries.append('%s:%s:%s-%s'
                                    % (library, copy, start_plate, end_plate))
                                start_plate = end_plate = x[2]
                                copy = x[1]
                                library = x[0]
                            else:
                                end_plate = x[2]
                        if copy: 
                            self.entries.append('%s:%s:%s-%s'
                                % (library, copy, start_plate, end_plate))
                                
                    def has_key(self, key):
                        if key == 'library_plates_screened': 
                            return True
                        return self.row.has_key(key)
                    def keys(self):
                        return self.row.keys()
                    def __getitem__(self, key):
                        if key == 'library_plates_screened':
                            return self.entries
                        else:
                            return self.row[key]
                conn = get_engine().connect()
                try:
                    for row in cursor:
                        yield Row(row)
                finally:
                    conn.close()
                    
            return library_copy_plates_screened_generator
        
        if 'library_plates_screened' in field_hash:
            rowproxy_generator = create_lcp_gen(rowproxy_generator)

        rowproxy_generator = \
           self._meta.authorization.get_row_property_generator(
               request.user, field_hash, rowproxy_generator)
                
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
         
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None),
            use_caching=True)
              

    def put_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_detail')
    
    def validate(self, _dict, patch=False, schema=None):

        errors = ActivityResource.validate(self, _dict, schema=schema, patch=patch)
        if _dict.get('library_plates_screened', None):
            if bool(_dict.get('is_for_external_library_plates', False)):
                errors['library_plates_screened'] = (
                    'Can not specifiy library plates if '
                    '"is_for_external_library_plates"')

        return errors


    def build_patch_detail(self, request, deserialized, log=None, **kwargs):
        '''
        TODO: refactor patch/post detail to remove common operations
        '''

        if not deserialized:
            raise Http404('no data sent')
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        kwargs_for_log = self.get_id(deserialized,schema=schema, validate=True,**kwargs)

        # NOTE: 20170321 not creating a screen "parent" log;
        # libraryScreening activities will stand on their own
        # original_screen_data = self.get_screen_resource()._get_detail_response_internal(**{
        #     'facility_id': original_data['screen_facility_id']})

        original_data = self._get_detail_response_internal(**kwargs_for_log)
        logger.debug('original libraryscreening data: %r', original_data)
        
        # FIXME: log needed so patch_obj can set assay plate logs, but this should be done here
        log = self.make_log(request)
        patch_result = self.patch_obj(request, deserialized, log=log, **kwargs)
        obj = patch_result[API_RESULT_OBJ]
        plate_meta = patch_result[API_RESULT_META]
        
        # NOTE: make the log uri more robust, with Screen id as well
        log.uri = '/'.join([
            log.ref_resource_name,'screen', obj.screen.facility_id, log.key])
        log.save()
        
        new_data = self._get_detail_response_internal(**kwargs_for_log)
        kwargs_for_log[HTTP_PARAM_USE_VOCAB] = True
        new_data_display = self._get_detail_response_internal(**kwargs_for_log)
        self.log_patch(request, original_data,new_data,log=log, schema=schema, **kwargs)
        log.save()
        meta = { 
            API_MSG_SCREENING_TOTAL_PLATE_COUNT: 
                new_data_display['library_plates_screened_count'],
            'Volume per well transferred from Plates (nL)': 
                new_data_display['volume_transferred_per_well_from_library_plates'],
        }
        meta.update(plate_meta)
        _data = { API_RESULT_DATA: [new_data] }
        _data[API_RESULT_META] = { SCHEMA.API_MSG_RESULT: meta }
        
        return _data

    def build_post_detail(self, request, deserialized, log=None, **kwargs):
        '''
        TODO: refactor patch/post detail to remove common operations
        '''
        
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        kwargs_for_log = self.get_id(
            deserialized,schema=schema, validate=False,**kwargs)
        
        logger.info('post detail: %r', kwargs_for_log)

        # NOTE: 20170321 not creating a screen "parent" log;
        # libraryScreening activities will stand on their own
        # original_screen_data = self.get_screen_resource()._get_detail_response_internal(**{
        #     'facility_id': original_data['screen_facility_id']})

        original_data = None
        if kwargs_for_log:
            # A full id exists, query for the existing state
            try:
                original_data = self._get_detail_response_internal(**kwargs_for_log)
            except Exception, e: 
                logger.exception('exception when querying for existing obj: %s', 
                    kwargs_for_log)
            
            if original_data is not None and len(original_data) != 0:
                raise ValidationError({ 
                    k: '%r Already exists' % v for k,v in kwargs_for_log.items() })
            original_data = None
        
        # FIXME: log needed so patch_obj can set assay plate logs, but this should be done here
        log = self.make_log(request)
        patch_result = self.patch_obj(request, deserialized, log=log, **kwargs)
        obj = patch_result[API_RESULT_OBJ]
        plate_meta = patch_result[API_RESULT_META]
        logger.info('patch meta: %r', plate_meta)

        id_attribute = schema['id_attribute']
        for id_field in id_attribute:
            val = getattr(obj, id_field,None)
            if val:
                kwargs_for_log['%s' % id_field] = val

        # NOTE: make the log uri more robust, with Screen id as well
        log.uri = '/'.join([
            log.ref_resource_name,'screen', obj.screen.facility_id, log.key])
        log.save()
        
        obj.apilog_uri = log.log_uri
        obj.save()

        # get new state, for logging
        new_data = self._get_detail_response_internal(**kwargs_for_log)
        if not new_data:
            raise InformationError(
                key='method', 
                msg='no data found for the new obj created by post: %r' % obj)
        self.log_patch(
            request, original_data,new_data,log=log, schema=schema,
            full_create_log=True, **kwargs)
        log.save()

        kwargs_for_log[HTTP_PARAM_USE_VOCAB] = True
        new_data_display = self._get_detail_response_internal(**kwargs_for_log)
        
        meta = { 
            API_MSG_SCREENING_TOTAL_PLATE_COUNT: 
                new_data_display['library_plates_screened_count'],
            'Volume per well transferred from Plates (nL)': 
                new_data_display['volume_transferred_per_well_from_library_plates'],
        }
        meta.update(plate_meta)
        _data = { API_RESULT_DATA: [new_data] }
        _data[API_RESULT_META] = { SCHEMA.API_MSG_RESULT: meta }
        
        return _data

    @write_authorization
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):

        logger.info('patch library screening: %r', deserialized)
        schema = kwargs.pop('schema', None)
        if schema is None:
            raise Exception('schema not initialized')
        fields = schema['fields']
        initializer_dict = {}
        ls_log = kwargs.get('log', None)
        if ls_log is None:
            raise ProgrammingError(
                'library screening log should be created by the callee of patch_obj')

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        logger.debug('param_hash: %r', param_hash)

        # FIXME: parse and validate only editable fields

        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
        patch = bool(id_kwargs)
        initializer_dict = self.parse(deserialized, create=not patch, schema=schema)
        errors = self.validate(initializer_dict, schema=schema, patch=patch)
        if errors:
            raise ValidationError(errors)
        
        if not patch:
            _key = 'screen_facility_id'
            _val = deserialized[_key]
            try:
                screen = Screen.objects.get(facility_id=_val)
                initializer_dict['screen'] = screen
            except ObjectDoesNotExist:
                raise ValidationError(
                    key=_key,
                    msg='does not exist: {val}'.format(val=_val))

        _key = 'performed_by_user_id'
        _val = deserialized.get(_key, None)
        if _val:
            try:
                performed_by_user = ScreensaverUser.objects.get(screensaver_user_id=_val)
                initializer_dict['performed_by'] = performed_by_user
            except ObjectDoesNotExist:
                raise ValidationError(
                    key=_key,
                    msg='does not exist: {val}'.format(val=_val))
        library_screening = None
        if patch:
            try:
                logger.info('%r', id_kwargs)
                library_screening = LibraryScreening.objects.get(
                    pk=id_kwargs['activity_id'])
                current_volume_transferred_per_well = \
                    library_screening.volume_transferred_per_well_from_library_plates
                new_volume_xfer = initializer_dict.get(
                    'volume_transferred_per_well_from_library_plates',None) 
                if ( new_volume_xfer is not None and 
                    current_volume_transferred_per_well
                        != new_volume_xfer ):
                    if library_screening.assayplate_set.exists():
                        raise ValidationError({
                            'volume_transferred_per_well_from_library_plates':
                                ( 
                                'Can not be changed if plates have been assigned; '
                                '(%r to %r' 
                                % (current_volume_transferred_per_well,new_volume_xfer)),
                            'library_plates_screened': (
                                'Remove all plates before changing volume assigned')
                            })
            except ObjectDoesNotExist:
                raise Http404(
                    'library_screening does not exist for: %r', id_kwargs)
        else:
            # Set the created_by field:
            # NOTE: after migration LibraryScreening will no longer have 
            # this field
            try:
                adminuser = ScreensaverUser.objects.get(username=request.user.username)
            except ObjectDoesNotExist as e:
                logger.error('admin user: %r does not exist', request.user.username )
                raise
            
            library_screening = LibraryScreening()
            library_screening.created_by = adminuser
        
        # TODO: volume sanity check and warning:
        # - if volume_transferred_per_well_from_library_plates is set:
        #   warn if not equal to: 
        #   volume_transferred_per_well_to_assay_plates * number_of_replicates
        
        for key, val in initializer_dict.items():
            setattr(library_screening, key, val)

        library_screening.classification = SCHEMA.VOCAB.activity.classification.SCREENING
        
        if library_screening.is_for_external_library_plates:
            library_screening.type = SCHEMA.VOCAB.activity.type.EXT_LIBRARY_SCREENING
        else:
            library_screening.type = SCHEMA.VOCAB.activity.type.LIBRARY_SCREENING
            
        library_screening.save()
        
        plate_meta = {}
        if library_screening.is_for_external_library_plates is not True:
            library_plates_screened = deserialized.get(
                'library_plates_screened', None)
            if not library_plates_screened and not patch:
                raise ValidationError(
                    key='library_plates_screened', 
                    msg='required')
                
            # override_param = parse_val(
            #     param_hash.get(API_PARAM_OVERRIDE, False),
            #         API_PARAM_OVERRIDE, 'boolean')
            # override_vol_param = parse_val(
            #     param_hash.get(API_PARAM_VOLUME_OVERRIDE, False),
            #         API_PARAM_VOLUME_OVERRIDE, 'boolean')
            
            if library_plates_screened is not None:
                logger.debug('save library screening, set assay plates: %r', 
                             library_screening)
                # FIXME: make_log_key should accept obj
                ls_log.key = str(library_screening.activity_id)
                # NOTE: make the log uri more robust, with library key as well
                ls_log.uri = '/'.join(map(str,[
                    ls_log.ref_resource_name,'screen', 
                    library_screening.screen.facility_id, ls_log.key]))
                ls_log.save()

                plate_meta = self._set_assay_plates(
                    request, schema, 
                    library_screening, library_plates_screened, ls_log) 
                logger.info('save library screening, assay plates set: %r', 
                            library_screening)
            
                ls_log.json_field = json.dumps(plate_meta)
                logger.info('parent_log: %r', ls_log)
            self.create_screen_screening_statistics(library_screening.screen)
            self.create_screened_experimental_well_count(library_screening)
        
        return { API_RESULT_OBJ: library_screening, API_RESULT_META: plate_meta}
        
    def create_screened_experimental_well_count(self, library_screening):
        # TODO: this should be dynamic
        with get_engine().connect() as conn:
            sql = (
                'select count(*) '
                'from well '
                'where plate_number in ( '
                '    select distinct(plate_number) from assay_plate ' 
                '    where library_screening_id = %s ) '
                "and library_well_type='experimental'")
            library_screening.screened_experimental_well_count = int(
                conn.execute(
                    sql, (library_screening.activity_id))
                .scalar() or 0)
            library_screening.save()

    def create_screen_screening_statistics(self, screen):
        
        # TODO: these statistics must be updated when the library definitions are
        # reloaded, see LibrariesDAO in SS1 and LibraryContentsLoader
        # NOTE: Logic here should mirror WellResource.update_screening_stats()
        
        with get_engine().connect() as conn:
            # NOTE: mixing Django connection with SQA connection
            # - thrown exceptions will rollback the nested SQA transaction
            # see: http://docs.sqlalchemy.org/en/latest/core/connections.html
            screen_facility_id = screen.facility_id
            sql = (
                'select count(w.well_id) '
                'from Well w, Screen s  '
                'join assay_plate ap using(screen_id) '
                'join plate p using(plate_id) '
                'join library_screening ls on(ap.library_screening_id=ls.activity_id) '
                'where   ap.replicate_ordinal = 0  '
                'and w.plate_number = p.plate_number ' 
                'and w.library_well_type = %s'
                'and s.facility_id = %s;')
            screen.screened_experimental_well_count = int(
                conn.execute(
                    sql, (WELL_TYPE.EXPERIMENTAL, screen_facility_id))
                .scalar() or 0)
            sql = (
                'select count(distinct(w.well_id)) '
                'from Well w, Screen s  '
                'join assay_plate ap using(screen_id) '
                'join plate p using(plate_id) '
                'join library_screening ls on(ap.library_screening_id=ls.activity_id) '
                'where   ap.replicate_ordinal = 0  '
                'and w.plate_number = p.plate_number ' 
                'and w.library_well_type = %s'
                'and s.facility_id = %s;')
            screen.unique_screened_experimental_well_count = int(
                conn.execute(
                    sql, (WELL_TYPE.EXPERIMENTAL, screen_facility_id))
                .scalar() or 0)
            logger.info('screen_screening_statistics: %d, %d',
                screen.screened_experimental_well_count, 
                screen.unique_screened_experimental_well_count)
            screen.save()
    
    @transaction.atomic    
    def _set_assay_plates(
            self, request, schema, library_screening, library_plates_screened,
            ls_log):
        '''
        - Create new assay plates
        - Adjust librarycopyplate volume
        - Adjust librarycopyplate screening count
        - Create librarycopyplate logs
        - TODO: create copy logs
        '''
        logger.debug('set assay plates screened for: %r, %r', 
            library_screening.activity_id, library_plates_screened)
        
        # Parse library_plate_ranges
        # E.G. Regex: /(([^:]+):)?(\w+):(\d+)-(\d+)/
        regex_string = schema['fields']['library_plates_screened']['regex']
        matcher = re.compile(regex_string)
        def show_plates(plates):
            return ['%s/%d' % (plate.copy.name, plate.plate_number) 
                for plate in plates]
        
        min_plate_volume_after_transfer = 0
        if library_screening.screen.screen_type == SCREEN_TYPE.RNAI:
            min_plate_volume_after_transfer = \
                self.MIN_WELL_VOL_RNAI
        elif library_screening.screen.screen_type == SCREEN_TYPE.SMALL_MOLECULE:
            min_plate_volume_after_transfer = \
                self.MIN_WELL_VOL_SMALL_MOLECULE
            
        # 1. Validate and parse the library_plates_screened input patterns
        new_library_plates_screened = []
        for lps in library_plates_screened:
            match = matcher.match(lps)
            if not match:
                raise ValidationError(
                    key='library_plates_screened',
                    msg=('%r does not match pattern: %s' 
                        % (lps, regex_string)))
                break
            else:
                start_plate = int(match.group(4))
                if match.group(6):
                    end_plate = int(match.group(6))
                else:
                    end_plate = start_plate
                new_library_plates_screened.append({
                    'library_short_name': match.group(2),
                    'copy_name': match.group(3),
                    'start_plate': start_plate,
                    'end_plate': end_plate,
                     })
        library_plates_screened = new_library_plates_screened
        
        # 2. Validate and find all the plates referenced in the ranges
        logger.info(
            'get the referenced plates for: %r', library_plates_screened)
        plate_ranges = []
        plate_keys = set()
        all_plate_ids = set() # extant ap's and new
        for _data in library_plates_screened:
            logger.debug('lps data: %r', _data)
            try:
                copy_name = _data['copy_name']
                library_short_name = _data['library_short_name']
                copy = Copy.objects.get(
                    name=copy_name,
                    library__short_name=library_short_name)
            except ObjectDoesNotExist:
                raise ValidationError(
                    key='library_plates_screened',
                    msg='{library_short_name}/{copy} does not exist: {val}'.format(
                        library_short_name=library_short_name,
                        copy=copy_name,
                        val=str(_data)))
            try:
                start_plate = Plate.objects.get(
                    copy=copy,
                    plate_number=_data['start_plate'])
                end_plate = Plate.objects.get(
                    copy=copy,
                    plate_number=_data['end_plate'])
                logger.debug(
                    'found start: %r, end: %r plates', start_plate, end_plate)
                if start_plate.copy.library != end_plate.copy.library:
                    raise ValidationError(
                        key='library_plates_screened',
                        msg=('plate range must be for a single library: '
                             '{start_plate}-{end_plate}').format(**_data))
                if (start_plate.copy.library.screen_type 
                        != library_screening.screen.screen_type):
                    raise ValidationError(
                        key='library_plates_screened',
                        msg=('library.screen_type!=screen.screen_type: '
                             '{start_plate}-{end_plate} (%s)' 
                                % start_plate.copy.library.screen_type
                             ).format(**_data))
                plate_range = range(
                    start_plate.plate_number, end_plate.plate_number + 1)

                # REMOVED: 20170412 - per JAS/KR; allow multiple copies, same plate
                # to be screened in one screening
                # if plate_numbers & set(plate_range):
                #     raise ValidationError(
                #         key='library_plates_screened',
                #         msg=('A plate number can only be screened once per '
                #             'Library Screening: {start_plate}-{end_plate}'
                #             ).format(**_data))
                plate_keys.update([ '%s/%d' % (copy.name, plate_number) 
                    for plate_number in plate_range])
                logger.debug('find the plate range: %s: %s-%s', 
                    copy.name, start_plate.plate_number, end_plate.plate_number)
                plate_range = Plate.objects.all().filter(
                    copy=copy,
                    plate_number__range=(start_plate.plate_number, end_plate.plate_number))
                plate_ranges.append(plate_range)
                all_plate_ids.update([x.plate_id for x in plate_range.all()])

            except ObjectDoesNotExist:
                logger.exception('plate range error')
                raise ValidationError({
                    'library_plates_screened':
                    ('plate range not found: {start_plate}-{end_plate}'
                        ).format(**_data),
                    'copy_name': _data['copy_name'] })
        logger.debug('plate keys: %r',plate_keys)
        
        # 3. Logging
        logger.debug('3. Cache current state for logging...')
        # Create a search criteria to poll the current plate state
        # TODO: cache and log the copy state as well
        _original_plate_data = []
        if library_screening.assayplate_set.exists():
            for ap in library_screening.assayplate_set.all():
                all_plate_ids.add(ap.plate.plate_id)

        logger.debug('Cache plate data...')
        if all_plate_ids:
            _original_plate_data = \
                self.get_plate_resource()._get_list_response_internal(
                plate_ids=all_plate_ids,
                visibilities = ['l','d'],
                includes =['-library_comment_array', '-comment_array'])
        
        if not _original_plate_data:
            raise Exception('no original plate data found')        
        logger.debug('Find extant assay plates that are kept, '
            'find and remove deleted assay plates...')
        extant_plates = set()
        deleted_plates = set()
        if library_screening.assayplate_set.exists():
            for ap in library_screening.assayplate_set.all():
                plate_key = '%s/%d' % (ap.plate.copy.name, ap.plate.plate_number)
                if plate_key in plate_keys:
                    extant_plates.add(ap.plate)
                else:
                    # 20161020: no longer tracking data_load actions for an 
                    # assay plate so this is removed:
                    # if ap.screen_result_data_loading:
                    #     raise ValidationError(
                    #         key='library_plates_screened',
                    #         msg=(
                    #             'Assay plate has data and cannot be removed: %d'
                    #             ) % ap.plate_number)
                    deleted_plates.add(ap.plate)
                    ap.delete()       
        logger.debug('deleted plates: %r', show_plates(deleted_plates)) 

        logger.debug('5. Create new assay plates...')
        created_plates = set()
        not_allowed_libraries = set()
        plate_errors = []
        plate_warnings = []
        for plate_range in plate_ranges:
            for plate in plate_range:
                if plate in created_plates:
                    plate_errors.append(
                        'plate: "%s/%s" overlaps an existing plate range'
                            % (plate.copy.name, plate.plate_number))    
                    continue
            
            for replicate in range(library_screening.number_of_replicates):
                for plate in plate_range:
                    if plate not in extant_plates:
                        # FIXME: 20170406 - allow other status types to be screened,
                        # show a warning if not "active"
                        if (plate.copy.library.screening_status 
                            in self.WARN_LIBRARY_SCREENING_STATUS):
                            not_allowed_libraries.add(plate.copy.library)
                        elif (plate.copy.library.screening_status 
                            not in self.ALLOWED_LIBRARY_SCREENING_STATUS):
                            plate_errors.append(
                                'plate: "%s/%s": Library status: "%s"' 
                                % (plate.copy.name, plate.plate_number,
                                    plate.library.screening_status))
                        if plate.status in self.WARN_PLATE_STATUS:
                            plate_warnings.append(
                                'plate: "%s/%s" status: "%s"'
                                    % (plate.copy.name,plate.plate_number,plate.status))
                        elif plate.status not in self.ALLOWED_PLATE_STATUS:
                            plate_errors.append(
                                'plate: "%s/%s" status: "%s"'
                                    % (plate.copy.name,plate.plate_number,plate.status))
                        if plate.copy.usage_type not in self.ALLOWED_COPY_USAGE_TYPE:
                            plate_errors.append(
                                'plate: "%s/%s": "%s"' 
                                % (plate.copy.name, plate.plate_number,
                                   plate.copy.usage_type))
                        # 20170407 allow libraryscreening even after 
                        #     # copywells have been created
                        # if plate.cplt_screening_count > 0:
                        #     # FIXME: 20170407 allow libraryscreening even after 
                        #     # copywells have been created
                        #     # implement
                        #     # CopyWellResource.allocateScreeningVolumes
                        #     
                        #     # FIXME:TODO: 20170412 - allow screening of 
                        #     # library_screning_plates after cherry pick volumes
                        #     # have been taken
                        #     
                        #     # Volume tracking will not work on the plate level
                        #     # after cherry pick volumes have been taken from the
                        #     # copy wells
                        #     plate_errors.append(
                        #         'plate: "%s/%s"; may not be screened after '
                        #         'cherry pick screenings (%d) have been performed'
                        #             % (plate.copy.name,plate.plate_number, 
                        #                plate.cplt_screening_count))
                        if plate_errors:
                            continue
                        # 2017041 - allow screening for "not_allowed" libraries
                        # if not_allowed_libraries and override_param is not True:
                        #    continue

                        ap = AssayPlate.objects.create(**{  
                            'plate': plate,
                            'plate_number': plate.plate_number,
                            'screen': library_screening.screen,
                            'library_screening': library_screening,
                            'replicate_ordinal': replicate
                            })
                        ap.save()
                        created_plates.add(plate)
                    else:
                        logger.debug('extant plate: %r', plate)
        if plate_errors:
            plate_errors = sorted(plate_errors)
            raise ValidationError(
                    key='library_plates_screened',
                    msg=sorted(plate_errors))

        if not_allowed_libraries:
            not_allowed_libraries = sorted([
                '%s - status: %s' % (l.short_name,l.screening_status)
                for l in not_allowed_libraries])
            # if override_param is not True:
            #     raise ValidationError({
            #         API_PARAM_OVERRIDE: 'required',
            #         'library_plates_screened': (
            #             'Override required to screen libraries that are '
            #             'not allowed'),
            #         'Libraries': not_allowed_libraries
            #         }
            #     )
        
        logger.debug('Update the plate screening related plates (and copywells)...')
        # TODO: deprecate volume change after plates are created.
        # if current_volume_tranferred_per_well != new_volume_transferred_per_well:
        #     
        #     for plate in extant_plates:
        #         plate_key = '%s/%d' % (plate.copy.name, plate.plate_number)
        #         logger.debug('plate: %r, remaining vol: %r, %r', 
        #             plate_key, plate.remaining_well_volume, 
        #             new_volume_transferred_per_well)
        #         current_remaining_well_volume = \
        #             plate.remaining_well_volume or Decimal(0)
        #         current_remaining_well_volume += current_volume_transferred_per_well
        #         new_remaining_well_volume = \
        #             current_remaining_well_volume - new_volume_transferred_per_well
        #         
        #         if new_remaining_well_volume < 0:
        #             # 20170407 - per JAS/KR,
        #             # raise an Error instead for insufficient vol
        #             logger.info('plate: %r, insufficient vol: %r', 
        #                 plate_key, new_remaining_well_volume)
        #             plates_insufficient_volume.append(
        #                 (plate_key, 
        #                     '(available: %s uL)' 
        #                         % si_unit.convert_decimal(
        #                             current_remaining_well_volume, 1e-6, 1),
        #                     '(requested: %s uL)' 
        #                         % si_unit.convert_decimal(
        #                             new_volume_transferred_per_well, 1e-6, 1)))
        # 
        #         plate.remaining_well_volume = new_remaining_well_volume
        #         plate.save()
        #         
        #         # FIXME/TODO: implement copywell vol adj
        #         # NOTE: usually, screening copies should not have copywells
        #         # TODO: implement this if screening policy is changed to allow
        #         cw_check_query = (
        #             plate.copywell_set.exclude(volume=F('initial_volume'))
        #                 .exclude(volume__isnull=True))
        #         if cw_check_query.exists():
        #             logger.info('copywells found: %r', [x for x in cw_check_query.all()])
        # 
        #             self.get_copywell_resource().allocate_library_screening_volumes(
        #                 )
        #             
        #             # raise ValidationError(
        #             #     key='library_plates_screened',
        #             #     msg=('Can not create a library screening if copy wells have '
        #             #     'been adjusted, plate: %r' % plate_key))
        
        logger.debug('Update and check created plates: %r' ,
            show_plates(created_plates))
        volume_to_transfer = library_screening.volume_transferred_per_well_from_library_plates
        if not volume_to_transfer:
            raise ValidationError(
                key='volume_transferred_per_well_from_library_plates',
                msg='required')
        plates_insufficient_volume = []
        plate_copywell_stats = {}
        plate_copywell_warnings = {}
        copywell_allocation_meta = {
            API_MSG_LCPS_INSUFFICIENT_VOLUME: plate_copywell_warnings,
            API_MSG_COPYWELLS_ALLOCATED: plate_copywell_stats
        }
        for plate in created_plates:
            plate_key = '%s/%d' % (plate.copy.name, plate.plate_number)
            plate.screening_count = (plate.screening_count or 0) + 1
            remaining_well_volume = plate.remaining_well_volume or Decimal(0)
            remaining_well_volume -= volume_to_transfer
            if remaining_well_volume < min_plate_volume_after_transfer:
                # 20170605 - JAS - allowed, but show warning
                logger.info('plate: %r, insufficient vol: %r', 
                    plate_key, remaining_well_volume)
                plates_insufficient_volume.append(
                    (plate_key, 
                        '(available: %s uL)' 
                            % si_unit.convert_decimal(
                                plate.remaining_well_volume, 1e-6, 1),
                        '(requested: %s nL)' 
                            % si_unit.convert_decimal(
                                volume_to_transfer, 1e-9, 1)))
            cw_check_query = (
                plate.copywell_set.exclude(volume=F('initial_volume'))
                    .exclude(volume__isnull=True))
            if cw_check_query.exists():
                logger.info('libraryscreening copywells to allocate found: %r', 
                    [x for x in cw_check_query.all()])
                # NOTE: parent_log is set to the library_screening_log:
                # (the log tree is flattened to one level)
                meta = self.get_copywell_resource()._allocate_well_volumes(
                    volume_to_transfer, cw_check_query.all(), ls_log)
                if API_MSG_LCPS_INSUFFICIENT_VOLUME in meta:
                    plate_copywell_warnings[plate_key] = meta[API_MSG_LCPS_INSUFFICIENT_VOLUME]
                plate_copywell_stats[plate_key] = meta[API_MSG_COPYWELLS_ALLOCATED] 
            # if cw_check_query.exists():
            #     logger.info('copywells found: %r', [x for x in cw_check_query.all()])
            #     raise ValidationError(
            #         key='library_plates_screened',
            #         msg=('Can not create a library screening if copy wells have '
            #         'been adjusted, plate: %r' % plate_key))
            plate.remaining_well_volume = remaining_well_volume
            plate.save()
                        
        plate_copywell_deallocate_stats = {}
        for plate in deleted_plates:
            plate_key = '%s/%d' % (plate.copy.name, plate.plate_number)
            plate.screening_count -= 1
            remaining_well_volume = plate.remaining_well_volume or Decimal(0)
            remaining_well_volume += volume_to_transfer
            plate.remaining_well_volume = remaining_well_volume
            cw_check_query = (
                plate.copywell_set.exclude(volume=F('initial_volume'))
                    .exclude(volume__isnull=True))
            if cw_check_query.exists():
                logger.info('libraryscreening copywells to deallocate found: %r', 
                    [x for x in cw_check_query.all()])
                # NOTE: parent_log is set to the library_screening_log:
                # (the log tree is flattened to one level)
                meta = self.get_copywell_resource()\
                    ._deallocate_well_volumes(
                        volume_to_transfer, cw_check_query.all(), ls_log)
                plate_copywell_deallocate_stats[plate_key] = \
                    meta[API_MSG_COPYWELLS_DEALLOCATED]

            plate.save()
        # Volume check
        if plates_insufficient_volume:
            # Modified: 20170407 - per JAS/KR,
            # raise an Error instead for insufficient vol
            # Modified 20170605 - overdraw volume but warn
            plates_insufficient_volume = sorted(plates_insufficient_volume)
            # msg = '%d plates' % len(plates_insufficient_volume)
            # if library_screening.screen.screen_type == SCREEN_TYPE.SMALL_MOLECULE:
            #     extra_msg = (' (%s uL is required for Small Molecule)'
            #         %  si_unit.convert_decimal(
            #             self.MIN_WELL_VOL_SMALL_MOLECULE, 1e-6, 1))
            #     msg += extra_msg
            # raise ValidationError({
            #     # API_PARAM_VOLUME_OVERRIDE: 'required',
            #     API_MSG_LCPS_INSUFFICIENT_VOLUME: msg,
            #     'library_plates_screened': plates_insufficient_volume
            #     })

        if all_plate_ids:
            logger.info('Fetch the new Plate state: '
                'log plate volume changes, screening count...')
            _new_plate_data = self.get_plate_resource()._get_list_response_internal(
                plate_ids=all_plate_ids,
                visibilities = ['l','d'],
                includes =['-library_comment_array', '-comment_array'])
            plate_logs = self.get_plate_resource().log_patches(
                request, _original_plate_data, _new_plate_data,
                parent_log=ls_log, api_action=API_ACTION.PATCH)
            logger.debug('plate_logs created: %d', len(plate_logs))
        
        # TODO: log the copy state as well

        logger.info('plates: updated: %r, created: %r, deleted: %r', 
            show_plates(extant_plates), show_plates(created_plates), 
            show_plates(deleted_plates))
        
        meta = {
            API_MSG_SCREENING_ADDED_PLATE_COUNT: len(created_plates),
            API_MSG_SCREENING_EXTANT_PLATE_COUNT : len(extant_plates),
            API_MSG_SCREENING_DELETED_PLATE_COUNT: len(deleted_plates),
            API_MSG_SCREENING_PLATES_UPDATED: 
                (len(created_plates) + len(deleted_plates))
            }
        if created_plates:
            meta.update(copywell_allocation_meta)
        if plate_copywell_deallocate_stats:
            meta[API_MSG_COPYWELLS_DEALLOCATED] = plate_copywell_deallocate_stats
        warnings = {}
        if not_allowed_libraries:
            warnings['library_screening_status'] = \
                "library status: " \
                + ', '.join(not_allowed_libraries)
        if plate_warnings:
            warnings['plate_status'] = sorted(plate_warnings)
        if warnings:
            meta[SCHEMA.API_MSG_WARNING] = warnings
        
        logger.info('return meta information: %r', meta)
        # MODIFIED: 20170605 - per JAS/KR, overdraw volume but warn
        if plates_insufficient_volume:
           meta[API_MSG_LCPS_INSUFFICIENT_VOLUME] = plates_insufficient_volume
        return meta
    
    @write_authorization
    @un_cache        
    @transaction.atomic    
    def delete_obj(self, request, deserialized, log=None, **kwargs):
        
        raise ApiNotImplemented(self._meta.resource_name, 'patch_list', { 
            'message': 
                'Library Screening may not be deleted - remove plates to negate'
        })
        #         
        #         if log is None:
        #             raise ProgrammingError('log must be set: %r', kwargs)
        #         
        #         activity_id = kwargs.get('activity_id', None)
        #         if activity_id:
        #             try:
        #                 ls = LibraryScreening.objects.get(activity_id=activity_id)
        #                 plates = set()
        #                 for ap in ls.assayplate_set.all():
        #                     plates.add(ap.plate)
        #                 
        #                 for plate in plates:
        #                     plate.screening_count -= 1
        #                     remaining_well_volume = plate.remaining_well_volume or 0
        #                     remaining_well_volume += ls.volume_transferred_per_well_from_library_plates
        #                     plate.remaining_well_volume = remaining_well_volume
        #                     plate.save()
        # 
        #                 # return meta information for logging
        #                 return {
        #                     API_MSG_SCREENING_ADDED_PLATE_COUNT: 0,
        #                     API_MSG_SCREENING_EXTANT_PLATE_COUNT : 0,
        #                     API_MSG_SCREENING_DELETED_PLATE_COUNT: len(plates)
        #                     }
        #             except ObjectDoesNotExist:
        #                 logger.warn('no such library_screening: %s' % activity_id)
        #                 raise Exception(
        #                     'library_screeningfor activity_id: %s not found' 
        #                     % activity_id)
        #         else:
        #             raise Exception(
        #                 'library_screening delete action requires an activity_id %s' 
        #                 % kwargs)

class RawDataTransformerResource(DbApiResource):

    ERROR_CONTROL_PARSE = 'Parse errors'
    ERROR_CONTROL_DUPLICATES = lims_utils.ERROR_DUPLICATE_WELLS
    ERROR_CONTROL_WELL_TYPE = (
        'Control wells must be \'empty\', \'DMSO\', or \'Library Control\'')
    ERROR_MATRIX_SIZE_DETECTED = \
        'Matrix size detected: %d, does not match Assay Plate Size: %d'
    ERROR_COLLATION_COUNT = 'Number of collations: '\
        'conditions (%d) * replicates (%d) * readouts (%d) = %d, '\
        'must be a divisor of the number matrices read: %d'
    ERROR_PLATE_COUNT = (
        'Plates required (%d) does not match # of plates entered (%d): '
        'Matrices read: %d (transformed: %d), '
        'Collations: %d')
    
    control_type_abbreviations = {
        'assay_positive_controls': 'P',
        'assay_negative_controls': 'N',
        'assay_other_controls': 'O',
        'library_controls': 'C'
    }
    library_well_type_abbreviations = {
        WELL_TYPE.EXPERIMENTAL: 'X',
        WELL_TYPE.EMPTY: 'E',
        WELL_TYPE.DMSO: 'D',
        WELL_TYPE.LIBRARY_CONTROL: 'C',
        WELL_TYPE.RNAI_BUFFER: 'B' 
    }

    class Meta:

        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'rawdatatransform'
        authorization = UserGroupAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        always_return_data = True 
        
    def __init__(self, **kwargs):
        super(RawDataTransformerResource, self).__init__(**kwargs)
        self.lcp_resource = None
        self.reagent_resource = None
        self.labcherrypick_resource = None

    def prepend_urls(self):

        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<screen_facility_id>([\w]+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<screen_facility_id>([\w]+))/"
                 r"(?P<cherry_pick_request_id>(\d+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
        ]
    
    def get_lcp_resource(self):
        if self.lcp_resource is None:
            self.lcp_resource = LibraryCopyPlateResource()
        return self.lcp_resource
    
    def get_labcherrypick_resource(self):
        if self.labcherrypick_resource is None:
            self.labcherrypick_resource = LabCherryPickResource()
        return self.labcherrypick_resource
    
    @read_authorization
    def get_detail(self, request, **kwargs):

        cherry_pick_request_id = kwargs.get('cherry_pick_request_id')
        screen_facility_id = kwargs.get('screen_facility_id')
        
        screen = None
        if screen_facility_id:
            screen = Screen.objects.get(facility_id=screen_facility_id)
        cpr = None
        if cherry_pick_request_id:
            cpr = CherryPickRequest.objects.get(cherry_pick_request_id=cherry_pick_request_id)
            
        # TODO: 20180227 - verify rebuild of schema
        schema = kwargs.pop('schema')
        schema = self.build_schema()

        _data = {}
        query = RawDataTransform.objects.all()
        if cpr:
            query = query.filter(cherry_pick_request=cpr)
            _data['cherry_pick_request_id'] = cpr.cherry_pick_request_id
        else:
            query = query.filter(screen=screen)
            _data['screen_facility_id'] = screen.facility_id

        if not query.exists():
            raise Http404
        
        if query.count() != 1:
            raise Http404('Wrong number of objects returned: %d', query.count())
        rdt = query.all()[0]
        
        for key in schema['fields'].keys():
            if hasattr(rdt, key):
                _data[key] = getattr(rdt,key)
        
        if rdt.rawdatainputfile_set.all().exists():
            input_files = []
            _data['input_files'] = input_files
            for rdif in rdt.rawdatainputfile_set.all():
                logger.info('found input file: %r', rdif)
                input_file = {}
                input_files.append(input_file)
                for key in schema['input_file_fields'].keys():
                    if hasattr(rdif, key):
                        input_file[key] = getattr(rdif,key)
        
        final_data = { API_RESULT_DATA: _data }
        
        return self.build_response(request, _data)
        
    def build_schema(self, user=None, **kwargs):
        schema = DbApiResource.build_schema(self, user=user, **kwargs)    
    
        extra_fields = self.get_field_resource()._build_fields(
            scopes=['fields.rawdatainputfile','fields.rawdataoutput'])
        
        schema['input_file_fields'] = { field['key']:field 
            for field in extra_fields if field['scope'] == 'fields.rawdatainputfile' }
        schema['output_fields'] = { field['key']:field 
            for field in extra_fields if field['scope'] == 'fields.rawdataoutput' }
    
        return schema
    
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, **kwargs):
        
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        
        logger.info('params: %r', param_hash.keys())
        
        is_for_detail = kwargs.pop('is_for_detail', False)
        
        try:
            
            # general setup
          
            manual_field_includes = set(param_hash.get('includes', []))
            
            (filter_expression, filter_hash, readable_filter_hash) = \
                SqlAlchemyResource.build_sqlalchemy_filters(
                    schema, param_hash=param_hash)
            filename = self._get_filename(readable_filter_hash, schema)

            filter_expression = \
                self._meta.authorization.filter(request.user,filter_expression)

                  
            order_params = param_hash.get('order_by', [])
            field_hash = self.get_visible_fields(
                schema['fields'], filter_hash.keys(), manual_field_includes,
                param_hash.get('visibilities'),
                exact_fields=set(param_hash.get('exact_fields', [])),
                order_params=order_params)
            order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
                order_params, field_hash)
             
            rowproxy_generator = None
            if use_vocab is True:
                rowproxy_generator = \
                    DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            rowproxy_generator = \
               self._meta.authorization.get_row_property_generator(
                   request.user, field_hash, rowproxy_generator)
 
            # specific setup
            _rawdatatransform = self.bridge['raw_data_transform']
            _rawdatainput = self.bridge['raw_data_input_file']
            _screen = self.bridge['screen']
            _cpr = self.bridge['cherry_pick_request']
            
            j = _rawdatatransform
            j = j.join(
                _screen, _rawdatatransform.c.screen_id==_screen.c.screen_id,
                isouter=True)
            j = j.join(
                _cpr, _rawdatatransform.c.cherry_pick_request_id==_cpr.c.cherry_pick_request_id,
                isouter=True)
            
            custom_columns = {
#                 'lookup_pmid': literal_column("'lookup_pmid'"),
                }

            base_query_tables = ['raw_data_transform', 'screen','cherry_pick_request' ] 
            columns = self.build_sqlalchemy_columns(
                field_hash.values(), base_query_tables=base_query_tables,
                custom_columns=custom_columns)
            
            stmt = select(columns.values()).select_from(j)
            # general setup
             
            (stmt, count_stmt) = self.wrap_statement(
                stmt, order_clauses, filter_expression)
            if not order_clauses and filter_expression is None:
                _alias = Alias(stmt)
                stmt = select([text('*')]).select_from(_alias)
            stmt = stmt.order_by('-screen_facility_id')
            
            title_function = None
            if use_titles is True:
                def title_function(key):
                    return field_hash[key]['title']
            if is_data_interchange:
                title_function = DbApiResource.datainterchange_title_function(
                    field_hash,schema['id_attribute'])
            
            return self.stream_response_from_statement(
                request, stmt, count_stmt, filename,
                field_hash=field_hash,
                param_hash=param_hash,
                is_for_detail=is_for_detail,
                rowproxy_generator=rowproxy_generator,
                title_function=title_function, meta=kwargs.get('meta', None))
             
        except Exception, e:
            logger.exception('on get_list %s' % self._meta.resource_name)
            raise e  

    def put_list(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_list')
    
    def put_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_detail')
    
    def patch_list(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'patch_list')
    
    def patch_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'patch_detail')
    
    @write_authorization
    @transaction.atomic
    def post_detail(self, request, **kwargs):
        '''
        Parse raw data input files containing plate read data:
        - Input matrices are collated based on the specified ordering of
        plates, conditions, readouts, and replicates.
        - Input matrix values are associated with library or cherry pick well
        data.
        - Assay and library control well data are parsed and associated with 
        raw data values
        - All input settings are stored on RawDataTransform and RawDataInputFile
        objects in the database.
        - Parsed data is returned in Excel spreadsheet format but not stored in 
        the database.
        '''
        
        logger.info('post_detail...')
        
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
                
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)

        if len(request.FILES) == 0:
            raise ValidationError(key='input_file', msg='Required')
        logger.info('request.FILES: %r', request.FILES.keys())
        
        fields = schema['fields']
        initializer_dict = self.parse(param_hash, fields=fields)
        logger.info('initializer: %r', initializer_dict)
        
        errors = self.validate(initializer_dict, patch=False, schema=schema)
        if errors:
            raise ValidationError(errors)
        # Expand plate ranges
        plate_ranges = initializer_dict['plate_ranges']
        logger.info('plate ranges: %r', plate_ranges)
        lcp_resource = self.get_lcp_resource()
        parsed_searches = lcp_resource.parse_plate_copy_search(plate_ranges)
        logger.info('parsed_searches: %r', parsed_searches[0]['plate_numbers_in_order_listed'])
        (plates,errors) = lcp_resource.find_plates(parsed_searches)
        plate_numbers = parsed_searches[0]['plate_numbers_in_order_listed']
        if errors:
            raise ValidationError(
                key='plate_ranges', 
                msg = 'Plates not found: %s' % ', '.join(sorted(errors)))
        # check for duplicates
        plate_check_set = set()
        duplicate_plates = set()
        for plate_number in plate_numbers:
            if plate_number not in plate_check_set:
                plate_check_set.add(plate_number)
            else:
                duplicate_plates.add(plate_number)
        if duplicate_plates:
            raise ValidationError(
                key='plate_ranges', 
                msg='plates specified more than once: %s' 
                    % ', '.join([str(x) for x in duplicate_plates]))
        logger.info('plate_numbers: %r', plate_numbers)
        
        screen_facility_id = initializer_dict.pop('screen_facility_id', None)
        cherry_pick_request_id = \
            initializer_dict.pop('cherry_pick_request_id', None)
        if screen_facility_id is None and cherry_pick_request_id is None:
            msg='must provide one of: %r' % [
                'screen_facility_id','cherry_pick_request_id']
            raise ValidationError({
                'screen_facility_id': msg,
                'cherry_pick_request_id': msg 
            })
        screen = None
        if screen_facility_id is not None and cherry_pick_request_id is None:
            try:
                screen = Screen.objects.get(
                    facility_id=screen_facility_id)
                initializer_dict['screen'] = screen
            except ObjectDoesNotExist:
                raise Http404('screen_facility_id %r does not exist' 
                    % screen_facility_id)
        cpr = None
        if cherry_pick_request_id is not None: 
            try: 
                cpr = CherryPickRequest.objects.get(
                    cherry_pick_request_id=cherry_pick_request_id)
                initializer_dict['cherry_pick_request'] = cpr
            except ObjectDoesNotExist:
                raise Http404('cherry_pick_request_id does not exist: %s' 
                    % cherry_pick_request_id)
        
        try:
            rdt = RawDataTransform.objects.get(
                screen=screen, cherry_pick_request=cpr)
            rdt.rawdatainputfile_set.all().delete()
        except ObjectDoesNotExist:
            logger.info('create a new raw data transform')
            rdt = RawDataTransform.objects.create(
                screen=screen, cherry_pick_request=cpr)
        
        for key, val in initializer_dict.items():
            if hasattr(rdt, key):
                setattr(rdt, key, val)

        _meta = {}

        aps = rdt.assay_plate_size
        lps = rdt.library_plate_size
        
        if aps > lps:
            if len(plate_numbers) % 4 != 0:
                raise ValidationError(key='plate_ranges', 
                    msg='Must be a multiple of 4 if assay_plate_size==1536')
        
        # Retrieve the library well / Lab cherry pick well data:
        # - also record the user entered control well information
        rnai_columns = [
            'vendor_entrezgene_symbols','vendor_entrezgene_id',
            'vendor_genbank_accession_numbers','vendor_gene_name', 
            'facility_entrezgene_symbols','facility_entrezgene_id',
            'facility_genbank_accession_numbers','facility_gene_name', 
            'vendor_name','vendor_identifier','is_deprecated']
        rnai_columns_to_write = []
        wells = {}
        lcp_copywells = {}
        control_well_named_ranges = None
        if screen:
            if rdt.screen.screen_type == 'rnai':
                rnai_columns_to_write = rnai_columns
                # TODO: pool
            exact_well_fields = [
                'library_well_type','well_id','plate_number','well_name']
            exact_well_fields.extend(rnai_columns_to_write)
            logger.info('get well information: %r, fields: %r ...',
                plate_numbers, exact_well_fields)
            wells = self.get_reagent_resource(screen.screen_type)\
                ._get_list_response_internal(**{
                    'plate_number__in': plate_numbers,
                    'exact_fields': exact_well_fields
                })
            if not wells:
                raise ProgrammingError(
                    'no wells found for the plates: %r', plate_numbers)
            logger.info('retrieved %d wells for transformation', len(wells))
            logger.info('sample well keys: %r', wells[0].keys())
            wells = { well['well_id']: well for well in wells }
            control_well_named_ranges = self.get_control_wells(rdt)
            
            self.set_control_wells_to_library_wells(
                rdt, control_well_named_ranges, plate_numbers, wells)
            
        elif cpr:
            includes = [
                'vendor_batch_id','vendor_name','vendor_identifier',
                '-structure_image','-molfile','-library_plate_comment_array']
            if cpr.screen.screen_type == 'rnai':
                rnai_columns_to_write = rnai_columns
                includes.extend(rnai_columns_to_write)
            lcp_copywells = \
                self.get_labcherrypick_resource()\
                    ._get_list_response_internal(**{
                        'cherry_pick_request_id': cpr.cherry_pick_request_id,
                        'source_copy_name__is_null': False,
                        'includes': includes,
                    })
            if not lcp_copywells:
                raise ProgrammingError('No lab cherry picks found for cpr: %r', 
                    cpr.cherry_pick_request_id)
            logger.info('retrieved %d lcp_copywells for transformation', 
                len(lcp_copywells))
            logger.info('sample lcp well keys: %r', lcp_copywells[0].keys())
            lcp_copywells = { 
                lims_utils.well_id(
                    lcp['cherry_pick_plate_number'],lcp['destination_well']):lcp
                        for lcp in lcp_copywells }
            control_well_named_ranges = self.get_control_wells(rdt)
            
            
        # Read in the input matrices
        input_file_fields = schema['input_file_fields']
        logger.info('input_file_fields: %r', input_file_fields.keys())
        
        vocab_scope = input_file_fields['readout_type']['vocabulary_scope_ref']
        readout_vocab = self.get_vocab_resource()._get_vocabularies_by_scope(vocab_scope)
        if not readout_vocab:
            logger.warn('no vocabulary found for scope: %r, field: %r', 
                vocab_scope, 'readout_type')
        def read_input_matrices(rdt, ordinal, filekey, input_file):
            logger.info('read matrices for file: %d, %s', ordinal, filekey)
            logger.info('data file: %r', input_file)
            input_file_initializer = {
                'ordinal': ordinal
                }
            for key,val in param_hash.items():
                if key.find(filekey) == 0:
                    field_key = key[len(filekey)+1:]
                    if field_key in input_file_fields:
                        field = input_file_fields[field_key]
                        input_file_initializer[field_key] = \
                            parse_val(val, field_key, field['data_type'])
            logger.info('input_file_initializer: %r', input_file_initializer)

            errors = self.validate(input_file_initializer, 
                patch=False, 
                schema={ 
                    'id_attribute': ['ordinal'],
                    'fields': input_file_fields })
            if errors:
                raise ValidationError(key=filekey, msg=errors)

            filename = initializer_dict.get('filename', None)
            if filename is None:
                filename = input_file.name
                input_file_initializer['filename'] = filename
            rdif = RawDataInputFile(**input_file_initializer)
            
            (matrices, errors) = raw_data_reader.read(input_file, filename)
            if errors: 
                raise ValidationError(key=filekey, msg=errors)
            if not matrices:
                raise ValidationError(key=filekey, msg='no matrices read')
            
            parse_errors = raw_data_reader.parse_to_numeric(matrices)
            if parse_errors:
                raise ValidationError(key=filekey, msg=parse_errors)
            
            assay_plate_size_read = len(matrices[0])*len(matrices[0][0])
            if assay_plate_size_read != aps:
                msg = self.ERROR_MATRIX_SIZE_DETECTED % (
                    assay_plate_size_read, aps)
                raise ValidationError({
                    filekey: msg, 'assay_plate_size': msg })
            
            collation = Collation.get_value(rdif.collation_order)
            logger.info('read collation: %r', collation)
            conditions = re.split(r'[\s,]+', rdif.conditions) \
                if rdif.conditions else ['C1',]
            replicates = [chr(ord('A')+x) for x in range(0,rdif.replicates)]
            readouts = re.split(r'[\s,]+', rdif.readouts) \
                if rdif.readouts else ['read1',]
            logger.info('conditions: %r, readouts: %r, replicates: %r',
                conditions, readouts, replicates)

            # Determine the plates to read and validate relative sizes
            
            collation_count = len(conditions)*len(replicates)*len(readouts)
            transformed_matrix_count = len(matrices)
            if aps > lps:
                transformed_matrix_count *=4
            elif lps > aps:
                collation_count *= 4

            if collation_count > len(matrices):
                msg = self.ERROR_COLLATION_COUNT % (
                    len(conditions),len(replicates),len(readouts), 
                    collation_count, len(matrices))
                raise ValidationError(key=filekey, msg=msg)
            if len(matrices)%collation_count != 0:
                msg = self.ERROR_COLLATION_COUNT % (
                    len(conditions),len(replicates),len(readouts), 
                    collation_count, len(matrices))
                raise ValidationError(key=filekey, msg=msg)
            
            plates_required = transformed_matrix_count/collation_count
            logger.info('collation count: %d, plates required: %d',
                collation_count, plates_required)
            if plates_required != len(plate_numbers):
                logger.info(str((len(matrices), transformed_matrix_count,
                    collation_count, plates_required, len(plate_numbers))))
                msg = self.ERROR_PLATE_COUNT % (
                    plates_required, len(plate_numbers),
                    len(matrices), transformed_matrix_count,
                    collation_count)
                raise ValidationError({
                    filekey: msg, 'plate_ranges': msg })
            
            counter = plate_matrix_transformer.create_matrix_counter(
                collation, plate_numbers, conditions, replicates, readouts)
            logger.info('counter: %r', counter)

            if aps != lps:
                matrices = plate_matrix_transformer.transform(
                    matrices, counter, aps,lps)
                logger.info('transformed matrices: %d', len(matrices))

            rdif.raw_data_transform = rdt
            rdif.save()
            
            # cache readout_type title
            if rdif.readout_type in readout_vocab:
                rdif.readout_title = readout_vocab[rdif.readout_type]['title']
            else:
                logger.warn('vocab not found for readout_type: %r, vocabs: %r',
                    rdif.readout_type, readout_vocab)
                rdif.readout_title = rdif.readout_type
            return (matrices, rdif, counter)
        
        
        with  NamedTemporaryFile(
            delete=False, suffix='%s' % request.user.username) as temp_file:
            
            workbook = xlsxwriter.Workbook(temp_file) #, {'constant_memory': True})
            
            try:
            
                input_file_data = []
                for ordinal, filekey in enumerate(sorted(request.FILES.keys())):
                    logger.info('read file: %d, %r', ordinal, filekey)
                    input_file = request.FILES[filekey]
            
                    (matrices, rdif, counter) = \
                        read_input_matrices(rdt, ordinal, filekey, input_file)
                    input_file_data.append((matrices, rdif, counter))
    
                    _matrix_read_meta = OrderedDict((
                        ('Ordinal', rdif.ordinal), 
                        ('Filename', rdif.filename), 
                        ('Collation', rdif.collation_order),
                        ('Readout Type', rdif.readout_title)
                    ))
                    _matrix_read_meta['Matrices'] = len(matrices)
                    if aps > lps:
                        _matrix_read_meta['Matrices read (1536 well)'] = len(matrices)/4
                    elif aps < lps:
                        _matrix_read_meta['Matrices read (96 well)'] = len(matrices)*4
                    logger.info('read matrices: %d', len(matrices))
                    
                    for k,v in counter.counter_hash.items():
                        _matrix_read_meta[k.title() + 's'] = ', '.join([str(x) for x in v])
                    _meta['File %d' % (rdif.ordinal+1)] = _matrix_read_meta
                    logger.info('Raw data transform file read meta: %r', _matrix_read_meta)
                    
                if screen:
                    self.write_screen_xlsx(
                        rdt, plate_numbers, input_file_data, workbook, wells, rnai_columns_to_write)
                elif cpr:
                    self.write_cpr_xlsx(
                        rdt, plate_numbers, input_file_data, workbook, lcp_copywells, 
                        control_well_named_ranges,rnai_columns_to_write)
                else:
                    raise ProgrammingError('no screen or cpr')

            finally:
                workbook.close()
                temp_file.close()
            
            rdt.temp_output_filename = temp_file.name
            rdt.save()
            logger.info('wrote temp file: %r', temp_file.name)
        
        _meta[SCHEMA.API_MSG_RESULT] = SCHEMA.API_MSG_SUCCESS
        
        return self.build_response(
            request, {API_RESULT_META: _meta }, response_class=HttpResponse, **kwargs)

    def get_control_wells(self, rdt):
        '''
        Read in the control well fields:
        - parse well selection input
        - transform the assay plate control wells to library plate format
        - set the control type abbreviation to well['type_abbreviation']
        - set the control label to well['control_label']
        collate errors: 
        - parsing
        - check for duplicated wells between types
        - check for control labels assigned to experimental wells
        '''
        
        logger.info('get_control_wells: %r', rdt)
        aps = rdt.assay_plate_size
        lps = rdt.library_plate_size
        combined_errors = defaultdict(dict)
        assay_control_named_ranges = {}
        control_well_fields = [
            'assay_positive_controls', 'assay_negative_controls',
            'assay_other_controls']
        for cfield in control_well_fields:
            (named_ranges, errors) = \
                lims_utils.parse_named_well_ranges(
                    getattr(rdt, cfield), aps)
            if errors:
                combined_errors[cfield][self.ERROR_CONTROL_PARSE] = errors
            assay_control_named_ranges[cfield] = named_ranges
            logger.info('control_well_field: %r, %r, named_well_ranges: %r',
                cfield, getattr(rdt, cfield), named_ranges)

        logger.info('assay control named ranges: %r', assay_control_named_ranges)

        (library_control_named_ranges, errors) = \
            lims_utils.parse_named_well_ranges(
                getattr(rdt, 'library_controls'), lps)
        if errors:
            error_hash = combined_errors['library_controls']
            error_hash[self.ERROR_CONTROL_PARSE] = errors
        logger.info('library_control_named_ranges: %r', 
            library_control_named_ranges)
        
        # Check for duplicated control wells between types:
        DUP_ERROR = self.ERROR_CONTROL_DUPLICATES
        # Assay well duplicates:
        assay_controls_flattened = defaultdict(set)
        for ctype, named_ranges in assay_control_named_ranges.items():
            for label, named_range in named_ranges.items():
                assay_controls_flattened[ctype].update(named_range['wells'])
        
        for test_ctype, test_wells in assay_controls_flattened.items():
            duplicate_wells = set()
            for ctype, assay_wells in assay_controls_flattened.items():
                if test_ctype == ctype:
                    continue
                duplicate_wells.update(
                    set(test_wells) & set(assay_wells))
            if duplicate_wells:
                combined_errors[test_ctype][DUP_ERROR] = duplicate_wells

        # check for library control well duplicates
        library_control_wells = set()
        for named_range in library_control_named_ranges.values():
            library_control_wells.update(named_range['wells'])
        
        if aps < lps:
            duplicate_wells = set()
            for assay_ctype, assay_wells in assay_controls_flattened.items():
                convoluted_wells = lims_utils.convolute_wells(
                    aps, lps, assay_wells)
                duplicates = library_control_wells & set(convoluted_wells)
                if duplicates:
                    duplicate_wells |= duplicates
                    combined_errors[assay_ctype][DUP_ERROR] = duplicates
            if duplicate_wells:
                combined_errors['library_controls'][DUP_ERROR] = duplicate_wells 
        elif lps < aps:
            duplicate_wells = set()
            for assay_ctype, assay_wells in assay_controls_flattened.items():
                deconvoluted_wells_in_quadrants = lims_utils.deconvolute_wells(
                    aps, lps, assay_wells)
                duplicates = library_control_wells & set(
                    [well for sublist in deconvoluted_wells_in_quadrants 
                        for well in sublist])
                if duplicates:
                    duplicate_wells |= duplicates
                    combined_errors[assay_ctype][DUP_ERROR] = duplicates
            if duplicate_wells:
                combined_errors['library_controls'][DUP_ERROR] = duplicate_wells 
        else:
            duplicate_wells = set()
            for assay_ctype, assay_wells in assay_controls_flattened.items():
                duplicates = library_control_wells & set(assay_wells)
                if duplicates:
                    duplicate_wells |= duplicates
                    combined_errors[assay_ctype][DUP_ERROR] = duplicates
            if duplicate_wells:
                combined_errors['library_controls'][DUP_ERROR] = duplicate_wells 
        
        if combined_errors:
            raise ValidationError(combined_errors)
        
        assay_control_named_ranges['library_controls'] = library_control_named_ranges
        
        return assay_control_named_ranges 
        
    def set_control_wells_to_library_wells(
        self, rdt, control_well_named_ranges, plates_in_order,wells):

        aps = rdt.assay_plate_size
        lps = rdt.library_plate_size
        
        # Associate the control well label and type with the wells
        combined_errors = defaultdict(dict)
        control_well_exceptions = {}
        
        library_control_named_ranges = control_well_named_ranges['library_controls']
        assay_control_named_ranges = { ctype: nr for ctype,nr in 
            control_well_named_ranges.items() if ctype != 'library_controls' }
        for well in wells.values():
            library_well_type = well.get('library_well_type')
            plate_number = well['plate_number']
            well_name = well['well_name']
            well['type_abbreviation'] = \
                self.library_well_type_abbreviations[library_well_type]
            
            if plate_number not in plates_in_order:
                # NOT a validation error, plate numbers were used to find wells
                raise ProgrammingError('plate not found: %r', plate_number)
            plate_index = plates_in_order.index(plate_number)
            
            assay_control_wellname = well_name
            if aps > lps:
                # convolute the well to (1536)
                quadrant = plate_index % 4
                assay_control_wellname = lims_utils.convolute_well(
                    lps,aps,well_name)[quadrant]
            elif aps < lps:
                # deconvolute the well to (96)
                assay_control_wellname = lims_utils.deconvolute_well(
                    lps,aps,well_name)
            
            library_control_exceptions = control_well_exceptions.setdefault(
                'library_controls',defaultdict(set))
            found = False
            for label, named_range in library_control_named_ranges.items():
                if well_name in named_range['wells']:
                    well['control_label'] = label
                    found = True
                    if library_well_type == WELL_TYPE.EXPERIMENTAL:
                        library_control_exceptions[well_name].add(
                            well['well_id'])
            
            if found is False:
                for ctype, named_ranges in assay_control_named_ranges.items():
                    ctype_exceptions = \
                        control_well_exceptions.setdefault(
                            ctype,defaultdict(set))
                    for label, named_range in named_ranges.items():
                        if assay_control_wellname in named_range['wells']:
                            well['type_abbreviation'] = \
                                self.control_type_abbreviations[ctype]
                            well['control_label'] = label
                            if library_well_type == WELL_TYPE.EXPERIMENTAL:
                                ctype_exceptions[assay_control_wellname].add(
                                    well['well_id'])
                            break
        
        logger.info('control_well_exceptions: %r', control_well_exceptions)    
        for ctype, well_exception_dict in control_well_exceptions.items():
            if not well_exception_dict:
                continue
            well_exception_dict = { wellname:','.join(well_ids) 
                for wellname, well_ids in well_exception_dict.items() }
            combined_errors[ctype][self.ERROR_CONTROL_WELL_TYPE] = \
                well_exception_dict
        
        if combined_errors:
            raise ValidationError(combined_errors)
        
    def write_cpr_xlsx(
        self, rdt, plate_numbers, input_file_data, workbook, lcp_well_hash,
        control_well_named_ranges,rnai_columns_to_write ):
        '''
        Write the plate matrices directly to a spreadsheet, in collation order:
        - merge in lab_cherry_pick data.
        '''
        DEBUG = False or logger.isEnabledFor(logging.DEBUG)
        logger.info('write cpr worksheet for %r...', rdt)

        cpr_id = rdt.cherry_pick_request.cherry_pick_request_id
        aps = rdt.assay_plate_size
        lps = rdt.library_plate_size
        
        control_well_hash = {}
        for ctype, named_ranges in control_well_named_ranges.items():
            for label,named_range in named_ranges.items():
                for wellname in named_range['wells']:
                    control_well_hash[wellname] = (ctype, label)
        
        headers = ['Plate', 'Well', 'Type']
        if aps > lps:
            headers = ['Plate','Well','Source Plate','Quadrant','Source Well','Type']
        elif lps > aps:
            headers = ['Plate','Well','Quadrant','Source Well','Type']

        row_size = lims_utils.get_rows(lps)
        col_size = lims_utils.get_cols(lps)
        logger.info('row size: %d, col size: %d', row_size, col_size)
        
        # NOTE: Even with "all_plates_in_single_worksheet" option, each input 
        # file must be in its own sheet, because collations may differ
        single_sheet = None
        sheet_name = None
        quadrant = None
        source_plates = None
        cumulative_output_row = 0
        if rdt.output_sheet_option \
                == 'all_plates_in_single_worksheet':
            sheet_name = 'data_%d' % (rdif.ordinal+1)
            single_sheet = workbook.add_worksheet(sheet_name)
        
        for plate_index, plate in enumerate(plate_numbers):
            cpr_plate = 'CP%d_%d' % (cpr_id, plate)
            new_sheet_name = 'CP%d_%d' % (cpr_id, plate)
            if aps > lps:
                quadrant = plate_index % 4
                source_plates = plate_numbers[(plate_index/4):(plate_index/4+4)]
                new_sheet_name = 'CP%d_%s' % (
                    cpr_id, ','.join([str(p) for p in source_plates]))
            elif lps > aps:
                pass
            if single_sheet:
                sheet = single_sheet
            elif sheet_name != new_sheet_name:
                sheet_name = new_sheet_name
                sheet = workbook.add_worksheet(sheet_name)
            logger.info('write sheet: %r', sheet_name)

            # Write the header row for the plate:
            for i, header in enumerate(headers):
                sheet.write_string(0,i,header)
            
            current_col = len(headers)
            for (matrices, rdif, counter) in input_file_data:
                for i,counter_readout in enumerate(counter.iterate_counter_columns()): 
                    collation_string = \
                        '{readout}_{condition}_{replicate}'.format(
                            **counter_readout).title()
                    if rdif.readout_title:
                        collation_string = '%s_%s' % (rdif.readout_title, collation_string)
                    else:
                        collation_string = '%s_%s' % (rdif.readout_type, collation_string)
                        
                    if DEBUG:
                        logger.info('write collation_string: col: %d, %s', 
                            current_col, collation_string)
                    sheet.write_string(0,current_col+i,collation_string )
                current_col += i + 1
    
            sheet.write_string(0,current_col,'Pre-Loaded Controls')
            current_col += 1
            sheet.write_string(0,current_col,'Library Plate')
            current_col += 1
            sheet.write_string(0,current_col,'Source Well')
            current_col += 1
            sheet.write_string(0,current_col,'Library Name')
            current_col += 1
            sheet.write_string(0,current_col,'Vendor ID')
            current_col += 1
            sheet.write_string(0,current_col,'Vendor Batch ID')
                
            if rnai_columns_to_write:
                current_col += 1
                sheet.write_string(0,current_col,'Gene Symbol')
                current_col += 1
                sheet.write_string(0,current_col,'Gene IDs')
                current_col += 1
                sheet.write_string(0,current_col,'Genbank Accession Nos')
                current_col += 1
                sheet.write_string(0,current_col,'Gene Names')
                    
            # Write the values for the plate:
            
            # NOTE: to support xlsxwriter 'constant_memory': True - optimized write, 
            # rows must be written sequentially        
            for rownum in range(0,row_size):
                for colnum in range(0, col_size):
                    output_row = 1 + cumulative_output_row + colnum + rownum * col_size
                    current_col = 0      
                    
                    wellname = lims_utils.get_well_name(rownum, colnum)
                    well_id = lims_utils.well_id(plate, wellname)
                    lcp_well = lcp_well_hash.get(well_id, None)
                    if DEBUG:
                        logger.info('write row: %d: %r', output_row, well_id)
                        logger.info('found lcp well: %r', lcp_well)
                    
                    sheet.write_string(output_row,current_col,cpr_plate)
                    current_col += 1
                    sheet.write_string(output_row,current_col,wellname)
                    current_col += 1
                    
                    assay_plate_wellname = wellname
                    if aps > lps:
                        assay_plate_wellname = \
                            lims_utils.convolute_well(
                                lps, aps, wellname)[quadrant]
                        sheet.write_string(output_row,current_col,
                            ','.join([str(p) for p in source_plates]))
                        current_col += 1
                        sheet.write_string(output_row,current_col,str(quadrant+1))
                        current_col += 1
                        sheet.write_string(output_row,current_col,assay_plate_wellname)
                        current_col += 1
                    elif lps > aps:
                        assay_plate_wellname = \
                            lims_utils.deconvolute_well(lps, aps, wellname)
                        (row,col) = lims_utils.well_row_col(wellname)
                        source_quadrant = lims_utils.deconvolute_quadrant(lps, aps, row, col)
                        sheet.write_string(output_row,current_col,str(source_quadrant+1))
                        current_col += 1
                        sheet.write_string(output_row,current_col,assay_plate_wellname)
                        current_col += 1
                    
                    control = control_well_hash.get(assay_plate_wellname, None)
                    
                    if control:
                        abbrev = self.control_type_abbreviations[control[0]]
                        sheet.write_string(output_row,current_col,abbrev)
                    elif lcp_well:
                        sheet.write_string(output_row,current_col,'X')
                    else:
                        sheet.write_string(output_row,current_col,'E')
                    current_col += 1

                    for j,(matrices, rdif, counter) in enumerate(input_file_data):
                        for i,counter_readout in enumerate(counter.iterate_counter_columns()): 
                            matrix_index = counter.get_index(dict(counter_readout, plate=plate))
                            matrix = matrices[matrix_index]
                            val = matrix[rownum][colnum]
                            if DEBUG:
                                logger.info('write output_row: %d, col: %d,  val: %r', 
                                    output_row, current_col+i, str(val))
                            sheet.write_number(output_row, current_col+i, val)
                        current_col += i +1

                    if control:
                        sheet.write_string(output_row,current_col,control[1])
                    elif lcp_well:    
                        current_col += 1
                        sheet.write_string(output_row,current_col,
                            str(lcp_well.get('library_plate')))
                        current_col += 1
                        sheet.write_string(output_row,current_col,
                            lcp_well.get('source_well_name'))
                        current_col += 1
                        sheet.write_string(output_row,current_col,
                            lcp_well.get('library_short_name'))
                        current_col += 1
                        vals = [lcp_well.get('vendor_name'),
                            lcp_well.get('vendor_identifier')]
                        vals = [ str(v) for v in vals if v ]
                        sheet.write_string(output_row,current_col,' '.join(vals))
                        current_col += 1
                        val = lcp_well.get('vendor_batch_id',None)
                        if val:
                            sheet.write_string(output_row,current_col,)

                        if rnai_columns_to_write:
                            current_col += 1
                            vals = [lcp_well.get('vendor_entrezgene_symbols'),
                                    lcp_well.get('facility_entrezgene_symbols')]
                            vals = sorted(set(
                                [ v for vs in vals if vs for v in vs if v ]))
                            sheet.write_string(output_row,current_col,', '.join(vals))
                            
                            current_col += 1
                            vals = [lcp_well.get('vendor_entrezgene_id'),
                                    lcp_well.get('facility_entrezgene_id')]
                            vals = sorted(set([ str(v) for v in vals if v ]))
                            sheet.write_string(output_row,current_col,', '.join(vals))
                            
                            current_col += 1
                            vals = [lcp_well.get('vendor_genbank_accession_numbers'),
                                    lcp_well.get('facility_genbank_accession_numbers')]
                            vals = sorted(set(
                                [ str(v) for vs in vals if vs for v in vs if v ]))
                            sheet.write_string(output_row,current_col,', '.join(vals))
                            
                            current_col += 1
                            vals = [lcp_well.get('vendor_gene_name'),
                                    lcp_well.get('facility_gene_name')]
                            vals = sorted(set([ str(v) for v in vals if v ]))
                            sheet.write_string(output_row,current_col,', '.join(vals))
            if single_sheet:
                cumulative_output_row = output_row                
            if aps > lps and quadrant < 4:
                cumulative_output_row = output_row                


    def write_screen_xlsx(
        self, rdt, plate_numbers, input_file_data, workbook, library_well_hash,
        rnai_columns_to_write):
        '''
        Write the plate matrices directly to a spreadsheet, in collation order:
        - merge in library well data.
        
        '''
        DEBUG = False or logger.isEnabledFor(logging.DEBUG)

        aps = rdt.assay_plate_size
        lps = rdt.library_plate_size

        logger.info('write workbook...')
        
        headers = ['Plate', 'Well', 'Type','Exclude']
        if aps > lps:
            headers = ['Plate','Well','Source Plate','Quadrant','Source Well','Type','Exclude']
        elif lps > aps:
            headers = ['Plate','Well','Quadrant','Source Well','Type','Exclude']

        row_size = lims_utils.get_rows(lps)
        col_size = lims_utils.get_cols(lps)
        logger.info('row size: %d, col size: %d', row_size, col_size)

        single_sheet = None
        sheet_name = None
        quadrant = None
        source_plates = None
        cumulative_output_row = 0
        if rdt.output_sheet_option == 'all_plates_in_single_worksheet':
            sheet_name = 'data'
            single_sheet = workbook.add_worksheet(sheet_name)
        
        for plate_index, plate in enumerate(plate_numbers):
            new_sheet_name = str(plate)
            if aps > lps:
                quadrant = plate_index % 4
                source_plates = plate_numbers[(plate_index/4):(plate_index/4+4)]
                new_sheet_name = ','.join([str(p) for p in source_plates])
            elif lps > aps:
                pass
            if single_sheet:
                sheet = single_sheet
            elif sheet_name != new_sheet_name:
                sheet_name = new_sheet_name
                sheet = workbook.add_worksheet(sheet_name)
            logger.info('write sheet: %r', sheet_name)

            # Write the header row for the plate:
            for i, header in enumerate(headers):
                sheet.write_string(0,i,header)
                        
            current_col = len(headers)
            for (matrices, rdif, counter) in input_file_data:
                for i,counter_readout in enumerate(counter.iterate_counter_columns()): 
                    collation_string = \
                        '{readout}_{condition}_{replicate}'.format(
                            **counter_readout).title()
                    if rdif.readout_title:
                        collation_string = '%s_%s' % (rdif.readout_title, collation_string)
                    else:
                        collation_string = '%s_%s' % (rdif.readout_type, collation_string)
                    if DEBUG:
                        logger.info('write collation_string: col: %d, %s', 
                            current_col, collation_string)
                    sheet.write_string(0,current_col+i,collation_string )
                current_col += i + 1
    
            sheet.write_string(0,current_col,'Pre-Loaded Controls')
            if rnai_columns_to_write:
                current_col += 1
                sheet.write_string(0,current_col,'Entrezgene Symbol')
                current_col += 1
                sheet.write_string(0,current_col,'Entrezgene ID')
                current_col += 1
                sheet.write_string(0,current_col,'Genbank Accession No.')
                current_col += 1
                sheet.write_string(0,current_col,'Catalog No.')
                current_col += 1
                sheet.write_string(0,current_col,'Gene Name')
                current_col += 1
                sheet.write_string(0,current_col,'Deprecated Pool')
                
            # NOTE: to support xlsxwriter 'constant_memory': True - optimized write, 
            # rows must be written sequentially  
            for rownum in range(0,row_size):
                for colnum in range(0, col_size):
                    output_row = 1 + cumulative_output_row + colnum + rownum * col_size
                    current_col = 0      
                    
                    wellname = lims_utils.get_well_name(rownum, colnum)
                    well_id = lims_utils.well_id(plate, wellname)
                    
                    well = library_well_hash[well_id]
                    if DEBUG:
                        logger.info('write row: %d: %r', output_row, wellname)
                    sheet.write_string(output_row,current_col,str(plate))
                    current_col += 1
                    sheet.write_string(output_row,current_col,wellname)
                    current_col += 1
                    
                    if aps > lps:
                        sheet.write_string(output_row,current_col,
                            ','.join([str(p) for p in source_plates]))
                        current_col += 1
                        source_wells = lims_utils.convolute_well(lps, aps, wellname)
                        sheet.write_string(output_row,current_col,str(quadrant+1))
                        current_col += 1
                        sheet.write_string(output_row,current_col,source_wells[quadrant])
                        current_col += 1
                    elif lps > aps:
                        (row,col) = lims_utils.well_row_col(wellname)
                        source_quadrant = lims_utils.deconvolute_quadrant(lps, aps, row, col)
                        source_wellname = lims_utils.deconvolute_well(lps, aps, wellname)
                        sheet.write_string(output_row,current_col,str(source_quadrant+1))
                        current_col += 1
                        sheet.write_string(output_row,current_col,source_wellname)
                        current_col += 1
                    
                    sheet.write_string(
                        output_row,current_col,well.get('type_abbreviation',None))
                    current_col += 1
                    # NOP sheet.write_string(output_row,current_col, 'exclude')
                    current_col += 1
                    
                    for (matrices, rdif, counter) in input_file_data:
                        for i,counter_readout in enumerate(counter.iterate_counter_columns()): 
                            matrix_index = counter.get_index(dict(counter_readout, plate=plate))
                            matrix = matrices[matrix_index]
                            val = matrix[rownum][colnum]
                            if DEBUG:
                                logger.info('write output_row: %d, col: %d,  val: %r', 
                                    output_row, current_col+i, str(val))
                            sheet.write_number(output_row, current_col+i, val)
                        current_col += i +1

                    control_label = well.get('control_label', None)
                    if control_label:
                        sheet.write_string(output_row,current_col,control_label)
                    elif rnai_columns_to_write:
                        current_col += 1
                        vals = [well.get('vendor_entrezgene_symbols'),
                                well.get('facility_entrezgene_symbols')]
                        vals = sorted(set(
                            [ v for vs in vals if vs for v in vs if v ]))
                        sheet.write_string(output_row,current_col,', '.join(vals))
                        
                        current_col += 1
                        vals = [well.get('vendor_entrezgene_id'),
                                well.get('facility_entrezgene_id')]
                        vals = sorted(set([ str(v) for v in vals if v ]))
                        sheet.write_string(output_row,current_col,', '.join(vals))
                        
                        current_col += 1
                        vals = [well.get('vendor_genbank_accession_numbers'),
                                well.get('facility_genbank_accession_numbers')]
                        vals = sorted(set(
                            [str(v) for vs in vals if vs for v in vs if v ]))
                        sheet.write_string(output_row,current_col,', '.join(vals))
                        
                        current_col += 1
                        vals = [well.get('vendor_name'),well.get('vendor_identifier')]
                        vals = [ str(v) for v in vals if v ]
                        sheet.write_string(output_row,current_col,' '.join(vals))

                        current_col += 1
                        vals = [well.get('vendor_gene_name'),
                                well.get('facility_gene_name')]
                        vals = sorted(set([ str(v) for v in vals if v ]))
                        sheet.write_string(output_row,current_col,', '.join(vals))

                        current_col += 1
                        sheet.write_string(output_row,current_col,
                            'Y' if well.get('is_deprecated') is True else '')

            if single_sheet:
                cumulative_output_row = output_row                
            if aps > lps and quadrant < 4:
                cumulative_output_row = output_row                
        

class ScreenResource(DbApiResource):
    
    class Meta:

        queryset = Screen.objects.all()
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'screen'
        authorization = ScreenAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        always_return_data = True 
        
    def __init__(self, **kwargs):
        super(ScreenResource, self).__init__(**kwargs)
        self.apilog_resource = None
        self.cpr_resource = None
        self.activity_resource = None
        self.lcp_resource = None
        self.libraryscreening_resource = None
        self.screenresult_resource = None
        self.su_resource = None
         
    def clear_cache(self, request, **kwargs):
        logger.info('clear screen caches')
        DbApiResource.clear_cache(self, request, **kwargs)
        caches['screen_cache'].clear()
        self.get_screenresult_resource().clear_cache(request, **kwargs)
        
    def get_su_resource(self):
        if self.su_resource is None:
            self.su_resource = ScreensaverUserResource()
        return self.su_resource
        
    def get_screenresult_resource(self):
        if self.screenresult_resource is None:
            self.screenresult_resource = ScreenResultResource()
        return self.screenresult_resource
    
    def get_librarycopyplate_resource(self):
        if self.lcp_resource is None:
            self.lcp_resource = LibraryCopyPlateResource()
        return self.lcp_resource
    
    def get_apilog_resource(self):
        if self.apilog_resource is None:
            self.apilog_resource = ApiLogResource()
        return self.apilog_resource
    
    def get_cpr_resource(self):
        if self.cpr_resource is None:
            self.cpr_resource = CherryPickRequestResource()
        return self.cpr_resource
    
    def get_activity_resource(self):
        if self.activity_resource is None:
            self.activity_resource = ActivityResource()
        return self.activity_resource
    
    def get_library_screening_resource(self):
        if self.libraryscreening_resource is None:
            self.libraryscreening_resource = LibraryScreeningResource()
        return self.libraryscreening_resource
        
    def prepend_urls(self):

        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/ui%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_detail_uiview'), 
                name="api_dispatch_screen_detail_uiview"),

            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/libraries%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_libraryview'),
                name="api_dispatch_screen_libraryview"),

            # FIXME: screen/{facility_id}/cherrypickrequest is the canonical form
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/cherrypicks%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_cherrypickview'),
                name="api_dispatch_screen_cherrypickview"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/cherrypickrequest%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_cherrypickview'),
                name="api_dispatch_screen_cherrypickrequest"),
                
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/plates_screened%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_plates_screened_view'),
                name="api_dispatch_plates_screened_view"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/copyplatesloaded%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_lcp_loadedview'),
                name="api_dispatch_screen_lcp_loadedview"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/billing%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_billingview'),
                name="api_dispatch_screen_billingview"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/datacolumns%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_datacolumnview'),
                name="api_dispatch_screen_datacolumnview"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/activities%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_activityview'),
                name="api_dispatch_screen_activityview"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/libraryscreening%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_screeningview'),
                name="api_dispatch_screen_screeningview"),
            url((r"^(?P<resource_name>%s)/"
                 r"(?P<facility_id>([\w]+))/screenresult%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_screenresultview'),
                name="api_dispatch_screen_screenresultview"),
            url(r"^(?P<resource_name>%s)/(?P<facility_id>([\w]+))/attachedfiles%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_attachedfileview'),
                name="api_dispatch_screen_attachedfileview"),
            url((r"^(?P<resource_name>%s)/(?P<facility_id>([\w]+))"
                 r"/attachedfiles/(?P<attached_file_id>([\d]+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_attachedfiledetailview'),
                name="api_dispatch_screen_attachedfiledetailview"),
            url(r"^(?P<resource_name>%s)/(?P<facility_id>([\w]+))/publications%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_publicationview'),
                name="api_dispatch_screen_publicationview"),
            url((r"^(?P<resource_name>%s)/(?P<facility_id>([\w]+))"
                 r"/publications/(?P<publication_id>([\d]+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_screen_publicationdetailview'),
                name="api_dispatch_screen_publicationdetailview"),
            url((r"^(?P<resource_name>%s)/(?P<facility_id>([\w]+))"
                 r"/plate_range_search%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_plate_range_search_view'), 
                name="api_dispatch_plate_range_search_view"),
            url((r"^(?P<resource_name>%s)/(?P<facility_id>([\w]+))"
                 r"/plate_range_search/(?P<activity_id>(\d+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_plate_range_search_view'), 
                name="api_dispatch_plate_range_search_view"),
        ]    

    def dispatch_plate_range_search_view(self, request, **kwargs):
        ''' 
        Find: 
        - plates already asssociated with the libraryscreenings for the screen
        - plates matched by the SCHEMA.API_PARAM_SEARCH
        Note: bypasses the "dispatch" framework call
        -- must be authenticated and authorized
        '''
        logger.info('screen plate range search view')
        return self.get_library_screening_resource().\
            dispatch_plate_range_search_view(request, **kwargs)
    
    def build_schema(self, user=None, **kwargs):
        
        logger.info('build screen schema for user: %r, %r', user, kwargs)
        schema = DbApiResource.build_schema(self, user=user, **kwargs)
        
        if self._meta.authorization.is_restricted_view(user):
            if DEBUG_SCREEN_ACCESS:
                logger.info(
                    'Screen schema: Remove sort and search capability for '
                    'restricted access fields')
            restricted_fields = set()
            for key,field in schema['fields'].items():
                if 'data_access_level' in field:
                    if field['data_access_level'] != 0:
                        field['filtering'] = False
                        field['ordering'] = False
                        restricted_fields.add(key)
            if DEBUG_SCREEN_ACCESS:
                logger.info('schema restricted_fields: %r', restricted_fields)
        return schema
        
    def dispatch_screen_detail_uiview(self, request, **kwargs):
        ''' 
        Special method to populate nested entities for the UI 
        - bypasses the "dispatch" framework call
        -- must be authenticated and authorized
        '''
        logger.info('dispatch_screen_detail_uiview')
        
        facility_id = kwargs.get('facility_id', None)
        if not facility_id:
            raise MissingParam('facility_id')
        
        self.is_authenticated(request)

        resource_name = kwargs.pop('resource_name', self._meta.resource_name)
        authorized = self._meta.authorization._is_resource_authorized(
            request.user, 'read', **kwargs)
        if authorized:
            authorized = self._meta.authorization.has_screen_read_authorization(
                request.user, facility_id)
        if authorized is not True:
            raise PermissionDenied
        
        is_restricted_view = self._meta.authorization.is_restricted_view(
            request.user)
        logger.info('is_restricted: %r: %r: %r', resource_name, request.user, 
            is_restricted_view)
                
        if request.method.lower() != 'get':
            return self.dispatch('detail', request, **kwargs)
        
        cache_key = 'detail_ui_%s_%s' % (facility_id, request.user.username) 
        screen_cache = caches['screen_cache']
        _data = screen_cache.get(cache_key)
        
        if not _data:
            logger.info('cache key not set: %s', cache_key)
            _data = self._get_detail_response_internal(
                user=request.user, **kwargs)
            if not _data:
                raise Http404('no screen found for facility_id: %r' % facility_id)
            else:
                # response = self.dispatch('detail', request, format='json', **kwargs )
                # if response.status_code == 200:
                #     _data = self._meta.serializer.deserialize(
                #         JSON_MIMETYPE, 
                #         response['Content-Type'])
                if _data.get('user_access_level_granted') == ACCESS_LEVEL.ALL: #3:
                    logger.info('retrieve status data...')
                    _status_data = \
                        self.get_apilog_resource()._get_list_response_internal(**{
                            'key': _data['facility_id'],
                            'ref_resource_name': 'screen',
                            'diff_keys': 'status',
                            'order_by': ['-date_time'],
                            'includes': ['diffs']
                            })
                    _data['status_data'] = _status_data
                
                    _cpr_data = \
                        self.get_cpr_resource()._get_list_response_internal(**{
                            'limit': 0,
                            'screen_facility_id__eq': _data['facility_id'],
                            'order_by': ['-date_requested'],
                            'exact_fields': [
                                'cherry_pick_request_id','date_requested', 
                                'requested_by_name'],
                            })
                    _data['cherry_pick_request_data'] = _cpr_data
                    
                    _latest_activities_data = \
                        self.get_activity_resource()._get_list_response_internal(**{
                            'screen_facility_id__eq': _data['facility_id'],
                            'limit': 1,
                            'order_by': ['-date_of_activity'],
                            'exact_fields': ['activity_id','type', 
                                'performed_by_name'],
                            })
                    _data['latest_activities_data'] = _latest_activities_data
                    
                    # TODO: attached files
                    # TODO: publications
                    screen_cache.set(cache_key, _data)
                else:
                    # do not cache if restricted
                    _data['is_restricted_view'] = True
        else:
            logger.info('cache key set: %s', cache_key)
        schema = self.build_schema(request.user)
        filename = self._get_filename({
            'facility_id': facility_id }, schema, True)
        # Serialize
        # FIXME: refactor to generalize serialization:
        # see build_response method
        content_type = self.get_serializer().get_accept_content_type(
            request,format=kwargs.get('format', None))
        if content_type in [XLS_MIMETYPE,CSV_MIMETYPE]:
            _data = {'objects': [_data]}
#             filename = 'screen_detail_%s' % facility_id
        response = HttpResponse(
            content=self.get_serializer().serialize(
                _data, content_type),
            content_type=content_type)
        if content_type == XLS_MIMETYPE:
            response['Content-Disposition'] = \
                'attachment; filename=%s.xlsx' % filename
        if content_type == CSV_MIMETYPE:
            response['Content-Disposition'] = \
                'attachment; filename=%s.csv' % filename
        downloadID = request.GET.get('downloadID', None)
        if downloadID:
            logger.info('set cookie "downloadID" %r', downloadID )
            response.set_cookie('downloadID', downloadID)
        else:
            logger.debug('no downloadID: %s' % request.GET )
    
        return response;
        
    def dispatch_screen_attachedfileview(self, request, **kwargs):
        kwargs['screen_facility_id'] = kwargs.pop('facility_id')
        method = 'list'
        if request.method.lower() == 'post':
            # if post is used, force to "post_detail"
            method = 'detail'
        return AttachedFileResource().dispatch(method, request, **kwargs)    

    def dispatch_screen_attachedfiledetailview(self, request, **kwargs):
        kwargs['screen_facility_id'] = kwargs.pop('facility_id')
        return AttachedFileResource().dispatch('detail', request, **kwargs)    
                
    def dispatch_screen_publicationview(self, request, **kwargs):
        kwargs['screen_facility_id'] = kwargs.pop('facility_id')
        method = 'list'
        if request.method.lower() == 'post':
            # if post is used, force to "post_detail"
            method = 'detail'
        return PublicationResource().dispatch(method, request, **kwargs)    

    def dispatch_screen_publicationdetailview(self, request, **kwargs):
        kwargs['screen_facility_id'] = kwargs.pop('facility_id')
        return PublicationResource().dispatch('detail', request, **kwargs)    
                
    def dispatch_screen_activityview(self, request, **kwargs):
        kwargs['screen_facility_id__eq'] = kwargs.pop('facility_id')
        return self.get_activity_resource().dispatch('list', request, **kwargs)    
    
    def dispatch_screen_screeningview(self, request, **kwargs):
        kwargs['screen_facility_id__eq'] = kwargs.pop('facility_id')
        return self.get_library_screening_resource().dispatch('list', request, **kwargs)    
    
    def dispatch_screen_screenresultview(self, request, **kwargs):
        logger.info('dispatch screenresultview...')
        kwargs['screen_facility_id'] = kwargs.pop('facility_id')
        return ScreenResultResource().dispatch('list', request, **kwargs)    
    
    def dispatch_screen_datacolumnview(self, request, **kwargs):
        kwargs['screen_facility_id__eq'] = kwargs.pop('facility_id')
        return DataColumnResource().dispatch('list', request, **kwargs)    
        
    def dispatch_screen_cherrypickview(self, request, **kwargs):
        kwargs['screen_facility_id'] = kwargs.pop('facility_id')
        return self.get_cpr_resource().dispatch('list', request, **kwargs)    
        
    def dispatch_screen_libraryview(self, request, **kwargs):
        kwargs['for_screen_facility_id'] = kwargs.pop('facility_id')
        # NOTE: authorization is checked on LibraryResource
        return LibraryResource().dispatch('list', request, **kwargs)    

    def dispatch_plates_screened_view(self, request, **kwargs):

        screen_facility_id = kwargs.pop('facility_id')
        # NOTE: authorization is performed in LibraryCopyPlateResource
        return self.get_librarycopyplate_resource()\
            .build_screened_plate_response(request, screen_facility_id, **kwargs)    

    def dispatch_screen_billingview(self, request, **kwargs):
        kwargs['visibilities'] = 'billing'
        return self.dispatch('detail', request, **kwargs)    

    @read_authorization
    def get_detail(self, request, **kwargs):
        facility_id = kwargs.get('facility_id', None)
        if not facility_id:
            raise MissingParam('facility_id')
        
        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])

        return self.build_list_response(request, **kwargs)

    def get_query(self, request, schema, param_hash):
        
        DEBUG_SCREEN = logger.isEnabledFor(logging.DEBUG)
        # screens_for_username = param_hash.get('screens_for_username', None)
        screens_for_userid = param_hash.get('screens_for_userid', None)
        # general setup

        facility_id = param_hash.pop('facility_id', None)
        screen_id = None
        if facility_id:
            param_hash['facility_id__eq'] = facility_id
            try:
                screen_id = Screen.objects.get(facility_id=facility_id).screen_id
            except:
                logger.info('no such screen: %r', facility_id)
        manual_field_includes = set(param_hash.get('includes', []))
        # for joins
        manual_field_includes.add('screen_id')
        manual_field_includes.add('has_screen_result')
        manual_field_includes.add('data_sharing_level')

        # Excluded fields for performance        
        default_excludes = ['activity_count','date_of_first_screening_activity',
            'date_of_last_activity','date_of_last_library_screening',
            'library_plates_data_loaded']
        if not facility_id:
            for key in default_excludes:
                if key not in manual_field_includes:
                    manual_field_includes.add('-%s'%key)
        
        extra_params = {}
        if screens_for_userid:
            screener_role_cte = ScreenResource.get_screener_role_cte().cte(
                'screener_roles1')
            manual_field_includes.add('screensaver_user_role')
            extra_params['user'] = screens_for_userid
        
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        logger.info('filter expression: %r', filter_expression)
        filename = self._get_filename(
            readable_filter_hash, schema, **extra_params)
        
        filter_expression = \
            self._meta.authorization.filter(request.user,filter_expression)

        order_params = param_hash.get('order_by', [])
        order_params.append('-facility_id')
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
        logger.debug('visible fields: %r', field_hash.keys())
        
        # specific setup
        base_query_tables = ['screen', 'screen_result']
        _screen = self.bridge['screen']
        _child_screen = _screen.alias('child_screen')
        _parent_screen = _screen.alias('ps')
        _screen_result = self.bridge['screen_result']
        _ap = self.bridge['assay_plate']
        _aw = self.bridge['assay_well']
        _library = self.bridge['library']
        _copy = self.bridge['copy']
        _plate = self.bridge['plate']
        _cpr = self.bridge['cherry_pick_request']
        _lcp = self.bridge['lab_cherry_pick']
        _cpap = self.bridge['cherry_pick_assay_plate']
        _cplt = self.bridge['cherry_pick_liquid_transfer']
        _sfs = self.bridge['screen_funding_supports']
        _screen_collaborators = self.bridge['screen_collaborators']
        _su = self.bridge['screensaver_user']
        _lhsu = _su.alias('lhsu')
        _user_cte = ScreensaverUserResource.get_user_cte().cte('users_s1')
        _collaborator = _user_cte.alias('collaborator')
        _activity = self.bridge['activity']
        _activity1 = _activity.alias('a1')
        
        _screen_keyword = self.bridge['screen_keyword']
        _screen_cell_lines = self.bridge['screen_cell_lines']
        _library_screening = self.bridge['library_screening']
        _cp_screening = self.bridge['cherry_pick_screening']
        _publication = self.bridge['publication']
        _attached_file = self.bridge['attached_file']
        _dc = self.bridge['data_column']
        _overlap_screens = self.get_create_screen_overlap_indexes()
        _overlap_screen = _screen.alias('overlap_screen')
        # create CTEs -  Common Table Expressions for the intensive queries:
        
        collaborators = (
            select([
                _screen_collaborators.c.screen_id,
                _collaborator.c.name,
                _collaborator.c.screensaver_user_id,
                _collaborator.c.username,
                _collaborator.c.email,
                _concat(
                    _collaborator.c.name, '<', _collaborator.c.email, '>'
                    ).label('fullname')])
            .select_from(_collaborator.join(
                _screen_collaborators,
                _collaborator.c.screensaver_user_id 
                    == _screen_collaborators.c.screensaveruser_id))
            .order_by(_screen_collaborators.c.screen_id, 
                      _collaborator.c.last_first))
        if screen_id:
            collaborators = collaborators.where(
                _screen_collaborators.c.screen_id==screen_id)
        collaborators = collaborators.cte('collaborators')


        # create a cte for the max screened replicates_per_assay_plate
        # - cross join version:
        # select 
        # ap.screen_id, ap.plate_number, 
        # ap.replicate_ordinal,lesser.replicate_ordinal  
        # from assay_plate ap 
        # left outer join assay_plate lesser 
        # on ap.plate_number=lesser.plate_number 
        # and ap.screen_id=lesser.screen_id 
        # and lesser.replicate_ordinal > ap.replicate_ordinal  
        # where lesser.replicate_ordinal is null;
        # - aggregate version:
        # select 
        # ap.screen_id, 
        # ap.plate_number, 
        # max(replicate_ordinal) as max_ordinal 
        # from assay_plate ap 
        # group by screen_id, plate_number
        # order by screen_id, plate_number            
        
        aps = select([_ap.c.screen_id, func.count(1).label('count')]).\
            select_from(_ap).\
            where(_ap.c.library_screening_id != None).\
            group_by(_ap.c.screen_id).\
            order_by(_ap.c.screen_id)
        if screen_id:
            aps = aps.where(
                _ap.c.screen_id==screen_id)
        aps = aps.cte('aps')

        # Removed - 20160408 
        # per discussion with JS, no need to try to link presumed assay_plates
        # from the screen_result to (screening visit) assay_plates;
        # Instead, we can infer replicate loading from the screen_result import
        # where the data_column shows the replicate
        # apdl = select([_ap.c.screen_id, func.count(1).label('count')]).\
        #     select_from(_ap).\
        #     where(_ap.c.screen_result_data_loading_id != None).\
        #     group_by(_ap.c.screen_id).\
        #     order_by(_ap.c.screen_id)
        # apdl = apdl.cte('apdl')
        
        # create a cte for the max screened replicates_per_assay_plate
        apsrc = select([
            _ap.c.screen_id,
            _ap.c.plate_id,
            func.max(_ap.c.replicate_ordinal).label('max_per_plate') ]).\
                select_from(_ap).\
                group_by(_ap.c.screen_id, _ap.c.plate_id).\
                order_by(_ap.c.screen_id, _ap.c.plate_id)
        if screen_id:
            apsrc = apsrc.where(
                _ap.c.screen_id==screen_id)
        apsrc = apsrc.cte('apsrc')
        
        # Altered - 20160408 
        # per discussion with JS, no need to try to link presumed assay_plates
        # from the screen_result to (screening visit) assay_plates;
        # Instead, we can infer replicate loading from the screen_result import
        # where the data_column shows the replicate
        # create a cte for the max data loaded replicates per assay_plate
        # apdlrc = select([
        #     _ap.c.screen_id,
        #     _ap.c.plate_number,
        #     func.max(_ap.c.replicate_ordinal).label('max_per_plate') ]).\
        #         select_from(_ap).\
        #         where(_ap.c.screen_result_data_loading_id != None).\
        #         group_by(_ap.c.screen_id, _ap.c.plate_number).\
        #         order_by(_ap.c.screen_id, _ap.c.plate_number)
        # apdlrc = apdlrc.cte('apdlrc')
        
        # FIXME: see #196 20180403
        # https://github.com/hmsiccbl/screensaver/issues/196
        # Library Plates Screened: 
        # (1) Total number of times library plates were screened (not counting replicates), 
        # TODO: this requires a new query: sum(count(ap.plate_number)) - probably
        # requires a double nesting
        # (2) unique library plates screened
        unique_lps = (
            select([
                _ap.c.screen_id,
                func.count(distinct(_ap.c.plate_number)).label('count')
            ])
            .select_from(_ap.join(_screen,_ap.c.screen_id==_screen.c.screen_id))
            .where(_ap.c.library_screening_id != None)
            .group_by(_ap.c.screen_id))
        if facility_id:
            unique_lps = unique_lps.where(_screen.c.facility_id == facility_id )
        unique_lps = unique_lps.cte('unique_lps')    
        assay_plate_screening_count = (
            select([
                _ap.c.screen_id,
                func.count(_ap.c.assay_plate_id).label('count')
            ])
            .select_from(_ap.join(_screen,_ap.c.screen_id==_screen.c.screen_id))
            .where(_ap.c.library_screening_id != None)
            .where(_ap.c.replicate_ordinal == 0)
            .group_by(_ap.c.screen_id))
        if facility_id:
            assay_plate_screening_count = \
                assay_plate_screening_count.where(
                    _screen.c.facility_id == facility_id )
        assay_plate_screening_count = \
            assay_plate_screening_count.cte('assay_plate_screening_count')    
        # Altered - 20160408 
        # per discussion with JS, no need to try to link presumed assay_plates
        # from the screen_result to (screening visit) assay_plates;
        # Instead, we can infer replicate loading from the screen_result import
        # where the data_column shows the replicate
        # create a cte for the max data loaded replicates per assay_plate
        # lpdl = select([
        #     _ap.c.screen_id,
        #     func.count(distinct(_ap.c.plate_number)).label('count')]).\
        #         select_from(_ap).\
        #         where(_ap.c.screen_result_data_loading_id != None).\
        #         group_by(_ap.c.screen_id).cte('lpdl')
        
        libraries_screened = (
            select([
                func.count(distinct(_library.c.library_id)).label('count'),
                _ap.c.screen_id])
            .select_from(
                _ap.join(_plate, _ap.c.plate_id == _plate.c.plate_id)
                    .join(_copy, _plate.c.copy_id == _copy.c.copy_id)
                    .join(_library, _copy.c.library_id == _library.c.library_id))
            .group_by(_ap.c.screen_id))
        if screen_id:
            libraries_screened = libraries_screened.where(
                _ap.c.screen_id==screen_id)
        libraries_screened = libraries_screened.cte('libraries_screened')
        # FIXME: use cherry_pick_liquid_transfer status vocabulary 
        tplcps = (
            select([
                _cpr.c.screen_id,
                func.count(1).label('count')])
            .select_from(
                _cpr.join(
                    _lcp,
                    _cpr.c.cherry_pick_request_id 
                        == _lcp.c.cherry_pick_request_id)
                .join(
                    _cpap,
                    _lcp.c.cherry_pick_assay_plate_id 
                        == _cpap.c.cherry_pick_assay_plate_id)
                .join(
                    _cplt,
                    _cplt.c.activity_id 
                        == _cpap.c.cherry_pick_liquid_transfer_id))
            .group_by(_cpr.c.screen_id)
            .where(_cplt.c.status == 'Successful'))
        if screen_id:
            tplcps = tplcps.where(
                _cpr.c.screen_id==screen_id)
        tplcps = tplcps.cte('tplcps')
        # Create an inner screen-screen_result query to prompt the  
        # query planner to index join not hash join
        new_screen_result = (select([
                _screen.c.screen_id,
                _screen_result.c.screen_result_id,
                _screen_result.c.date_loaded,
                _screen_result.c.experimental_well_count
            ])
            .select_from(
                _screen.join(
                    _screen_result,
                    _screen.c.screen_id == _screen_result.c.screen_id,
                    isouter=True)))
        if facility_id:
            new_screen_result = new_screen_result.where(
                _screen.c.facility_id == facility_id)
        new_screen_result = new_screen_result.cte('screen_result')
            
        lab_head_table = ScreensaverUserResource.get_lab_head_cte().cte('lab_heads')

        try:
            custom_columns = {
                # default to admin level; screen_property_generator will update
                'user_access_level_granted': literal_column('3'),
                'overlapping_positive_screens': (
                    select([
                        func.array_to_string(
                            func.array_agg(_overlap_screen.c.facility_id),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(
                        _overlap_screens.join(
                            _overlap_screen, 
                            _overlap_screens.c.overlap_screen_id
                                ==_overlap_screen.c.screen_id))
                    .where(_overlap_screens.c.screen_id
                        == literal_column('screen.screen_id'))
                    ),
                'reconfirmation_screens': (
                    select([
                        func.array_to_string(
                            func.array_agg(_child_screen.c.facility_id),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(_child_screen)
                    .where(_child_screen.c.parent_screen_id
                        == literal_column('screen.screen_id'))),
                'primary_screen': (
                    select([_parent_screen.c.facility_id])
                    .select_from(_parent_screen)
                    .where(_parent_screen.c.screen_id
                        == literal_column('screen.parent_screen_id'))),
                'collaborator_ids': (
                    select([
                        func.array_to_string(
                            func.array_agg(collaborators.c.screensaver_user_id),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(collaborators)
                    .where(collaborators.c.screen_id 
                        == literal_column('screen.screen_id'))),
                'collaborator_usernames': (
                    select([
                        func.array_to_string(
                            func.array_agg(collaborators.c.username),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(collaborators)
                    .where(collaborators.c.screen_id 
                        == literal_column('screen.screen_id'))),
                'collaborator_names': (
                    select([
                        func.array_to_string(
                            func.array_agg(collaborators.c.name),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(collaborators)
                    .where(collaborators.c.screen_id 
                        == literal_column('screen.screen_id'))),
                'lab_affiliation': lab_head_table.c.lab_affiliation,
                'lab_name': lab_head_table.c.lab_name_full,
                'lab_head_name': lab_head_table.c.name,
                'lab_head_username': lab_head_table.c.username,
                'lab_head_id': lab_head_table.c.screensaver_user_id,
                'lead_screener_name': (
                    select([_concat(_su.c.first_name, ' ', _su.c.last_name)])
                    .select_from(_su)
                    .where(_su.c.screensaver_user_id == _screen.c.lead_screener_id)),
                'lead_screener_id': (
                    select([_su.c.screensaver_user_id])
                    .select_from(_su)
                    .where(_su.c.screensaver_user_id == _screen.c.lead_screener_id)),
                'lead_screener_username': (
                    select([_su.c.username])
                    .select_from(_su)
                    .where(_su.c.screensaver_user_id == _screen.c.lead_screener_id)),
                'has_screen_result': (
                    case([(new_screen_result.c.screen_result_id != None, 1)],
                        else_= 3 )                    
                    ),
                'cell_lines': (
                    select([
                        func.array_to_string(
                            func.array_agg(_screen_cell_lines.c.cell_line),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(_screen_cell_lines)
                    .where(_screen_cell_lines.c.screen_id 
                        == literal_column('screen.screen_id'))),
                'date_of_first_screening_activity': (
                    select([func.min(_activity1.c.date_of_activity)])
                    .select_from(_activity1)
                    .where(_activity1.c.screen_id
                        ==literal_column('screen.screen_id'))
                    .where(_activity1.c.classification=='screening')
                    ),        
                'date_of_last_activity': (
                    select([func.max(_activity1.c.date_of_activity)])
                    .select_from(_activity1)
                    .where(_activity1.c.screen_id
                        ==literal_column('screen.screen_id'))
                    ),        
                'date_of_last_library_screening': (
                    select([func.max(_activity1.c.date_of_activity)])
                    .select_from(_activity1)
                    .where(_activity1.c.screen_id
                        ==literal_column('screen.screen_id'))
                    .where(_activity1.c.classification=='screening')
                    .where(_activity1.c.type=='library_screening')
                    ),        
                'activity_count': (
                    select([func.count(_activity.c.activity_id)])
                    .select_from(_activity)
                    .where(_activity.c.screen_id
                        ==literal_column('screen.screen_id'))
                    ),
                # # TODO: rework the update activity
                # 'screenresult_last_imported': (
                #     select([screen_result_update_activity.c.date_of_activity])
                #     .select_from(screen_result_update_activity)
                #     .where(screen_result_update_activity.c.screen_id 
                #         == literal_column('screen.screen_id'))),
                'funding_supports': (
                    select([
                        func.array_to_string(
                            func.array_agg(literal_column('funding_support')),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(
                        select([_sfs.c.funding_support])
                        .select_from(_sfs)
                        .order_by(_sfs.c.funding_support)
                        .where(_sfs.c.screen_id == literal_column('screen.screen_id'))
                    .alias('inner'))),
                'total_plated_lab_cherry_picks': (
                    select([tplcps.c.count])
                    .select_from(tplcps)
                    .where(tplcps.c.screen_id == _screen.c.screen_id)),
                
                # TODO: convert to vocabulary
                'assay_readout_types': literal_column(
                    "(select array_to_string(array_agg(f1.assay_readout_type),'%s') "
                    '    from ( select distinct(assay_readout_type) '
                    '        from data_column ds '
                    '        join screen_result using(screen_result_id) '
                    '        where screen_id=screen.screen_id'
                    "        and (assay_readout_type = '') is false  ) as f1 ) " 
                    % LIST_DELIMITER_SQL_ARRAY),
                
                'unique_library_plates_screened': (
                    select([unique_lps.c.count])
                    .select_from(unique_lps)
                    .where(unique_lps.c.screen_id == _screen.c.screen_id)),
                'library_plate_screening_count': (
                    select([assay_plate_screening_count.c.count])
                    .select_from(assay_plate_screening_count)
                    .where(assay_plate_screening_count.c.screen_id == _screen.c.screen_id)),
                'library_screenings': (
                    select([func.count(_library_screening.c.activity_id)])
                    .select_from(
                        _activity.join(
                            _library_screening,
                            _library_screening.c.activity_id 
                                == _activity.c.activity_id))
                    .where(_activity.c.screen_id == _screen.c.screen_id)),
                'cherry_pick_screenings': (
                    select([func.count(_cp_screening.c.activity_id)])
                    .select_from(
                        _activity.join(
                            _cp_screening,
                            _cp_screening.c.activity_id 
                                == _activity.c.activity_id))
                    .where(_activity.c.screen_id == _screen.c.screen_id)),

                'assay_plates_screened': (
                    select([aps.c.count]).select_from(aps)
                    .where(aps.c.screen_id == _screen.c.screen_id)),
                # Altered - 20160408 
                # per discussion with JS, no need to try to link presumed assay_plates
                # from the screen_result to (screening visit) assay_plates;
                # Instead, we can infer replicate loading from the screen_result import
                # where the data_column shows the replicate
                # 'assay_plates_data_loaded': ( 
                #     select([apdl.c.count])
                #     .select_from(apdl)
                #     .where(apdl.c.screen_id == _screen.c.screen_id)),
                'libraries_screened_count': (
                    select([libraries_screened.c.count])
                    .select_from(libraries_screened)
                    .where(libraries_screened.c.screen_id == _screen.c.screen_id)),
                
                # FIXME: update extant records
                # # FIXME: use administrative activity vocabulary
                # 'last_data_loading_date': literal_column(
                #     '( select activity.date_created '
                #     '  from activity '
                #     '  join administrative_activity aa using(activity_id) '
                #     '  join screen_update_activity on update_activity_id=activity_id  '
                #     "  where administrative_activity_type = 'Screen Result Data Loading' " 
                #     '  and screen_id=screen.screen_id '
                #     '  order by date_created desc limit 1 )'
                #     ),
                'min_screened_replicate_count': (
                    select([func.min(apsrc.c.max_per_plate) + 1])
                    .select_from(apsrc)
                    .where(apsrc.c.screen_id == _screen.c.screen_id)),
                'max_screened_replicate_count': (
                    select([func.max(apsrc.c.max_per_plate) + 1])
                    .select_from(apsrc)
                    .where(apsrc.c.screen_id == _screen.c.screen_id)),
                # Altered - 20160408 
                # per discussion with JS, no need to try to link presumed assay_plates
                # from the screen_result to (screening visit) assay_plates;
                # Instead, we can infer replicate loading from the screen_result import
                # where the data_column shows the replicate
                # 'min_data_loaded_replicate_count': (
                #     select([func.min(apdlrc.c.max_per_plate) + 1])
                #     .select_from(apdlrc)
                #     .where(apdlrc.c.screen_id == _screen.c.screen_id)),
                # 'max_data_loaded_replicate_count': (
                #     select([func.max(apdlrc.c.max_per_plate) + 1])
                #     .select_from(apdlrc)
                #     .where(apdlrc.c.screen_id == _screen.c.screen_id)),
                'last_data_loading_date':
                    new_screen_result.c.date_loaded,
                'experimental_well_count': 
                    literal_column('screen_result.experimental_well_count'),
                # 20181030 - Migrated to status
                # 'pin_transfer_approved_by_name': (
                #     select([_concat(_su.c.first_name, ' ', _su.c.last_name)])
                #     .select_from(_su)
                #     .where(_su.c.screensaver_user_id == _screen.c.pin_transfer_approved_by_id)),
                # 'pin_transfer_approved_by_username': (
                #     select([_su.c.username])
                #     .select_from(_su)
                #     .where(_su.c.screensaver_user_id == _screen.c.pin_transfer_approved_by_id)),
                'keywords': (
                    select([
                        func.array_to_string(
                            func.array_agg(_screen_keyword.c.keyword),
                            LIST_DELIMITER_SQL_ARRAY)])
                   .select_from(_screen_keyword)
                   .where(_screen_keyword.c.screen_id == _screen.c.screen_id)),
                'screen_id': _screen.c.screen_id,
                'publications': (
                    select([
                        func.array_to_string(
                            func.array_agg(literal_column('title')),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(
                        select([_publication.c.title])
                        .select_from(_publication)
                        .order_by(_publication.c.publication_id)
                        .where(_publication.c.screen_id 
                            == literal_column('screen.screen_id'))
                    .alias('inner'))),
                'publication_ids': (
                    select([
                        func.array_to_string(
                            func.array_agg(literal_column('publication_id')),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(
                        select([_publication.c.publication_id])
                        .select_from(_publication)
                        .order_by(_publication.c.publication_id)
                        .where(_publication.c.screen_id 
                            == literal_column('screen.screen_id'))
                    .alias('inner'))),
                'attached_files': (
                    select([
                        func.array_to_string(
                            func.array_agg(_attached_file.c.filename),
                            LIST_DELIMITER_SQL_ARRAY)])
                   .select_from(_attached_file)
                   .where(_attached_file.c.screen_id == _screen.c.screen_id)),
            }

            # Altered - 20160408 
            # per discussion with JS, no need to try to link presumed assay_plates
            # from the screen_result to (screening visit) assay_plates;
            # Instead, we can infer replicate loading from the screen_result import
            # where the data_column shows the replicate
            # 'library_plates_data_loaded': (
            #     select([lpdl.c.count])
            #     .select_from(lpdl)
            #     .where(lpdl.c.screen_id == _screen.c.screen_id)),
            # FIXME: 20180227 - rework to be performant
            custom_columns['library_plates_data_loaded'] = (
                select([func.count(distinct(_aw.c.plate_number))])
                .select_from(
                    _aw.join(_screen_result,
                        _aw.c.screen_result_id
                            == _screen_result.c.screen_result_id))
                .where(_screen_result.c.screen_id == _screen.c.screen_id)
                )
            if screen_id:
                custom_columns['library_plates_data_loaded'] = \
                    custom_columns['library_plates_data_loaded']\
                        .where(_screen_result.c.screen_id==screen_id)
        
        except Exception, e:
            logger.exception('on custom columns creation')
            raise 
        
        if screens_for_userid:
            custom_columns['screensaver_user_role'] = \
                screener_role_cte.c.screensaver_user_role
        
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)

        # build the query statement

        j = _screen
        if screens_for_userid:
            j = j.join(
                screener_role_cte,
                screener_role_cte.c.screen_id == _screen.c.screen_id)
        
        j = j.join(new_screen_result,
            _screen.c.screen_id == new_screen_result.c.screen_id)
        
        j = j.join(
            lab_head_table, 
            lab_head_table.c.screensaver_user_id==_screen.c.lab_head_id,
            isouter=True)
        
        stmt = select(columns.values()).select_from(j)

        if screens_for_userid:
            stmt = stmt.where(
                screener_role_cte.c.screensaver_user_id == screens_for_userid)

        # TODO: test if more efficient filtering in sql
        # stmt = self._meta.authorization.filter_in_sql(
        #     request.user, stmt, _screen)
        
        # general setup
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        if DEBUG_SCREEN: 
            logger.info(
                'stmt: %s',
                str(stmt.compile(
                    dialect=postgresql.dialect(),
                    compile_kwargs={"literal_binds": True})))
          
        return (field_hash, columns, stmt, count_stmt, filename)

    def build_list_response(self, request, **kwargs):
        ''' 
        ScreenResource
        '''
        logger.info('ScreenResource - build_list_response')
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
         
        is_for_detail = kwargs.pop('is_for_detail', False)
        
        manual_field_includes = set(param_hash.get('includes', []))
        
        # add fields for authorization filters
        manual_field_includes.add('user_access_level_granted')
        if self._meta.authorization.is_restricted_view(request.user):
            manual_field_includes.add('overlapping_positive_screens')
            manual_field_includes.add('screen_id')
            
        param_hash['includes'] = manual_field_includes
             
        (field_hash, columns, stmt, count_stmt, filename) = \
            self.get_query(request, schema, param_hash)
         
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            # use "use_vocab" as a proxy to also adjust siunits for viewing
            rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
                field_hash, rowproxy_generator)
        
        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)
                
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
         
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None),
            use_caching=True)
          

    @classmethod
    def get_screener_role_cte(cls):
        
        bridge = get_tables()
        _su = bridge['screensaver_user']
        _screen = bridge['screen']
        _screen_collab = bridge['screen_collaborators']
        collab = _su.alias('collab')
        ls = _su.alias('ls')
        pi = _su.alias('pi')
        
        j = _screen
        j = j.join(
            _screen_collab,
            _screen_collab.c.screen_id == _screen.c.screen_id, isouter=True)
        j = j.join(
            collab,
            collab.c.screensaver_user_id == _screen_collab.c.screensaveruser_id,
            isouter=True)
        j = j.join(
            ls, ls.c.screensaver_user_id == _screen.c.lead_screener_id,
            isouter=True)
        j = j.join(
            pi, pi.c.screensaver_user_id == _screen.c.lab_head_id,
            isouter=True)
        
        sa = (
            select([
                _screen.c.facility_id,
                _screen.c.screen_id,
                ls.c.screensaver_user_id.label('lead_screener_id'),
                pi.c.screensaver_user_id.label('pi_id'),
                func.array_agg(collab.c.screensaver_user_id).label('collab_ids')])
            .select_from(j)
            .group_by(_screen.c.facility_id, _screen.c.screen_id,
                ls.c.screensaver_user_id, pi.c.screensaver_user_id)).cte('screen_associates')
        screener_roles = (
            select([
                _su.c.screensaver_user_id,
                sa.c.facility_id,
                sa.c.screen_id,
                case([
                    (_su.c.screensaver_user_id == sa.c.lead_screener_id,
                        'lead_screener'),
                    (_su.c.screensaver_user_id == sa.c.pi_id,
                        'principal_investigator')
                    ],
                    else_='collaborator').label('screensaver_user_role')
                ])
            .select_from(sa)
            .where(or_(
                # TODO: replace with "any_()" from sqlalchemy 1.1 when avail
                _su.c.screensaver_user_id == text(' any(collab_ids) '),
                _su.c.screensaver_user_id == sa.c.lead_screener_id,
                _su.c.screensaver_user_id == sa.c.pi_id))
        )
        return screener_roles
    
    @write_authorization
    @un_cache        
    @transaction.atomic    
    def delete_obj(self, request, deserialized, **kwargs):
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        
        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
        Screen.objects.get(**id_kwargs).delete()
    
#     def validate(self, _dict, patch=False, current_object=None):
#         errors = DbApiResource.validate(self, _dict, patch=patch)
#         # if not errors:
#         #     errors = {}
#         #     dped = _dict.get('data_privacy_expiration_date', None)
#         #     dped_notified = _dict.get('data_privacy_expiration_notified_date', None)
#         #     min_dped = _dict.get('min_allowed_data_privacy_expiration_date', None)
#         #     max_dped = _dict.get('max_allowed_data_privacy_expiration_date', None)
#         #     if not dped:
#         #         if min_dped or max_dped:
#         #             errs['data_privacy_expiration_date'] = \
#         #                 'can not be null if min/max dates are set'
#         #     if dped_notified:
#         #         if min_dped:
#         #             errs['min_allowed_data_privacy_expiration_date'] = \
#         #                 'can not be set if the expiration notified date is set'
#         #         if self.max_allowed_data_privacy_expiration_date:
#         #             errs['min_allowed_data_privacy_expiration_date'] = \
#         #                 'can not be set if the expiration notified date is set'
#         return errors

    @write_authorization
    @un_cache
    @transaction.atomic        
    def post_detail(self, request, **kwargs):
        '''
        POST is used to create or update a resource; not idempotent;
        - The LIMS client will use POST to create exclusively
        '''
        logger.info('post_detail, screen')
        deserialize_meta = None
        if kwargs.get('data', None):
            # allow for internal data to be passed
            deserialized = kwargs['data']
        else:
            deserialized, deserialize_meta = self.deserialize(
                request, format=kwargs.get('format', None))

        logger.debug('patch detail %s, %s', deserialized,kwargs)
        
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        logger.debug('param_hash: %r', param_hash)
        override_param = parse_val(
            param_hash.get(API_PARAM_OVERRIDE, False),
                API_PARAM_OVERRIDE, 'boolean')
        patch_facility_id = deserialized.get('facility_id', None)
        if patch_facility_id and override_param is not True:
            raise ValidationError({
                'facility_id':
                    'May not be specified in screen creation: %s' % patch_facility_id,
                API_PARAM_OVERRIDE: 'required' })
        
        if not patch_facility_id:    
            # find a new facility id
            max_facility_id_sql = '''
                select facility_id::text from screen 
                where parent_screen_id is null
                and study_type is null 
                and facility_id ~ '^[[:digit:]]+$' 
                order by facility_id::integer desc
                limit 1;
            '''
            # NOTE: not using facility_id ~ E'^\\d+$' syntax, which is valid when
            # "standard_conforming_strings = "on";
            # the test harness sets the standard_conforming_strings = off for the 
            # test connections; and does not recognize the E'\\d syntax at all. 
            # (even if standard_conforming_strings is set to "on").
            with get_engine().connect() as conn:
                max_facility_id = int(
                    conn.execute(max_facility_id_sql).scalar() or 0)
                patch_facility_id = str(max_facility_id+1)

        logger.info('new screen facility id to be created: %r', patch_facility_id)
        kwargs['facility_id'] = patch_facility_id

        if 'test_only' in param_hash:
            logger.info('test_only flag: %r', param_hash.get('test_only'))    
            raise InformationError(
                'successful post, "test_only" flag is set, rollback...')
        # FIXME: deserialize_meta is not used
        return self.patch_detail(request,**kwargs)

    @write_authorization
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):
        
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        id_kwargs = self.get_id(deserialized, validate=True, schema=schema, **kwargs)
        logger.info('patch screen: %r', id_kwargs)
        create = False
        try:
            screen = Screen.objects.get(**id_kwargs)
        except ObjectDoesNotExist, e:
            create = True
            screen = Screen(**id_kwargs)

        initializer_dict = self.parse(deserialized,  schema=schema, create=create)
            
        _key = 'lab_head_id'
        lab_head_id = initializer_dict.get(_key)
        if lab_head_id:
            try: 
                lab_head = ScreensaverUser.objects.get(
                    screensaver_user_id=lab_head_id)
                initializer_dict['lab_head'] = lab_head
                
            except ObjectDoesNotExist:
                raise ValidationError(
                    key=_key,
                    msg='No such user: %r' % initializer_dict[_key])
        # Look for the lab_head_username alternate natural key
        _key = 'lab_head_username'
        lab_head_username = initializer_dict.get(_key)
        if lab_head_username:
            try: 
                lab_head = ScreensaverUser.objects.get(
                    username=lab_head_username)
                logger.info('found alternate lab_head_username: %r', 
                    lab_head_username)
                if lab_head_id and lab_head_id != lab_head.screensaver_user_id:
                    raise ValidationError(
                        key='lab_head_username',
                        msg='may not be set if "lab_head_id" is set')
                initializer_dict['lab_head'] = lab_head
                initializer_dict['lab_head_id'] = lab_head.screensaver_user_id
            except ObjectDoesNotExist:
                raise ValidationError(
                    key=_key,
                    msg='No such user: %r' % initializer_dict[_key])
        
        _key = 'lead_screener_id'
        lead_screener_id = initializer_dict.get(_key)
        if lead_screener_id:
            try:
                # may not be null
                initializer_dict['lead_screener'] = (
                    ScreensaverUser.objects.get(
                        screensaver_user_id=initializer_dict[_key]))
            except ObjectDoesNotExist:
                raise ValidationError(
                    key=_key,
                    msg='No such user: %r' % initializer_dict[_key])
        # Look for the lab_head_username alternate natural key
        _key = 'lead_screener_username'
        lead_screener_username = initializer_dict.get(_key)
        if lead_screener_username:
            try: 
                lead_screener = ScreensaverUser.objects.get(
                    username=lead_screener_username)
                logger.info('found alternate lead_screener_username: %r', 
                    lead_screener_username)
                if lead_screener_id \
                    and lead_screener_id != lead_screener.screensaver_user_id:
                    raise ValidationError(
                        key='lead_screener_username',
                        msg='may not be set if "lead_screener_id" is set')
                
                initializer_dict['lead_screener'] = lead_screener
                initializer_dict['lead_screener_id'] = lead_screener.screensaver_user_id
                
            except ObjectDoesNotExist:
                raise ValidationError(
                    key=_key,
                    msg='No such user: %r' % initializer_dict[_key])

        _key = 'collaborator_ids'
        collaborator_ids = initializer_dict.get(_key)
        collaborators = []
        if collaborator_ids:
            for collaborator_id in collaborator_ids:
                try:
                    collaborators.append(ScreensaverUser.objects.get(
                        screensaver_user_id=collaborator_id))
                except ObjectDoesNotExist:
                    raise ValidationError(
                        key=_key,
                        msg='No such screensaver_user_id: %r' % collaborator_id)
            initializer_dict['collaborators'] = collaborators
        # Look for the collaborator_usernames - alternate natural keys
        _key = 'collaborator_usernames'
        collaborator_usernames = initializer_dict.get(_key)
        if collaborator_usernames:
            if collaborators:
                raise ValidationError(
                    key=_key,
                    msg='May not be set if "collaborator_ids" is set')
            alternate_collaborators = []
            for collaborator_username in collaborator_usernames:
                try:
                    alternate_collaborators.append(
                        ScreensaverUser.objects.get(
                            username=collaborator_username))
                except ObjectDoesNotExist:
                    raise ValidationError(
                        key=_key,
                        msg='No such username: %r' % collaborator_username)
            initializer_dict['collaborators'] = alternate_collaborators
                
        # _key = 'pin_transfer_approved_by_username'
        # pin_transfer_approved_by = None
        # if _key in initializer_dict:
        #     val = initializer_dict[_key]
        #     if val:
        #         pin_transfer_approved_by = \
        #             self.get_su_resource()._get_detail_response_internal(
        #             exact_fields=['screensaver_user_id','is_staff'],
        #             username=val)
        #         if not pin_transfer_approved_by: 
        #             raise ValidationError(
        #                 key=_key,
        #                 msg='No such username: %r' % val)
        #         if pin_transfer_approved_by.get('is_staff',False) != True:
        #             raise ValidationError(
        #                 key='pin_transfer_approved_by_username',
        #                 msg='Must be a staff user')
        # pin_transfer_date_approved = \
        #     initializer_dict.get('pin_transfer_date_approved',None)
        # pin_transfer_comments = \
        #     initializer_dict.get('pin_transfer_comments', None)

        errors = self.validate(initializer_dict,  schema=schema, patch=not create)
        if errors:
            raise ValidationError(errors)
        
        related_initializer = {}
        related_initializer['cell_lines'] = \
            initializer_dict.pop('cell_lines', None)
        related_initializer['funding_supports'] = \
            initializer_dict.pop('funding_supports', None)
        related_initializer['keywords'] = \
            initializer_dict.pop('keywords', None)
        related_initializer['publications'] = \
            initializer_dict.pop('publications', None)
            
        for key, val in initializer_dict.items():
            if hasattr(screen, key):
                setattr(screen, key, val)
        screen.clean()
        screen.save()
        logger.info('screen.study_type: %r', screen.study_type)
        # NOTE: collaborators cannot be set until after the object is saved:
        # the many-to-many related manager is not functional until then.
        if 'collaborators' in initializer_dict:
            screen.collaborators = initializer_dict.get('collaborators', None)
        
        logger.info('save/created screen: %r', screen)
        
        # related objects
        
        _key = 'cell_lines'
        _val = related_initializer.get(_key, None)
        if _val is not None:
            (ScreenCellLines.objects
                .filter(screen=screen)
                .exclude(cell_line__in=_val)
                .delete())
            current_cell_lines = (
                ScreenCellLines.objects.filter(screen=screen)
                    .values_list('cell_line', flat=True))
            for cell_line in _val:
                if cell_line not in current_cell_lines:
                    ScreenCellLines.objects.create(
                        screen=screen,
                        cell_line=cell_line)
        _key = 'funding_supports'
        _val = related_initializer.get(_key, None)
        if _val is not None:
            (ScreenFundingSupports.objects
                .filter(screen=screen)
                .exclude(funding_support__in=_val)
                .delete())
            current_funding_supports = (
                ScreenFundingSupports.objects.filter(screen=screen)
                    .values_list('funding_support', flat=True))
            for funding_support in _val:
                if funding_support not in current_funding_supports:
                    ScreenFundingSupports.objects.create(
                        screen=screen,
                        funding_support=funding_support)
        
        # # Set the pin transfer approval data
        # if pin_transfer_approved_by is not None:
        #     screen.pin_transfer_approved_by_id \
        #         = pin_transfer_approved_by['screensaver_user_id']
        #     
        # elif screen.pin_transfer_approved_by is None:
        #     # secondary pin transfer validation
        #     if pin_transfer_date_approved:
        #         raise ValidationError(
        #             key='pin_transfer_date_approved',
        #             msg='requires pin_transfer_approved_by_username')    
        #     if pin_transfer_comments:
        #         raise ValidationError(
        #             key='pin_transfer_comments',
        #             msg='requires pin_transfer_approved_by_username')    
            
        # TODO: deprecated <20180925
        _key = 'keywords'
        _val = related_initializer.get(_key, None)
        if _val is not None:
            (ScreenKeyword.objects
                .filter(screen=screen)
                .exclude(keyword__in=_val)
                .delete())
            current_keywords = (
                ScreenKeyword.objects.filter(screen=screen)
                    .values_list('keyword', flat=True))
            for keyword in _val:
                if keyword not in current_keywords:
                    ScreenKeyword.objects.create(
                        screen=screen,
                        keyword=keyword)
        
        _key = 'publications'
        _val = related_initializer.get(_key, None)
        if _val is not None:
            current_publications = set([ 
                str(x) for x in
                    screen.publication_set.all()
                        .values_list('publication_id', flat=True)])
            logger.info('current: %r, val: %r', current_publications, _val)
            publications_delete = current_publications - set(_val)
            logger.info('delete publications: %r', publications_delete)
            if publications_delete:
                query = Publication.objects.all().filter(
                    publication_id__in=publications_delete)
                logger.info('delete: %r', [x for x in query])
                query.delete()
            # Note: prefer create publications with screen reference set
            publications_add = set(_val) - current_publications
            logger.info('add publications: %r', publications_add)
            if publications_add:
                query = Publication.objects.all().filter(
                    publication_id__in=publications_add)
                screen.publication_set.add(query)
        
        _key = 'primary_screen'
        _val = deserialized.get(_key)
        if _val:
            if ( screen.parent_screen 
                and screen.parent_screen.facility_id != _val):
                raise ValidationError(
                    key=_key,
                    msg='May not be reassigned')
            elif screen.parent_screen is None:
                try:
                    parent_screen = Screen.objects.get(facility_id=_val)
                    
                    if screen.screen_type != parent_screen.screen_type:
                        raise ValidationError(
                            key=_key,
                            msg='Primary screen must have the same screen_type')
                    if screen.lab_head != parent_screen.lab_head:
                        raise ValidationError(
                            key=_key,
                            msg='Primary screen must have the same lab_head')
                    
                    screen.parent_screen = parent_screen
                except ObjectDoesNotExist:
                    raise ValidationError(
                        key=_key,
                        msg='Does not exist: %r' % _val)

        screen.save()
        logger.info('patch_obj done')
        return { API_RESULT_OBJ: screen }

class StudyResource(ScreenResource):
    
    class Meta:
        resource_name = 'study'
        max_limit = 10000
        always_return_data = True
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        authorization = ScreenAuthorization(resource_name)
        serializer = LimsSerializer()
        queryset = Screen.objects.all()  # .order_by('facility_id')
        
    def __init__(self, **kwargs):
        super(StudyResource, self).__init__(**kwargs)
    
    def prepend_urls(self):

        urls = super(StudyResource, self).prepend_urls()
        
        urls = [
            url(r"^(?P<resource_name>%s)"
                r"/create_confirmed_positive_study%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_create_confirmed_positive_study'), 
                name="api_dispatch_create_confirmed_positive_study"),
            url(r"^(?P<resource_name>%s)"
                r"/create_screened_count_study%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_create_screened_count_study_v2'), 
                name="api_dispatch_create_screened_count_study"),
            url(r"^(?P<resource_name>%s)"
                r"/create_screened_count_study_v1%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_create_screened_count_study_v1'), 
                name="api_dispatch_create_screened_count_study_v1"),
        ] + urls
        return urls
            
    def build_schema(self, user=None, **kwargs):
        # Bypass Screen schema
        schema = DbApiResource.build_schema(self, user=user, **kwargs)
        return schema

    
    @read_authorization
    def build_list_response(self, request, **kwargs):
        
        kwargs['study_type__is_null'] = False
        
        return super(StudyResource,self).build_list_response(request, **kwargs)

    @write_authorization
    @un_cache
    @transaction.atomic        
    def dispatch_create_screened_count_study_v1(self, request, **kwargs):
        '''
        Create/Update the Screened Count Study:
        - Count of positives for each well
        - Screening count for each well
        '''

        logger.info('create the create_screened_count_study...')

        request_method = request.method.lower()
        if request_method != 'post':
            raise BadRequestError(key='method', msg='Only POST is allowed')

        convert_request_method_to_put(request)
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        deserialize_meta = None
        if kwargs.get('data', None):
            # allow for internal data to be passed
            deserialized = kwargs.pop('data')
        else:
            deserialized,deserialize_meta = self.deserialize(
                request, format=kwargs.get('format', None))
        if 'facility_id' not in deserialized:
            raise ValidationError(key='facility_id', msg='required')
        
        COL_NAME_POSITIVES_COUNT = 'Screen Positives Count'
        COL_DESC_POSITIVES_COUNT = \
            'Number of times scored as positive across all %s Screens'
        COL_NAME_SCREENED_COUNT = 'Screened Count'
        COL_DESC_SCREENED_COUNT = 'Number of times screened for all %s Screens'
        
        facility_id = deserialized['facility_id']
        data_sharing_level = deserialized.get('data_sharing_level', None)
        if data_sharing_level is None:
            deserialized['data_sharing_level'] = DSL.SHARED
        patch = True
        try: 
            current_study = Screen.objects.get(facility_id=facility_id)
        except ObjectDoesNotExist:
            patch = False    
        if patch is True:
            if 'study_type' in deserialized:
                del deserialized['study_type']
            if 'screen_type' in deserialized:
                del deserialized['screen_type']    
            
        _data = self.build_patch_detail(request, deserialized, **kwargs)
        _data = _data[API_RESULT_DATA][0]
        logger.info('study patched: %r', _data)
        study_obj = Screen.objects.get(facility_id=_data['facility_id'])
        logger.info('study created/updated: %r', study_obj)
        study_obj.date_created = _now()
        study_obj.study_type = 'in_silico'
        study_obj.save()
        
        col_desc_screened_count = COL_DESC_SCREENED_COUNT
        col_desc_positives_count = COL_DESC_POSITIVES_COUNT
        if study_obj.screen_type == 'small_molecule':
            col_desc_screened_count = col_desc_screened_count % 'Small Molecule'
            col_desc_positives_count = col_desc_positives_count % 'Small Molecule'
        elif study_obj.screen_type == 'rnai':
            col_desc_screened_count = col_desc_screened_count % 'RNAi'
            col_desc_positives_count = col_desc_positives_count % 'RNAi'
        else:
            raise ValidationError(key='screen_type', msg='Unknown type')
        
        # Create the Data Columns
        result_columns = OrderedDict((
            ('E', {
                'name': COL_NAME_POSITIVES_COUNT,
                'data_worksheet_column': 'E',
                'data_type': 'integer',
                'description': col_desc_positives_count,
            }),
            ('F', {
                'name': COL_NAME_SCREENED_COUNT,
                'data_worksheet_column': 'F',
                'data_type': 'integer',
                'description': col_desc_screened_count,
            }),
        ))
        
        sql = '''
        with aws as (
            select 
            well_id,
            is_positive,
            screen_id
            from assay_well
            join well w using(well_id)
            join screen_result using(screen_result_id)
            join screen using(screen_id)
            where screen.screen_type = '{screen_type}'
            and screen.study_type is null
            and w.library_well_type = '{well_type}'
        )
        select
            well_id,
            count(*) as screened_count,
            count(case when is_positive then 1 end ) as positives_count
        from aws
        group by well_id
        order by well_id
        '''
        sql = sql.format(screen_type=study_obj.screen_type, 
                         well_type=WELL_TYPE.EXPERIMENTAL)
        columns = ['well_id', 'screened_count', 'positives_count']
        
        # Create the study by iterating through the report:
        # Collate the number of confirmed positives for each Screen
        with get_engine().connect() as conn:
            
            logger.info('execute the screened_count_study query...')
            logger.debug('execute sql: %s', sql)
            result = conn.execute(text(sql))
            logger.info('executed the confirmed_positive_study query')
            
            def result_value_generator(result):
                for row in result:
                    _dict = dict(zip(columns,row))
                    yield {
                        'well_id': _dict['well_id'],
                        'assay_well_control_type': None,
                        'E': _dict['positives_count'],
                        'F': _dict['screened_count'],
                    }
            logger.info('creating study values for: %r', study_obj)
            screen_result_meta = \
                self.get_screenresult_resource().create_screen_result(
                    request, study_obj, result_columns, 
                    result_value_generator(result))
            logger.info('study values created for: %r', study_obj)
        
        meta = {
            SCHEMA.API_MSG_RESULT: SCHEMA.API_MSG_SUCCESS
        }
        meta.update(screen_result_meta)
        if deserialize_meta:
            meta.update(deserialize_meta)
        if not self._meta.always_return_data:
            response = self.build_response(request, { API_RESULT_META: meta }, **kwargs)
            return response
        else:
            new_data = self._get_detail_response_internal(facility_id=facility_id)
            data = { API_RESULT_META: meta, API_RESULT_DATA: [new_data]}
            response = self.build_response(request, data, status_code=201)
            return response 

    @write_authorization
    @un_cache
    @transaction.atomic        
    def dispatch_create_screened_count_study_v2(self, request, **kwargs):
        '''
        Create/Update the Screened Count Study:
        - Count of positives for each well
        - Screening count for each well: 
        - for small_molecule, create counts based on assay_type
        '''

        logger.info('create the create_screened_count_study...')

        request_method = request.method.lower()
        if request_method != 'post':
            raise BadRequestError(key='method', msg='Only POST is allowed')

        convert_request_method_to_put(request)
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        deserialize_meta = None
        if kwargs.get('data', None):
            # allow for internal data to be passed
            deserialized = kwargs.pop('data')
        else:
            deserialized, deserialize_meta = self.deserialize(
                request, format=kwargs.get('format', None))
        if 'facility_id' not in deserialized:
            raise ValidationError(key='facility_id', msg='required')
        
        schema = kwargs.get('schema', None)
        if not schema:
            schema = self.build_schema()
        
        COL_NAME_POSITIVES_COUNT = 'Positives Count'
        COL_DESC_POSITIVES_COUNT = \
            'Number of times scored as positive across all %s Screens'
        COL_NAME_SCREENED_COUNT = 'Screened Count'
        COL_DESC_SCREENED_COUNT = 'Number of times screened for all %s Screens'
        
        COL_NAME_POSITIVES_TYPED_COUNT = 'Positives Count ({assay_type})'
        COL_DESC_POSITIVES_TYPED_COUNT = \
            'Number of times scored as positive across all {assay_type} Screens'
        COL_NAME_SCREENED_TYPED_COUNT = 'Screened Count ({assay_type})'
        COL_DESC_SCREENED_TYPED_COUNT = \
            'Number of times screened for all {assay_type} Screens'
        
        facility_id = deserialized['facility_id']
        data_sharing_level = deserialized.get('data_sharing_level', None)
        if data_sharing_level is None:
            deserialized['data_sharing_level'] = DSL.SHARED
        patch = True
        try: 
            current_study = Screen.objects.get(facility_id=facility_id)
        except ObjectDoesNotExist:
            patch = False    
        if patch is True:
            if 'study_type' in deserialized:
                del deserialized['study_type']
            if 'screen_type' in deserialized:
                del deserialized['screen_type']    
            
        _data = self.build_patch_detail(request, deserialized, **kwargs)
        _data = _data[API_RESULT_DATA][0]
        logger.info('study patched: %r', _data)
        study_obj = Screen.objects.get(facility_id=_data['facility_id'])
        logger.info('study created/updated: %r', study_obj)
        study_obj.date_created = _now()
        study_obj.study_type = 'in_silico'
        study_obj.save()
        
        col_desc_screened_count = COL_DESC_SCREENED_COUNT
        col_desc_positives_count = COL_DESC_POSITIVES_COUNT
        if study_obj.screen_type == 'small_molecule':
            col_desc_screened_count = col_desc_screened_count % 'Small Molecule'
            col_desc_positives_count = col_desc_positives_count % 'Small Molecule'
        elif study_obj.screen_type == 'rnai':
            col_desc_screened_count = col_desc_screened_count % 'RNAi'
            col_desc_positives_count = col_desc_positives_count % 'RNAi'
        else:
            raise ValidationError(key='screen_type', msg='Unknown type')
        
        # Create the Data Columns
        result_columns = OrderedDict((
            ('E', {
                'name': COL_NAME_POSITIVES_COUNT,
                'data_worksheet_column': 'E',
                'data_type': 'integer',
                'description': col_desc_positives_count,
            }),
            ('F', {
                'name': COL_NAME_SCREENED_COUNT,
                'data_worksheet_column': 'F',
                'data_type': 'integer',
                'description': col_desc_screened_count,
            }),
        ))
        sql = '''
        with aws as (
            select 
            well_id,
            is_positive,
            assay_type,
            screen_id
            from assay_well
            join well w using(well_id)
            join screen_result using(screen_result_id)
            join screen using(screen_id)
            where screen.screen_type = '{screen_type}'
            and screen.study_type is null
            and w.library_well_type = '{well_type}'
        )
        select
            well_id,
            count(*) as screened_count,
            count(case when is_positive then 1 end ) as positives_count
        '''
        columns = ['well_id', 'screened_count', 'positives_count']
        
        assay_type_col_to_letter = OrderedDict()
        if study_obj.screen_type == 'small_molecule':
            
            vocab_scope = 'screen.assay_type'
            assay_type_vocab = \
                self.get_vocab_resource()._get_vocabularies_by_scope(vocab_scope)
            
            if not assay_type_vocab:
                logger.warn('no vocabulary found for scope: %r, field: %r', 
                    vocab_scope, 'readout_type')
            for i,(assay_type, vocab) in enumerate(assay_type_vocab.items()):
                new_column_letter = chr(ord('F')+i*2+1)
                result_columns[new_column_letter] = {
                    'name': COL_NAME_POSITIVES_TYPED_COUNT.format(
                        assay_type=vocab['title']),
                    'data_worksheet_column': new_column_letter,
                    'data_type': 'integer',
                    'description': COL_DESC_POSITIVES_TYPED_COUNT.format(
                        assay_type=vocab['title'])
                    }
                col_name = 'positive_{assay_type}_count'.format(
                    assay_type=assay_type)
                sql_col = ", count(case when is_positive "\
                    "and assay_type='{assay_type}' then 1 end) as  " + col_name 
                sql_col = sql_col.format(assay_type=assay_type)
                sql += sql_col
                columns.append(col_name)
                assay_type_col_to_letter[col_name] = new_column_letter
                
                new_column_letter = chr(ord('F')+i*2+2)
                result_columns[new_column_letter] = {
                    'name': COL_NAME_SCREENED_TYPED_COUNT.format(
                        assay_type=vocab['title']),
                    'data_worksheet_column': new_column_letter,
                    'data_type': 'integer',
                    'description': COL_DESC_SCREENED_TYPED_COUNT.format(
                        assay_type=vocab['title'])
                    }
                col_name = 'screened_{assay_type}_count'.format(
                    assay_type=assay_type)
                sql_col = ", count(case when assay_type='{assay_type}' "\
                    "then 1 end) as  " + col_name 
                sql_col = sql_col.format(assay_type=assay_type)
                sql += sql_col
                columns.append(col_name)
                assay_type_col_to_letter[col_name] = new_column_letter
                
        sql += '''
            from aws
            group by well_id
            order by well_id '''
        sql = sql.format(screen_type=study_obj.screen_type,
                         well_type=WELL_TYPE.EXPERIMENTAL)
        
        # Create the study by iterating through the report:
        # Collate the number of confirmed positives for each Screen
        with get_engine().connect() as conn:
            
            logger.info('execute the screened_count_study...')
            logger.info('execute sql: %s', sql)
            result = conn.execute(text(sql))
            logger.info('executed the confirmed_positive_study')
            
            def result_value_generator(result):
                for row in result:
                    _dict = dict(zip(columns,row))
                    output = {
                        'well_id': _dict['well_id'],
                        'assay_well_control_type': None,
                        'E': _dict['positives_count'],
                        'F': _dict['screened_count'],
                    }
                    if assay_type_col_to_letter:
                        for assay_type_col_name, letter \
                            in assay_type_col_to_letter.items():
                            output[letter] = _dict[assay_type_col_name]
                    yield output
            logger.info('creating study values for: %r', study_obj)
            screen_result_meta = \
                self.get_screenresult_resource().create_screen_result(
                    request, study_obj, result_columns, 
                    result_value_generator(result))
            logger.info('study values created for: %r', study_obj)
        meta = {
            SCHEMA.API_MSG_RESULT: SCHEMA.API_MSG_SUCCESS
        }
        meta.update(screen_result_meta)
        if deserialize_meta:
            meta.update(deserialize_meta)
        
        if not self._meta.always_return_data:
            response = self.build_response(request, {API_RESULT_META: meta }, **kwargs)
            return response
        else:
            new_data = self._get_detail_response_internal(facility_id=facility_id)
            data = { API_RESULT_META: meta, API_RESULT_DATA: [new_data]}
            response = self.build_response(request, data, status_code=201)
            return response 

    @write_authorization
    @un_cache
    @transaction.atomic        
    def dispatch_create_confirmed_positive_study(self, request, **kwargs):
        '''
        Create/Update the Confirmed Positives Study:
        - Count of follow-up screens for well
        - M+1 columns named "N duplexes confirming positive", 
        where 0 <= N <= M, and M is the max number of duplexes per pool in any 
        library, currently = 4). The value in each column is the number of 
        follow-up screens that confirmed the well as a positive with N duplexes
        - "Weighted Average" is the average number of confirmed positives per
        screen: 
        sum((duplexes_confirming_positive)*(count of screens))/number of screens
        '''

        logger.info('create the create_confirmed_positive_study...')

        request_method = request.method.lower()
        if request_method != 'post':
            raise BadRequestError(key='method', msg='Only POST is allowed')

        convert_request_method_to_put(request)
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        deserialize_meta = None
        if kwargs.get('data', None):
            # allow for internal data to be passed
            deserialized = kwargs.pop('data')
        else:
            deserialized,deserialize_meta = self.deserialize(
                request, format=kwargs.get('format', None))
        if 'facility_id' not in deserialized:
            raise ValidationError(key='facility_id', msg='required')
        
        VOCAB_CONFIRMED_POSITIVE = '3'
        VOCAB_FALSE_POSITIVE = '2'
        COL_NAME_WEIGHTED_AVG = 'Weighted Average'
        COL_DESC_WEIGHTED_AVG = 'Average number of confirmed positives per screen'
        COL_NAME_NUMBER_SCREENS = 'Number of screens'
        COL_DESC_NUMBER_SCREENS = 'Number of follow up screens testing duplexes for the pool well'
        COL_NAME_DUPLEX_COUNT = 'Number of screens confirming with %d duplexes'
        COL_DESC_DUPLEX_COUNT = 'Number of screens confirming with %d duplexes'

        facility_id = deserialized['facility_id']
        data_sharing_level = deserialized.get('data_sharing_level', None)
        if data_sharing_level is None:
            deserialized['data_sharing_level'] = DSL.SHARED
        patch = True
        try: 
            current_study = Screen.objects.get(facility_id=facility_id)
        except ObjectDoesNotExist:
            patch = False    
        if patch is True:
            if 'study_type' in deserialized:
                del deserialized['study_type']
            if 'screen_type' in deserialized:
                del deserialized['screen_type']    
            
        _data = self.build_patch_detail(request, deserialized, **kwargs)
        _data = _data[API_RESULT_DATA][0]
        logger.info('study patched: %r', _data)
        study_obj = Screen.objects.get(facility_id=_data['facility_id'])
        logger.info('study created/updated: %r', study_obj)
        study_obj.study_type = 'in_silico'
        study_obj.date_created = _now()
        study_obj.save()
        
        # Create the Data Columns
        result_columns = OrderedDict((
            ('E', {
                'name': COL_NAME_WEIGHTED_AVG,
                'data_worksheet_column': 'E',
                'data_type': 'numeric',
                'decimal_places': 2, 
                'description': COL_DESC_WEIGHTED_AVG,
            }),
            ('F', {
                'name': COL_NAME_NUMBER_SCREENS,
                'data_worksheet_column': 'F',
                'data_type': 'integer',
                'description': COL_DESC_NUMBER_SCREENS,
            }),
        ))

        for i in range(0,5):
            data_worksheet_column_letter = chr(ord('F')+i+1)
            result_columns[data_worksheet_column_letter] = {
                'name': COL_NAME_DUPLEX_COUNT % i,
                'description': COL_DESC_DUPLEX_COUNT %i,
                'data_type': 'integer',
                'data_worksheet_column': data_worksheet_column_letter
            }
        
        # SQL report: all pool reagents -> duplex reagents -> screen -> confirmed_positive_value
        sql = '''
            with pool_reagents as (
            select r.well_id, pr.* from silencing_reagent pr
              join reagent r using(reagent_id)
              join well w using(well_id)
              join library l using(library_id)
              where l.is_pool is true
              and l.screen_type = 'rnai' )
            select 
                pr.well_id as pr_well_id, 
                pr.reagent_id as pr_id,
                dr.well_id as dr_well_id, 
                dr.reagent_id as dr_id, 
                aw.confirmed_positive_value, 
                sr.screen_id,
                s.facility_id
              from pool_reagents pr
              join reagent prr using(reagent_id)
              join well prw on(prw.well_id=prr.well_id) 
              join silencing_reagent_duplex_wells srdw on(pr.reagent_id=srdw.silencingreagent_id)
              join well dw on(dw.well_id=srdw.well_id)
              join reagent dr on(dr.well_id=dw.well_id)
              join assay_well aw on(dw.well_id=aw.well_id)
              join screen_result sr using(screen_result_id)
              join screen s using(screen_id)
              where aw.confirmed_positive_value 
                  in ('{vocab_confirmed_positive}','{vocab_false_positive}')
              order by pr_id, dr_id, sr.screen_id
        '''
            
        sql = sql.format(vocab_confirmed_positive=VOCAB_CONFIRMED_POSITIVE,
            vocab_false_positive=VOCAB_FALSE_POSITIVE)
        columns = ['pr_well_id', 'pr_id', 'dr_well_id', 'dr_id', 
            'confirmed_positive_value', 'screen_id','facility_id']
        
        # Create the study by iterating through the report:
        # Collate the number of confirmed positives for each Screen
        with get_engine().connect() as conn:
            
            logger.info('execute the confirmed_positive_study...')
            logger.debug('execute sql: %s', sql)
            result = conn.execute(text(sql))
            logger.info('executed the confirmed_positive_study')
            
            def result_value_generator(result):
                
                def create_result(well_id, screens, screen_confirmed_positives):
                    screen_count = len(screens)
                    screens_confirming_zero_duplexes = len(screens)-len(screen_confirmed_positives)
                    result_row = {
                        'well_id': well_id  ,
                        'assay_well_control_type': None,
                        'F': screen_count ,
                        'G': screens_confirming_zero_duplexes
                    }
                    
                    weighted_value_sum = 0
                    weighted_average = 0
                    pool_reagent_counts = defaultdict(int)
                    
                    for i in range(1,5):
                        data_worksheet_column_letter = chr(ord('G')+i)
                        pool_reagent_counts[i] = 0
                        screens_for_weight = len([
                            num for num in screen_confirmed_positives.values() 
                            if num==i ])
                        weighted_value_sum += i * screens_for_weight
                        result_row[data_worksheet_column_letter] = screens_for_weight
                    if weighted_value_sum > 0:
                        weighted_average = round(weighted_value_sum/(1.0*screen_count), 2)
                    result_row['E'] = weighted_average
                    logger.debug('yield row: %r', result_row)
                    return result_row
                
                screens = None
                screen_confirmed_positives = None
                current_pool_well = None
                for row in result:
                    _dict = dict(zip(columns,row))
                    logger.debug('dict: %r', _dict)
                    if current_pool_well != _dict['pr_well_id']:
                        if current_pool_well is not None:
                            yield create_result(
                                current_pool_well, screens, 
                                screen_confirmed_positives)
                        current_pool_well = _dict['pr_well_id']
                        screens = set()
                        screen_confirmed_positives = defaultdict(int)

                    screens.add(_dict['facility_id'])
                    if _dict['confirmed_positive_value'] == VOCAB_CONFIRMED_POSITIVE:
                        screen_confirmed_positives[_dict['facility_id']] += 1
                if screens:
                    yield create_result(
                        current_pool_well, screens, screen_confirmed_positives)

            logger.info('creating study values for: %r', study_obj)
            screen_result_meta = \
                self.get_screenresult_resource().create_screen_result(
                    request, study_obj, result_columns, 
                    result_value_generator(result))
            logger.info('study values created for: %r', study_obj)
        
        meta = {
            SCHEMA.API_MSG_RESULT: SCHEMA.API_MSG_SUCCESS
        }
        meta.update(screen_result_meta)
        if deserialize_meta:
            meta.update(deserialize_meta)
        if not self._meta.always_return_data:
            response = self.build_response(request, {API_RESULT_META: meta }, **kwargs)
            return response
        else:
            new_data = self._get_detail_response_internal(facility_id=facility_id)
            data = { API_RESULT_META: meta, API_RESULT_DATA: [new_data]}
            response = self.build_response(request, data, status_code=201)
            return response 

    @write_authorization
    @un_cache
    @transaction.atomic        
    def post_detail(self, request, **kwargs):
        '''
        POST is used to create or update a resource; not idempotent;
        - The LIMS client will use POST to create exclusively
        '''
        logger.info('post_detail, study')
        deserialize_meta = None
        if kwargs.get('data', None):
            # allow for internal data to be passed
            deserialized = kwargs['data']
        else:
            deserialized, deserialize_meta = self.deserialize(
                request, format=kwargs.get('format', None))

        logger.debug('patch detail %s, %s', deserialized,kwargs)

        patch_facility_id = deserialized.get('facility_id', None)
        if not patch_facility_id:    
            raise ValidationError(
                key='facility_id', msg='required')
        logger.info('new study facility id to be created: %r', patch_facility_id)
        kwargs['facility_id'] = patch_facility_id
        
        return self.patch_detail(request,**kwargs)

    @write_authorization
    @un_cache  
    @transaction.atomic      
    def patch_detail(self, request, **kwargs):
        '''
        PATCH is used to create or update a resource; not idempotent
        '''
        
        deserialize_meta = None
        if kwargs.get('data', None):
            # allow for internal data to be passed
            deserialized = kwargs.pop('data')
        else:
            deserialized, deserialize_meta = self.deserialize(
                request, format=kwargs.get('format', None))
        data_sharing_level = deserialized.get('data_sharing_level', None)
        if data_sharing_level is None:
            deserialized['data_sharing_level'] = DSL.SHARED
        _data = self.build_patch_detail(request, deserialized, **kwargs)
        return self.build_response(
            request, _data, response_class=HttpResponse, **kwargs)

class ScreensaverUserResourceAuthorization(UserResourceAuthorization):

    def get_user_screens(self, screensaver_user):
        my_screens = Screen.objects.all().filter(
            Q(lead_screener=screensaver_user)
            | Q(lab_head=screensaver_user)
            | Q(collaborators=screensaver_user))
        logger.info('user: %r, screens: %r', 
            screensaver_user.username, [s.facility_id for s in my_screens])
        return [screen for screen in my_screens]
    
    def get_associated_users(self, user):
        ''' collaborators '''
        
        screensaver_user = ScreensaverUser.objects.get(username=user.username)
        associated_users = set([screensaver_user])
        
        for screen in self.get_user_screens(screensaver_user):
            associated_users.add(screen.lead_screener)
            associated_users.add(screen.lab_head)
            associated_users.update([su for su in screen.collaborators.all()])

        logger.info('user: %r, associated users: %r', user.username, associated_users)
        
        return associated_users
    
    def filter(self, user, filter_expression):

        if self.is_restricted_view(user) is False:
            return filter_expression

        associated_users = \
            self.get_associated_users(user)
        auth_filter = column('screensaver_user_id').in_([
                user.screensaver_user_id for user in associated_users])
        
        if filter_expression is not None:
            filter_expression = and_(filter_expression, auth_filter)
        else:
            filter_expression = auth_filter

        return filter_expression


    def get_row_property_generator(self, user, fields, extant_generator):
        
        # TODO: consider allowing access to own data for "view_groups" restricted fields
        
        return extant_generator
    
        
class UserChecklistAuthorization(ScreensaverUserResourceAuthorization):        

    def _is_resource_authorized(
        self, user, permission_type, **kwargs):
        authorized = \
            super(UserChecklistAuthorization, self)\
                ._is_resource_authorized(user, permission_type, **kwargs)
        if authorized is True:
            return True
        
        return user.is_active

    def filter(self, user, filter_expression):
        
        if self.is_restricted_view(user) is False:
            return filter_expression
        
        auth_filter = column('username') == user.username
        if filter_expression is not None:
            filter_expression = and_(filter_expression, auth_filter)
        else:
            filter_expression = auth_filter

        return filter_expression

    def get_row_property_generator(self, user, fields, extant_generator):
        # If the user may see the CPR, there are no property restrictions
        return extant_generator

    
class UserChecklistResource(DbApiResource):

    class Meta:
        
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'userchecklist'
        authorization = UserChecklistAuthorization(resource_name)
        serializer = LimsSerializer()

    def __init__(self, **kwargs):
        
        self.screensaveruser_resource = None
        super(UserChecklistResource, self).__init__(**kwargs)

    def get_su_resource(self):
        if self.screensaveruser_resource is None:
            self.screensaveruser_resource = ScreensaverUserResource()
        return self.screensaveruser_resource
    
    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url((r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))/" 
                 r"(?P<name>([\w_]+))%s$")
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_list'), name="api_dispatch_list"),
        ]    

    @read_authorization
    def get_detail(self, request, **kwargs):

        name = kwargs.get('name', None)
        if not name:
            raise MissingParam('name')
        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, **kwargs):

        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        
        is_for_detail = kwargs.pop('is_for_detail', False)
        checklist_name = param_hash.pop('name', None)
        if checklist_name:
            param_hash['name__eq'] = checklist_name
        
        # general setup
        
        manual_field_includes = set(param_hash.get('includes', []))
        manual_field_includes.add('is_checked');
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filename = self._get_filename(
            readable_filter_hash, schema, is_for_detail)
        filter_expression = \
            self._meta.authorization.filter(request.user,filter_expression)
              
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
         
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)
        
        # specific setup
        #             _su = self.bridge['screensaver_user']
        _user_cte = ScreensaverUserResource.get_user_cte().cte('cl_user')
        _admin_cte = ScreensaverUserResource.get_user_cte().cte('cl_admin')
        #             _admin = _su.alias('admin')
        _up = self.bridge['reports_userprofile']
        _user_checklist = self.bridge['user_checklist']
        _vocab = self.bridge['reports_vocabulary']
        
        # get the checklist vocabulary
        checklist_table = (
            select([
                _vocab.c.ordinal,
                _vocab.c.key.label('name'),
                _vocab.c.is_retired.label('is_retired'),
            ])
            .select_from(_vocab)
            .where(_vocab.c.scope == 'userchecklist.name'))
        checklist_table = Alias(checklist_table)
        
        # build the entered checklists
        
        j = _user_checklist
        j = j.join(
            _user_cte, _user_checklist.c.screensaver_user_id 
                == _user_cte.c.screensaver_user_id)
        j = j.join(
            _admin_cte, _user_checklist.c.admin_user_id 
                == _admin_cte.c.screensaver_user_id)
        entered_checklists = select([
            _user_cte.c.screensaver_user_id,
            _user_cte.c.username,
            _user_cte.c.name.label('user_name'),
            _user_cte.c.first_name.label('user_first_name'),
            _user_cte.c.last_name.label('user_last_name'),
            _user_checklist.c.name,
            _user_checklist.c.is_checked,
            _user_checklist.c.date_effective,
            _user_checklist.c.date_notified,
            _admin_cte.c.username.label('admin_username'),
            _admin_cte.c.name.label('admin_name'),
            
            ]).select_from(j)
        username = param_hash.pop('username', None)
        if username:
            entered_checklists = entered_checklists.where(
                _user_cte.c.username == username)
        screensaver_user_id = param_hash.pop('screensaver_user_id', None)
        if screensaver_user_id:
            entered_checklists = entered_checklists.where(
                _user_cte.c.screensaver_user_id == screensaver_user_id)
        entered_checklists = entered_checklists.cte('entered_checklists')
        
        # This entire query doesn't fit the pattern, construct it manually
        custom_columns = {
            'screensaver_user_id': func.coalesce(
                entered_checklists.c.screensaver_user_id, screensaver_user_id),
            'username': func.coalesce(
                entered_checklists.c.username, username),
            'user_name': entered_checklists.c.user_name,
            'user_first_name': entered_checklists.c.user_first_name,
            'user_last_name': entered_checklists.c.user_last_name,
            'admin_username': entered_checklists.c.admin_username,
            'admin_name': entered_checklists.c.admin_name,
            'name' : checklist_table.c.name,
            'is_checked': func.coalesce(
                entered_checklists.c.is_checked, False),
            'status': case([
                ( entered_checklists.c.is_checked == True, 'activated')],
                else_=case([
                    (entered_checklists.c.date_effective != None, 'deactivated')],
                    else_='not_completed')) ,
            'date_effective': entered_checklists.c.date_effective,
            'date_notified': entered_checklists.c.date_notified,
            'is_retired': checklist_table.c.is_retired,
            'ordinal': checklist_table.c.ordinal,
            }
        
        base_query_tables = ['user_checklist', 'screensaver_user'] 
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)
        
        isouter = False
        if username is not None or screensaver_user_id is not None:
            # if username/suid, then this is a user specific view:
            # - outer join in the two so that a full list is generated
            isouter = True
            
        j = checklist_table
        j = j.join(
            entered_checklists,
            checklist_table.c.name == entered_checklists.c.name,
            isouter=isouter)
        
        stmt = select(columns.values()).select_from(j)
        if username is None and screensaver_user_id is None:
            stmt = stmt.order_by(
                entered_checklists.c.user_last_name, 
                entered_checklists.c.user_first_name)
        stmt = stmt.order_by(checklist_table.c.ordinal)
        # general setup
        if 'is_retired' not in readable_filter_hash:
            stmt = stmt.where(
                func.coalesce(checklist_table.c.is_retired,False) != True)
         
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        
        # compiled_stmt = str(stmt.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)
        
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
        
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None),
            use_caching=True)

    @write_authorization
    @un_cache
    @transaction.atomic
    def delete_obj(self, request, deserialized, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'delete_obj')

    @write_authorization
    @transaction.atomic()
    def patch_obj(self, request, deserialized, **kwargs):
        
        logger.info('patch checklist item: %r', deserialized)
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        fields = schema['fields']
        
        screensaver_user_id = deserialized.get('screensaver_user_id', None)
        if not screensaver_user_id:
            screensaver_user_id = kwargs.get('screensaver_user_id', None)
        if not screensaver_user_id:
            raise ValidationError(key='screensaver_user_id', msg='required')
        item_name = deserialized.get('name', None)
        if not item_name:
            raise ValidationError(key='name', msg='required')

        try:
            user = ScreensaverUser.objects.get(screensaver_user_id=screensaver_user_id)
        except ObjectDoesNotExist:
            raise ValidationError(
                key='screensaver_user_id',
                msg='screensaver_user_id does not exist: %r' % screensaver_user_id)
        create = False
        try:
            uci = UserChecklist.objects.get(
                screensaver_user=user,
                name=item_name)
            patch = True
            logger.info('UserChecklist to patch: %r', uci)
        except ObjectDoesNotExist:
            logger.info(
                'UserChecklist does not exist: %s/%s, creating' 
                % (screensaver_user_id, item_name))
            uci = UserChecklist(
                screensaver_user=user,
                name=item_name)
            create = True

        initializer_dict = self.parse(deserialized, create=create, schema=schema)
        errors = self.validate(initializer_dict, patch=not create, schema=schema)
        if errors:
            raise ValidationError(errors)

        admin_username = deserialized.get('admin_username', None)
        if not admin_username:
            raise ValidationError(
                key='admin_username',
                msg='required')
        try:
            admin_user = ScreensaverUser.objects.get(username=admin_username)
            initializer_dict['admin_user_id'] = admin_user.pk
            # Note, hasattr does not work for foreign keys if not initialized, 
            # must use the key
        except ObjectDoesNotExist:
            raise ValidationError(
                key='admin_username',
                msg='username does not exist: %r' % admin_username)

        for key, val in initializer_dict.items():
            if hasattr(uci, key):
                setattr(uci, key, val)
        
        uci.save()
        logger.info('UserChecklist %r created: %r', uci, create)
        return { API_RESULT_OBJ: uci }


class LabAffiliationResource(DbApiResource):
    '''
    Manages the Lab Affiliations
    '''

    class Meta:

        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'labaffiliation'
        authorization = UserGroupAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        max_limit = 10000
        always_return_data = True

    def __init__(self, **kwargs):
        
        super(LabAffiliationResource, self).__init__(**kwargs)
        self.su_resource = None
        
    def get_screensaver_resource(self):
        if self.su_resource is None:
            self.su_resource = ScreensaverUserResource()
        return self.su_resource

    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)/(?P<lab_affiliation_id>([\d]+))%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            ]

    @read_authorization
    def get_detail(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, **kwargs):

        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        
        is_for_detail = kwargs.pop('is_for_detail', False)

        try:
            
            # general setup
            
            manual_field_includes = set(param_hash.get('includes', []))
            exact_fields = set(param_hash.get('exact_fields', []))
        
            (filter_expression, filter_hash, readable_filter_hash) = \
                SqlAlchemyResource.build_sqlalchemy_filters(
                    schema, param_hash=param_hash)
            filename = self._get_filename(
                readable_filter_hash, schema, is_for_detail)
            filter_expression = \
                self._meta.authorization.filter(request.user,filter_expression)
                  
            order_params = param_hash.get('order_by', [])
            field_hash = self.get_visible_fields(
                schema['fields'], filter_hash.keys(), manual_field_includes,
                param_hash.get('visibilities'),
                exact_fields=set(param_hash.get('exact_fields', [])),
                order_params=order_params)
            order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
                order_params, field_hash)
             
            rowproxy_generator = None
            if use_vocab is True:
                rowproxy_generator = \
                    DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
            rowproxy_generator = \
                self._meta.authorization.get_row_property_generator(
                    request.user, field_hash, rowproxy_generator)
 
            # specific setup
            _la = self.bridge['lab_affiliation']
            _lab_su = self.bridge['screensaver_user']
            
            _lab_head_cte = self.get_screensaver_resource().get_user_cte().cte('la_lab_heads')
            
            custom_columns = {
                'lab_head_ids': (
                    select([
                        func.array_to_string(
                            func.array_agg(_lab_head_cte.c.screensaver_user_id),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(_lab_su.join(
                        _lab_head_cte, 
                        _lab_su.c.screensaver_user_id
                            ==_lab_head_cte.c.screensaver_user_id))
                    .where(_lab_su.c.lab_affiliation_id 
                        == literal_column('lab_affiliation.lab_affiliation_id'))),
                'lab_head_names': (
                    select([
                        func.array_to_string(
                            func.array_agg(_lab_head_cte.c.name),
                            LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(_lab_su.join(
                        _lab_head_cte, 
                        _lab_su.c.screensaver_user_id
                            ==_lab_head_cte.c.screensaver_user_id))
                    .where(_lab_su.c.lab_affiliation_id
                        == literal_column('lab_affiliation.lab_affiliation_id'))),
                }

            # delegate to the user resource
            base_query_tables = [
                'lab_affiliation'] 
            columns = self.build_sqlalchemy_columns(
                field_hash.values(), base_query_tables=base_query_tables,
                custom_columns=custom_columns)

            # build the query statement
            
            j = _la
            
            stmt = select(columns.values()).select_from(j)
            # natural ordering
            stmt = stmt.order_by(_la.c.category, _la.c.name)
            
            # general setup
             
            (stmt, count_stmt) = self.wrap_statement(
                stmt, order_clauses, filter_expression)
            # logger.info(
            #     'stmt: %s',
            #     str(stmt.compile(
            #         dialect=postgresql.dialect(),
            #         compile_kwargs={"literal_binds": True})))
            title_function = None
            if use_titles is True:
                def title_function(key):
                    return field_hash[key]['title']
            if is_data_interchange:
                title_function = DbApiResource.datainterchange_title_function(
                    field_hash,schema['id_attribute'])
            
            return self.stream_response_from_statement(
                request, stmt, count_stmt, filename,
                field_hash=field_hash,
                param_hash=param_hash,
                is_for_detail=is_for_detail,
                rowproxy_generator=rowproxy_generator,
                title_function=title_function, meta=kwargs.get('meta', None),
                use_caching=True)
             
        except Exception, e:
            logger.exception('on get_list')
            raise e  
        
    @write_authorization
    @transaction.atomic()
    def patch_obj(self, request, deserialized, **kwargs):
        
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        fields = schema['fields']
        
        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
        if not id_kwargs:
            # Look for alternate identifier; unique field "name"
            lab_affiliation_name = deserialized.get('name')
            if lab_affiliation_name:
                try:
                    lab_affiliation  = \
                        LabAffiliation.objects.get(name=lab_affiliation_name)
                    id_kwargs = { 'lab_affiliation_id': lab_affiliation.lab_affiliation_id }
                    logger.info('found lab_affiliation by name: %r', lab_affiliation)
                except ObjectDoesNotExist:
                    logger.info('affiliation_name does not exist: %r', lab_affiliation_name)
        
        patch = bool(id_kwargs)
        initializer_dict = self.parse(deserialized, schema=schema, create=not patch)
        errors = self.validate(initializer_dict, schema=schema, patch=patch)
        if errors:
            raise ValidationError(errors)

        lab_affiliation = None
        if patch is True:
            try:
                lab_affiliation = LabAffiliation.objects.get(**id_kwargs)
            except ObjectDoesNotExist:
                raise Http404(
                    'Lab Affiliation does not exist for: %r', id_kwargs)
        else:
            lab_affiliation = LabAffiliation.objects.create(**id_kwargs)

        for key, val in initializer_dict.items():
            if hasattr(lab_affiliation, key):
                setattr(lab_affiliation, key, val)
        
        lab_affiliation.save()
        return { API_RESULT_OBJ: lab_affiliation }


class ScreensaverUserResource(DbApiResource):    
    
    VOCAB_USER_AGREEMENT_RNAI = UserAgreementResource.VOCAB_USER_AGREEMENT_RNAI
    VOCAB_USER_AGREEMENT_SM = UserAgreementResource.VOCAB_USER_AGREEMENT_SM

    class Meta:

        # queryset = ScreensaverUser.objects.all()
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'screensaveruser'
        authorization = ScreensaverUserResourceAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        excludes = ['digested_password']
        max_limit = 10000
        always_return_data = True
        # TODO: utilize the cache_control mechanism to signal cache status
        # to the client (max-age, etag)
        # cache = SimpleCache(timeout=10)

    def __init__(self, **kwargs):
        
        self.user_resource = None
        self.useragreement_resource = None
        super(ScreensaverUserResource, self).__init__(**kwargs)

    def get_ua_resource(self):
        if self.useragreement_resource is None:
            self.useragreement_resource = UserAgreementResource()
        return self.useragreement_resource
    
    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)/(?P<username>([\w]+))%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))/groups%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_groupview'),
                name="api_dispatch_user_groupview"),
            url(r"^(?P<resource_name>%s)/(?P<username>([\w]+))/groups%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_groupview'),
                name="api_dispatch_user_groupview"),
            url(r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))/checklist%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_checklistview'),
                name="api_dispatch_user_checklistview"),
            url(r"^(?P<resource_name>%s)/(?P<username>([\w]+))/checklist%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_checklistview'),
                name="api_dispatch_user_checklistview"),
            url(r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))/attachedfiles%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_attachedfileview'),
                name="api_dispatch_user_attachedfileview"),
            url(r"^(?P<resource_name>%s)/(?P<username>([\w]+))/attachedfiles%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_attachedfileview'),
                name="api_dispatch_user_attachedfileview"),
            url(r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))/useragreement%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_useragreement_view'),
                name="api_dispatch_useragreement_view"),
            url(r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))/useragreement/" 
                r"(?P<type>(\w+))%s$"
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_useragreement_view'),
                name="api_dispatch_useragreement_view"),
            url(r"^(?P<resource_name>%s)/(?P<username>([\w]+))/useragreement%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_useragreement_view'),
                name="api_dispatch_useragreement_view"),
            url(r"^(?P<resource_name>%s)/(?P<username>([\w]+))/useragreement/"
                r"(?P<type>(\w+))%s$"
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_useragreement_view'),
                name="api_dispatch_useragreement_view"),
            url((r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))"
                 r"/attachedfiles/(?P<attached_file_id>([\d]+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_attachedfiledetailview'),
                name="api_dispatch_user_attachedfiledetailview"),
            url((r"^(?P<resource_name>%s)/(?P<username>([\w]+))"
                 r"/attachedfiles/(?P<attached_file_id>([\d]+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_attachedfiledetailview'),
                name="api_dispatch_user_attachedfiledetailview"),
            url(r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))/serviceactivities%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_serviceactivityview'),
                name="api_dispatch_user_serviceactivityview"),
            url((r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))"
                 r"/serviceactivities/(?P<activity_id>([\d]+))%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_serviceactivitydetailview'),
                name="api_dispatch_user_serviceactivitydetailview"),
            url(r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))/activities%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_activityview'),
                name="api_dispatch_user_activityview"),
            url(r"^(?P<resource_name>%s)/(?P<screensaver_user_id>([\d]+))/screens%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_user_screenview'),
                name="api_dispatch_user_screenview"),
        ]    

    def dispatch_user_groupview(self, request, **kwargs):
            
        screensaver_user_id = kwargs.get('screensaver_user_id')
        if screensaver_user_id:
            su = ScreensaverUser.objects.get(screensaver_user_id=screensaver_user_id)
            if not su.username:
                raise Http404(
                    'Specified user is not a login user (no username): {}'.format( 
                    screensaver_user_id))
            kwargs['username'] = su.username
        return UserGroupResource().dispatch('list', request, **kwargs)    
    
    def dispatch_user_checklistview(self, request, **kwargs):
        return UserChecklistResource().dispatch('list', request, **kwargs)    
    
    def dispatch_user_attachedfileview(self, request, **kwargs):
        method = 'list'
        if request.method.lower() == 'post':
            # if put is used, force to "post_detail"
            method = 'detail'
        return AttachedFileResource().dispatch(method, request, **kwargs)   
    
    @write_authorization
    @un_cache        
    @transaction.atomic    
    def update_user_agreement(self, request, screensaver_user_id, **kwargs): 
        ''' Update the User Agreement for the user and provide managment functions:
        - invalidate Lab Members if data sharing level is different,
        - add/remove login capabilities on activate/deactivate
        '''

        logger.info('update_user_agreement: %r', screensaver_user_id)

        if request.method.lower() not in ['post','patch']:
            raise Exception('Only POST/PATCH is allowed for user agreement update')
            
        kwargs_for_user = {
            'exact_fields': ['screensaver_user_id', 'is_active',
                'sm_data_sharing_level', 'rnai_data_sharing_level',
                'lab_head_id', 'username','ecommons_id', 'classification'] 
        }
        kwargs_for_user['screensaver_user_id'] = screensaver_user_id
        original_user_data = self._get_detail_response_internal(**kwargs_for_user)
        if not original_user_data:
            msg = 'User not found: %r'
            raise ValidationError({
                'screensaver_user_id': msg % screensaver_user_id,
                'username': msg % username
            })
        parent_log = self.make_log(
            request, attributes=original_user_data, api_action='PATCH')
        parent_log.save()
        
        logger.info('perform the POST/PATCH operation on the UA resource...')
        
        # 1. Perform the User Agreement Patch
        # TODO: response type may not be JSON
        method = 'detail'
        response = self.get_ua_resource().dispatch(
            method, request, parent_log=parent_log, 
            screensaver_user_id=screensaver_user_id, **kwargs)    
        _data = self.get_serializer().deserialize(
            LimsSerializer.get_content(response), JSON_MIMETYPE)
        _meta = _data.get(API_RESULT_META, {})
        logger.debug('original meta: %r', _meta)
        user_agreement_data = _data.get(API_RESULT_DATA)
        assert len(user_agreement_data) == 1, 'too many objects returned'
        user_agreement_data = user_agreement_data[0]
        logger.debug('ua data: %r', user_agreement_data)

        logger.info('user agreement POST/PATCH completed')

        user_data = self._get_detail_response_internal(
            screensaver_user_id=screensaver_user_id)

        
        # 2. Perform checks for user:
        # 2.a activate if username is set and UA is active
        extra_meta = {}
        if user_data.get('is_active') is not True:
            if user_agreement_data.get('date_active') is not None \
                    and user_agreement_data.get('date_expired') is None:
                if not original_user_data.get('username')\
                    and not original_user_data.get('ecommons_id'):
                    extra_meta['Note'] = \
                        'Login capability not set: User must have an '\
                        'eCommons ID or Username to enable login.'
                else:
                    logger.info('UserAgreement is active: %r, add login access for user')
                    # NOTE: is_active is a reports.user property and will only be 
                    # set if the user has a ecommonsId or username set
                    user_schema = self.build_schema(request.user)
                    user_patch_data = {
                        'screensaver_user_id': screensaver_user_id,
                        'is_active': True }
                    logger.info('User Agreement: Patch "is_active=true" for user: %r...', 
                        screensaver_user_id)
                    patch_result = self.patch_obj(
                        request, user_patch_data, schema=user_schema)
                    extra_meta.update(patch_result.get(API_RESULT_META))
                    logger.info('ScreensverUser "is_active" Patch result: %r', patch_result)
                    
                    parent_log.diffs['is_active'] = [False, True]
                    parent_log.save()
        else:
            
            # 2.b If all user agreements are inactive, turn off login capability
            active_user_agreements = UserAgreement.objects\
                .filter(screensaver_user_id=screensaver_user_id)\
                .filter(date_active__isnull=False)\
                .filter(date_expired__isnull=True)
            if not active_user_agreements.exists():
            
                user_patch_data = {
                    'screensaver_user_id': screensaver_user_id,
                    'is_active': False }
                logger.info(
                    'no active user agreements exist for %r, removing is_active...', 
                    screensaver_user_id)
                patch_result = self.patch_obj(request, user_patch_data, **kwargs)
                
                logger.info('ScreensverUser patch result: %r', patch_result)
                extra_meta['Account deactivated'] = 'true'
                parent_log.diffs['is_active'] = [ True, False ]
                parent_log.save()
            else:
                logger.info('preserve "is_active" flag for: %r, active user agreements exist: %r',
                    screensaver_user_id, [x for x in active_user_agreements.all()])

        # Lab head checks
        # 2.c deactivate lab members if is lab head and DSL changes
        current_dsl = user_agreement_data.get('data_sharing_level')
        current_type = user_agreement_data.get('type')
        PI_ROLE = SCHEMA.VOCAB.screensaver_user.classification.PRINCIPAL_INVESTIGATOR
        if original_user_data.get('classification') != PI_ROLE:
            lab_head_id = original_user_data.get('lab_head_id')
            # 1. Disallow setting DSL to a level different than the Lab Head
            if current_dsl:
                try:
                    lh_user_agreement = UserAgreement.objects.get(
                        screensaver_user_id=lab_head_id, 
                        type=current_type)
                    if current_dsl != lh_user_agreement.data_sharing_level:
                        raise ValidationError(
                            key='data_sharing_level',
                            msg='Must match Lab Head value: %r' 
                                % lh_user_agreement.data_sharing_level)    
                except ObjectDoesNotExist:
                    logger.info('no lab head user agreement found')
                    raise ValidationError(
                        key='status',
                        msg='User may not be active until their lab head has a user agreement')
        else:
            # 2. If lab head DSL != lab_member DSL's, require override 
            # and deactivate lab_member DSLs
            param_hash = self._convert_request_to_dict(request)
            param_hash.update(kwargs)
            override_param = parse_val(
                param_hash.get(API_PARAM_OVERRIDE, False),
                    API_PARAM_OVERRIDE, 'boolean')
            lab_member_uas_to_deactivate = UserAgreement.objects\
                .filter(
                    screensaver_user__lab_head_id=screensaver_user_id, 
                    type__exact=current_type,
                    date_active__isnull=False,
                    date_expired__isnull=True)\
                .exclude(screensaver_user_id=screensaver_user_id)\
                .exclude(data_sharing_level=current_dsl)
            
            uas_to_deactivate = []
            for ua in lab_member_uas_to_deactivate.all():
                logger.info('lab member ua to deactivate: %r, override: %r', 
                    ua, override_param)
                uas_to_deactivate.append(
                    '({}) {} {}, level: {}'.format(
                        ua.screensaver_user.screensaver_user_id,
                        ua.screensaver_user.first_name,
                        ua.screensaver_user.last_name,
                        ua.data_sharing_level))
                if override_param is True:
                    user_agreement_input = {
                        'type': current_type,
                        'screensaver_user_id': ua.screensaver_user_id,
                        'status': SCHEMA.VOCAB.user_agreement.status.INACTIVE
                        }
                    logger.info('deactivate lab member UA: %r', user_agreement_data)
                    result = self.get_ua_resource()._patch_detail_internal(
                        user_agreement_input, request.user, parent_log=parent_log)
                    # Will raise exception on fail
                    
                    # 20181011 - TODO: if lab member has no active agreements,
                    # should remove "is_active" flag?
                    
            if uas_to_deactivate:
                if override_param is not True:
                    raise ValidationError({
                        'Message': (
                            'Setting the Lab Head Data Sharing Level to [%s] will '
                            'invalidate the Lab Member Agreements with '
                            'differing Data Sharing Levels') % str(current_dsl),
                        API_PARAM_OVERRIDE: 'required',
                        'Lab Member Agreements to deactivate': uas_to_deactivate
                    })
                else:
                    extra_meta['Users Deactivated'] = ', '.join(uas_to_deactivate)
        
        new_user_data = self._get_detail_response_internal(**kwargs_for_user)
        self.log_patch(request, original_user_data, new_user_data, log=parent_log)
        parent_log.save()
        
        _meta.update(extra_meta)
        
        data = { API_RESULT_DATA: [user_agreement_data,], API_RESULT_META: _meta }
        return self.build_response(request, data)

    def dispatch_useragreement_view(self, request, screensaver_user_id=None, username=None, **kwargs):
        if username is not None:
            su = ScreensaverUser.objects.get(username=username)
            screensaver_user_id = su.screensaver_user_id

        if request.method.lower() in ['post','patch']:
            logger.info('%r update user agreement...', request.method)
            return self.update_user_agreement(request, screensaver_user_id)
        else:
            logger.info('dispatching: %r', request.method)
            return self.get_ua_resource().dispatch(
                'list', request, 
                screensaver_user_id=screensaver_user_id, **kwargs)    

    def dispatch_user_attachedfiledetailview(self, request, **kwargs):
        return AttachedFileResource().dispatch('detail', request, **kwargs)    
    
    def dispatch_user_serviceactivityview(self, request, **kwargs):
        kwargs['serviced_user_id__eq'] = kwargs.pop('screensaver_user_id')
        # kwargs['classification__ne'] = SCHEMA.VOCAB.activity.classification.SCREENING
        return ActivityResource().dispatch('list', request, **kwargs)    
    
    def dispatch_user_serviceactivitydetailview(self, request, **kwargs):
        return ActivityResource().dispatch('detail', request, **kwargs)    
    
    def dispatch_user_activityview(self, request, **kwargs):
        screensaver_user_id = kwargs.pop('screensaver_user_id')
        
        # TODO: 20171111 - performance
        # Can be made more performant by directly joining user to activity table
        nested_search_data = [
            { 'serviced_user_id__eq' : screensaver_user_id},
            {'performed_by_user_id__eq': screensaver_user_id},
        ]
        
        kwargs[SCHEMA.API_PARAM_NESTED_SEARCH] = nested_search_data
        return ActivityResource().dispatch('list', request, **kwargs)    

    def dispatch_user_screenview(self, request, **kwargs):
        kwargs['screens_for_userid'] = kwargs.pop('screensaver_user_id')
        return ScreenResource().dispatch('list', request, **kwargs)    

    def build_schema(self, user=None, **kwargs):
        
        schema = super(ScreensaverUserResource, self).build_schema(user=user, **kwargs)
        sub_schema = self.get_user_resource().build_schema(user=user, **kwargs);
        fields = {}
        fields.update(sub_schema['fields'])
        for key, val in schema['fields'].items():
            if key in fields:
                fields[key].update(val)
            else:
                fields[key] = val
        schema['fields'] = fields
        logger.debug('=== final screensaver_user fields: %r', fields)
        return schema
    
    @classmethod
    def get_user_cte(cls):
        
        bridge = get_tables()
        _su = bridge['screensaver_user']
        
        j = _su
        user_table = (
            select([
                _su.c.screensaver_user_id,
                _su.c.username,
                _concat(_su.c.first_name, ' ', _su.c.last_name).label('name'),
                _concat(_su.c.last_name, ', ', _su.c.first_name).label('last_first'),
                _su.c.first_name,
                _su.c.last_name,
                _su.c.email,
                _su.c.lab_head_id,
                _su.c.classification
                ])
            .select_from(j))
        return user_table
        
    @classmethod
    def get_lab_head_cte(cls, alias_qualifier=''):
        bridge = get_tables()
        _su = bridge['screensaver_user']
        _user = ScreensaverUserResource.get_user_cte().cte(
            'lab_head_users_%s'%alias_qualifier)
        affiliation_table = bridge['lab_affiliation']
        _vocab = bridge['reports_vocabulary']
        la_categories = (
            select([_vocab.c.key, _vocab.c.scope, _vocab.c.title ])
                .select_from(_vocab)
                .where(_vocab.c.scope=='labaffiliation.category')).cte(
                    'labaffiliation_category')
        
        
        lab_head_table = (
            select([
                _user.c.screensaver_user_id,
                _user.c.name,
                _user.c.username,
                affiliation_table.c.lab_affiliation_id,
                affiliation_table.c.name.label('lab_affiliation_name'),
                affiliation_table.c.category.label('lab_affiliation_category'),
                _concat(
                    affiliation_table.c.name, ' (',
                    la_categories.c.title, ')').label('lab_affiliation'),
                func.array_to_string(
                    array([
                        _user.c.last_first,
                        ' - ', affiliation_table.c.name,
                        ' (', la_categories.c.title, ')']), '').label('lab_name_full'),
                ])
            .select_from(
                _user.join(
                    _su, _user.c.screensaver_user_id==_su.c.screensaver_user_id)
                .join(affiliation_table, 
                    _su.c.lab_affiliation_id==affiliation_table.c.lab_affiliation_id,
                    isouter=True )
                .join(la_categories, affiliation_table.c.category==la_categories.c.key))
            .where(_su.c.classification=='principal_investigator')
            )
        return lab_head_table
        
    def get_user_resource(self):

        if not self.user_resource:
            self.user_resource = UserResource()
        return self.user_resource
        
    @read_authorization
    def get_detail(self, request, **kwargs):

        screensaver_user_id = kwargs.get('screensaver_user_id', None)
        username = kwargs.get('username', None)
        ecommons_id = kwargs.get('ecommons_id', None)
        if screensaver_user_id is None and username is None and ecommons_id is None:
            msg = 'must provide a screensaver_user_id or username parameter'
            raise BadRequest({'username': msg, 'screensaver_user_id': msg })

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, **kwargs):

        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        
        is_for_detail = kwargs.pop('is_for_detail', False)

        # general setup
            
        manual_field_includes = set(param_hash.get('includes', []))
        exact_fields = set(param_hash.get('exact_fields', []))
        
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        
        extra_file_params = {}
        if 'is_staff' in readable_filter_hash:
            if readable_filter_hash['is_staff'].lower() == 'false':
                extra_file_params['screener'] = ''
            else:
                extra_file_params['staff'] = ''
        filename = self._get_filename(
            readable_filter_hash, schema, is_for_detail, **extra_file_params)

        filter_expression = self._meta.authorization.filter(
            request.user, filter_expression)
              
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
             
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)
 
        # specific setup
        _su = self.bridge['screensaver_user']
        _au = self.bridge['auth_user']
        _up = self.bridge['reports_userprofile']
        _s = self.bridge['screen']
        _screen_collab = self.bridge['screen_collaborators']
        _lhsu = _su.alias('lhsu')
        _user_agreement = self.bridge['user_agreement']

        lab_head_table = ScreensaverUserResource.get_lab_head_cte().cte('lab_heads')
        lab_member = ScreensaverUserResource.get_user_cte().cte('users_labmember')
            
        _all_screens = union(
            select([
                _screen_collab.c.screensaveruser_id.label('screensaver_user_id'),
                _screen_collab.c.screen_id,
                _s.c.facility_id, 
                literal_column("'collaborator'").label('role')])
            .select_from(_screen_collab.join(
                _s, _s.c.screen_id==_screen_collab.c.screen_id)),
            select([
                _s.c.lead_screener_id.label('screensaver_user_id'),
                _s.c.screen_id, 
                _s.c.facility_id, 
                literal_column("'lead_screener'").label('role')]),
            select([
                _s.c.lab_head_id.label('screensaver_user_id'),
                _s.c.screen_id,
                _s.c.facility_id, 
                literal_column("'lab_head'").label('role')])
        ).order_by('facility_id').cte('all_screens')
                
            
        custom_columns = {
            'is_staff': func.coalesce(_au.c.is_staff, False),
            'is_active': func.coalesce(_au.c.is_active, False),
            'is_superuser': func.coalesce(_au.c.is_superuser, False),
            'username': _au.c.username,
            'name': _concat_with_sep(
                args=[_su.c.last_name,_su.c.first_name,], sep=', '),
            'screens': (
                select([func.array_to_string(
                    func.array_agg(_all_screens.c.facility_id),
                    LIST_DELIMITER_SQL_ARRAY)])
                .select_from(_all_screens)
                .where(_all_screens.c.screensaver_user_id
                    ==_su.c.screensaver_user_id)
                ),
            'screens_lab_head': (
                select([func.array_to_string(
                    func.array_agg(_all_screens.c.facility_id),
                    LIST_DELIMITER_SQL_ARRAY)])
                .select_from(_all_screens)
                .where(_all_screens.c.screensaver_user_id
                    ==_su.c.screensaver_user_id)
                .where(_all_screens.c.role=='lab_head')
                ),
            'screens_lead': (
                select([func.array_to_string(
                    func.array_agg(_all_screens.c.facility_id),
                    LIST_DELIMITER_SQL_ARRAY)])
                .select_from(_all_screens)
                .where(_all_screens.c.screensaver_user_id
                    ==_su.c.screensaver_user_id)
                .where(_all_screens.c.role=='lead_screener')
                ),
            'screens_collaborator': (
                select([func.array_to_string(
                    func.array_agg(_all_screens.c.facility_id),
                    LIST_DELIMITER_SQL_ARRAY)])
                .select_from(_all_screens)
                .where(_all_screens.c.screensaver_user_id
                    ==_su.c.screensaver_user_id)
                .where(_all_screens.c.role=='collaborator')
                ),
            'lab_name': lab_head_table.c.lab_name_full,
            'lab_head_id': lab_head_table.c.screensaver_user_id,
            'lab_head_username': lab_head_table.c.username,
            'lab_member_ids': (
                select([
                    func.array_to_string(
                        func.array_agg(literal_column('screensaver_user_id')),
                        LIST_DELIMITER_SQL_ARRAY)])
                .select_from(
                    select([lab_member.c.screensaver_user_id])
                    .select_from(lab_member)
                    .order_by(lab_member.c.last_first)
                    .where(lab_member.c.lab_head_id
                        ==literal_column('screensaver_user.screensaver_user_id'))
                    .where(lab_member.c.lab_head_id!=lab_member.c.screensaver_user_id)
                    .alias('inner'))        
                ),
            'lab_member_names': (
                select([
                    func.array_to_string(
                        func.array_agg(literal_column('name')),
                        LIST_DELIMITER_SQL_ARRAY)])
                .select_from(
                    select([lab_member.c.name, lab_member.c.email])
                    .select_from(lab_member)
                    .order_by(lab_member.c.last_first)
                    .where(lab_member.c.lab_head_id
                        ==literal_column('screensaver_user.screensaver_user_id'))
                    .where(lab_member.c.lab_head_id!=lab_member.c.screensaver_user_id)
                    .alias('inner'))        
                ),
            'lab_member_emails': (
                select([
                    func.array_to_string(
                        func.array_agg(func.coalesce(literal_column('email'),'null')),
                        LIST_DELIMITER_SQL_ARRAY)])
                .select_from(
                    select([lab_member.c.name,lab_member.c.email])
                    .select_from(lab_member)
                    .order_by(lab_member.c.last_first)
                    .where(lab_member.c.lab_head_id
                        ==literal_column('screensaver_user.screensaver_user_id'))
                    .where(lab_member.c.lab_head_id!=lab_member.c.screensaver_user_id)
                    .alias('inner'))        
                ),
            'lab_affiliation_name': lab_head_table.c.lab_affiliation_name,
            'lab_affiliation_category': lab_head_table.c.lab_affiliation_category,
            # Removed: 20180920 - no longer needed (JAS)
            # 'facility_usage_roles': (
            #     select([
            #         func.array_to_string(
            #             func.array_agg(_fur.c.facility_usage_role),
            #             LIST_DELIMITER_SQL_ARRAY)])
            #     .select_from(_fur)
            #     .where(_fur.c.screensaver_user_id 
            #         == _su.c.screensaver_user_id)),
            'sm_data_sharing_level': (
                select([_user_agreement.c.data_sharing_level])
                .select_from(_user_agreement)
                .where(_user_agreement.c.type==self.VOCAB_USER_AGREEMENT_SM)
                .where(_user_agreement.c.date_expired==None)
                .where(_user_agreement.c.screensaver_user_id
                    ==_su.c.screensaver_user_id)),
            'rnai_data_sharing_level': (
                select([_user_agreement.c.data_sharing_level])
                .select_from(_user_agreement)
                .where(_user_agreement.c.type==self.VOCAB_USER_AGREEMENT_RNAI)
                .where(_user_agreement.c.date_expired==None)
                .where(_user_agreement.c.screensaver_user_id
                    ==_su.c.screensaver_user_id)),
        }

        # delegate to the user resource
        default_fields = ['fields.screensaveruser', 'fields.user']
        _temp = { key:field for key, field in field_hash.items() 
            if field.get('scope', None) in default_fields }
        field_hash = _temp
        logger.debug('final field hash: %s', field_hash.keys())
        sub_columns = self.get_user_resource().build_sqlalchemy_columns(
            field_hash.values(),
            custom_columns=custom_columns)
        base_query_tables = [
            'screensaver_user', 'reports_user', 'auth_user'] 
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=sub_columns)

        # build the query statement
        
        j = _su
        j = j.join(_up, _su.c.user_id == _up.c.id, isouter=True)
        j = j.join(_au, _up.c.user_id == _au.c.id, isouter=True)
        
        j = j.join(
            lab_head_table, 
            _su.c.lab_head_id==lab_head_table.c.screensaver_user_id,
            isouter=True)
        
        stmt = select(columns.values()).select_from(j)
        # natural ordering
        stmt = stmt.order_by(_su.c.last_name, _su.c.first_name)
            
        # general setup
         
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        # logger.info(
        #     'stmt: %s',
        #     str(stmt.compile(
        #         dialect=postgresql.dialect(),
        #         compile_kwargs={"literal_binds": True})))

        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
        
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None),
            use_caching=True)
        
    def put_detail(self, request, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'put_detail')
                
    @write_authorization
    @un_cache        
    @transaction.atomic    
    def delete_obj(self, request, deserialized, **kwargs):
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        
        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
        ScreensaverUser.objects.get(**id_kwargs).delete()

    def get_id(self, deserialized, validate=False, schema=None, **kwargs):
        
        logger.debug('su get_id: %r, %r', deserialized, kwargs)
        id_kwargs = {}
        
        screensaver_user_id = kwargs.get('screensaver_user_id', None)
        if screensaver_user_id is None:
            screensaver_user_id = deserialized.get('screensaver_user_id')
        if screensaver_user_id is not None:
            screensaver_user_id = parse_val(
                screensaver_user_id, 'screensaver_user_id', 'integer')
            if screensaver_user_id is not None:
                id_kwargs['screensaver_user_id'] = screensaver_user_id
        if not id_kwargs:
            username = kwargs.get('username', None)
            if username is None:
                username = deserialized.get('username', None)
            if username:
                username = username.strip()
                if len(username) > 0:
                    id_kwargs['username'] = username
        if not id_kwargs:
            ecommons_id = kwargs.get('ecommons_id', None)
            if ecommons_id is None:
                ecommons_id = deserialized.get('ecommons_id', None)
            if ecommons_id:
                ecommons_id = ecommons_id.strip()
                if len(ecommons_id) > 0:
                    id_kwargs['ecommons_id'] = ecommons_id
            
        if not id_kwargs:
            if validate is True:
                raise ValueError, (
                    'Neither screensaver_user_id, username nor ecommons_id'
                    ' were specified')
            
        logger.debug('screensaver_user get_id: %r', id_kwargs)
        return id_kwargs

    @write_authorization
    @un_cache
    @transaction.atomic        
    def post_detail(self, request, **kwargs):
        '''
        Special POST to allow for screensaver_user_id generation
        '''
        logger.info('post_detail, screensaveruser')
        deserialize_meta = None
        if kwargs.get('data', None):
            # allow for internal data to be passed
            deserialized = kwargs.pop('data')
        else:
            deserialized, deserialize_meta = self.deserialize(
                request, format=kwargs.get('format', None))
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')

        logger.debug('patch detail %s, %s', deserialized,kwargs)
        
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        logger.debug('param_hash: %r', param_hash)

        id_kwargs = self.get_id(
            deserialized, validate=False, schema=schema, **kwargs)
        
        if not id_kwargs:
           
            if 'first_name' not in deserialized:
                raise ValidationError(key='first_name', msg='required')
            if 'last_name' not in deserialized:
                raise ValidationError(key='last_name', msg='required')
            
            try:
                first_name=parse_val(
                    deserialized['first_name'],'first_name', 'string')
                last_name = parse_val(
                    deserialized['last_name'],'last_name', 'string')
                extant_users = ScreensaverUser.objects.filter(
                    first_name__iexact=first_name,
                    last_name__iexact=last_name)
                if extant_users.exists():
                    ids = [su.screensaver_user_id for su in extant_users.all()]
                    msg = ('Screensaver User: %r has already been created using '
                        'the given first and last names: %s %s'
                        % (ids, first_name,last_name ))
                    raise ValidationError({
                        'first_name': msg, 'last_name': msg })
            except ObjectDoesNotExist:
                logger.info(
                    'POST OK: create new user (no username/ecommons specified): %r', 
                    deserialized)
        else:
            try:
                extant_user = ScreensaverUser.objects.get(**id_kwargs)
                msg = 'User %r, identified by %r already exists'
                raise ValidationError({
                    key:msg%(extant_user.screensaver_user_id, value) 
                        for key, value in id_kwargs.items() 
                })
                
            except ObjectDoesNotExist:
                logger.info('POST OK: Create new user %r', id_kwargs)
                
        return super(ScreensaverUserResource,self).post_detail(
            request, data=deserialized, **kwargs)
    
    @write_authorization
    @un_cache
    @transaction.atomic    
    def patch_obj(self, request, deserialized, **kwargs):

        DEBUG_USER_PATCH = False or logger.isEnabledFor(logging.DEBUG)
        
        PI_ROLE = SCHEMA.VOCAB.screensaver_user.classification.PRINCIPAL_INVESTIGATOR
        
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        fields = schema['fields']

        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
        logger.info('id_kwargs: %r', id_kwargs)
        screensaver_user = None
        is_patch = bool(id_kwargs)
        if id_kwargs:
            if 'screensaver_user_id' in id_kwargs:
                screensaver_user = ScreensaverUser.objects.get(
                    screensaver_user_id=id_kwargs['screensaver_user_id'])
            else:
                try:
                    if 'username' in id_kwargs:
                        screensaver_user = ScreensaverUser.objects.get(
                            username=id_kwargs['username'])
                    elif 'ecommons_id' in id_kwargs:
                        screensaver_user = ScreensaverUser.objects.get(
                            ecommons_id=id_kwargs['ecommons_id'])
                except ObjectDoesNotExist:
                    logger.info('User for %r not found, creating', id_kwargs)
                    is_patch=False
        else:
            logger.info('creating non-login user: %r', deserialized)

        if screensaver_user:
            is_patch = True
            logger.info('patching screensaveruser: %r', screensaver_user)
        else:
            logger.info('creating Screensaver User: %r', deserialized)

        errors = self.validate(deserialized, schema=schema,patch=is_patch)
        if errors:
            raise ValidationError(errors)
        
        screensaveruser_fields = { name:val for name, val in fields.items() 
            if val['scope'] == 'fields.screensaveruser'}
        initializer_dict = self.parse(
            deserialized, create=not is_patch, schema=schema)
        initializer_dict = { k:v for k,v in initializer_dict.items()
            if k in screensaveruser_fields}
        logger.info('patch screensaveruser initializer_dict: %r, %r', is_patch, initializer_dict)
        
        if screensaver_user is None:
            if id_kwargs:
                logger.info('Create log in Screensaver User for: %r', id_kwargs)
                screensaver_user = ScreensaverUser(**id_kwargs)
            else:
                logger.info(
                    'Create non-log in Screensaver User for: %r %r',
                    initializer_dict['first_name'],initializer_dict['last_name'])
                screensaver_user = ScreensaverUser(
                    first_name=initializer_dict['first_name'],
                    last_name=initializer_dict['last_name'])
            logger.info('saving new user...')
            screensaver_user.save()
            logger.info('new user saved: %r: %r', 
                screensaver_user, screensaver_user.date_created)
        messages = []
        if initializer_dict:
            # Validation rules for username, ecommons_id
            new_username = initializer_dict.pop('username',None)
            new_ecommons = initializer_dict.pop('ecommons_id',None)

            if new_username is not None:
                new_username = new_username.strip()
                if len(new_username)==0:
                    new_username = None
            if new_ecommons is not None:
                new_ecommons = new_ecommons.strip()
                if len(new_ecommons)==0:
                    new_ecommons = None

            if new_username is not None and new_ecommons is not None:
                if new_username != new_ecommons:
                    raise ValidationError({
                        'username': 'does not match ecommons_id',
                        'ecommons_id': 'does not match username' })
            
            if new_username is not None:
                if screensaver_user.username is None:
                    screensaver_user.username = new_username
                elif screensaver_user.username != new_username:
                    raise ValidationError(
                        key='username', 
                        msg='immutable (current val: %r)' % screensaver_user.username)
            
            if new_ecommons is not None:
                if screensaver_user.ecommons_id is None:
                    logger.info('screensaver_user: %r, setting ecommons: %r', 
                        screensaver_user.screensaver_user_id, new_ecommons)
                    screensaver_user.ecommons_id = new_ecommons
                elif screensaver_user.ecommons_id != new_ecommons:
                    raise ValidationError(
                        key='ecommons_id', 
                        msg='immutable (current val: %r)' % screensaver_user.ecommons_id)
                    
                if screensaver_user.username is None:
                    logger.info('screensaver_user: %r, setting username to new ecommons id: %r', 
                        screensaver_user.screensaver_user_id, new_ecommons)
                    screensaver_user.username = new_ecommons
                elif screensaver_user.username != new_ecommons:
                    raise ValidationError(
                        key='username', 
                        msg='immutable (via ecommons_id)')
            
            # Validation rules for first_name, last_name
            first_name = initializer_dict.pop('first_name', None)
            last_name = initializer_dict.pop('last_name', None)
            
            if first_name is not None or last_name is not None:
                if last_name is None:
                    last_name = screensaver_user.last_name
                if first_name is None:
                    first_name = screensaver_user.first_name
                if first_name != screensaver_user.first_name \
                    or last_name != screensaver_user.last_name:
                    extant_users = ScreensaverUser.objects.filter(
                        first_name__iexact=first_name,
                        last_name__iexact=last_name)
                    if extant_users.exists():
                        ids = [su.screensaver_user_id for su in extant_users.all()]
                        msg = ('Screensaver User: %r has already been created using '
                            'the given first and last names: %s %s'
                            % (ids, first_name,last_name ))
                        raise ValidationError({
                            'first_name': msg, 'last_name': msg })
                    screensaver_user.first_name = first_name
                    screensaver_user.last_name = last_name

            _key = 'lab_head_id'
            lab_head_id = initializer_dict.get(_key)
            lab_head = None
            if lab_head_id:
                try:
                    lab_head = ScreensaverUser.objects.get(
                        screensaver_user_id=lab_head_id)
                    if lab_head.classification != PI_ROLE:
                        raise ValidationError(
                            key=_key,
                            msg = 'Chosen lab head "user.classification" '
                                'must be %r ' % PI_ROLE)
                    logger.info('initialize user with lab head: %r', lab_head)
                except ObjectDoesNotExist, e:
                    raise ValidationError(
                        key=_key,
                        msg='Screensaver user for %r not found' % lab_head_id)

            _key = 'lab_head_username'
            lab_head_username = initializer_dict.get(_key)
            if lab_head_username:
                logger.info('Init: lab_head_username: %r', lab_head_username)
                if lab_head:
                    if lab_head.username and lab_head.username != lab_head_username:
                        raise ValidationError(
                            key=_key, 
                            msg='must be the same user as the "lab_head_id"')
                else:
                    try:
                        lab_head = ScreensaverUser.objects.get(
                            username=lab_head_username)
                        if lab_head.classification != PI_ROLE:
                            raise ValidationError(
                                key=_key,
                                msg = 'Chosen lab head "user.classification" '
                                    'must be %r ' % PI_ROLE)
                        logger.info('initialize user with lab head: %r', lab_head)
                        initializer_dict['lab_head_id'] = lab_head.screensaver_user_id
                    except ObjectDoesNotExist, e:
                        raise ValidationError(
                            key=_key,
                            msg='Screensaver user for %r not found' 
                                % lab_head_username)
            
            _key = 'classification'
            classification = initializer_dict.get(_key)
            if classification:
                if classification == PI_ROLE:
                    if lab_head is not None:
                        raise ValidationError(
                            key=_key,
                            msg='Classification may not be % for lab member'
                                % PI_ROLE)
                elif screensaver_user.classification == PI_ROLE:
                    raise ValidationError(
                        key=_key, msg='May not be changed from %r'
                            % PI_ROLE)
            
            _key = 'lab_affiliation_name'
            lab_affiliation_name = initializer_dict.get(_key)
            if lab_affiliation_name:
                lab_affiliation = LabAffiliation.objects.get(
                    name=lab_affiliation_name)
                la_id = initializer_dict.get('lab_affiliation_id')
                if la_id:
                    if la_id != lab_affiliation.lab_affiliation_id:
                        raise ValidationError(
                            key=_key,
                            msg='may not be set if "lab_affiliation_id" is set')
                else:
                    initializer_dict['lab_affiliation_id'] = \
                        lab_affiliation.lab_affiliation_id
            
            logger.info('final initializer: %r', initializer_dict)
            for key, val in initializer_dict.items():
                if hasattr(screensaver_user, key):
                    setattr(screensaver_user, key, val)
        else:
            logger.info('no (basic) screensaver_user fields to update %r', 
                screensaver_user)
            
        screensaver_user.save()
        logger.info('User saved: %r: %r', 
            screensaver_user, screensaver_user.date_created)
        
        # Tie ScreensaverUser entry to Reports.User:
        # If reports_userprofile exists, or username is set, patch the 
        # reports_userprofile
        reports_kwargs = {}
        if screensaver_user.username is not None:
            reports_kwargs['username'] = screensaver_user.username
        user = None
        if not reports_kwargs:
            logger.info('non-login user: no username or ecommons, no report profile created.')
            messages.append(
                'Non login user (no username or eCommons ID')
        else:
            # create/get userprofile
            # NOTE: reports.user patch data must include first/last names
            if 'first_name' not in deserialized or 'last_name' not in deserialized:
                deserialized.update({
                    'first_name': screensaver_user.first_name,
                    'last_name': screensaver_user.last_name})
            
            deserialized['username'] = screensaver_user.username
            # if DEBUG_USER_PATCH:
            logger.info('patch the reports user: %r, %r',
                reports_kwargs, deserialized)
            patch_response = self.get_user_resource().patch_obj(
                request, deserialized, **reports_kwargs)
            #if DEBUG_USER_PATCH:
            logger.info('patched userprofile %s', patch_response)
            user = patch_response[API_RESULT_OBJ]

        is_staff = False
        if user:
            if screensaver_user.user is None:
                logger.info('set the reports userprofile: %r to the su: %r', 
                    user, screensaver_user)
                screensaver_user.user = user
            else:
                if screensaver_user.user != user:
                    raise ValidationError(
                        key='username',
                        msg='ss user found: %r, not equal to current: %r'\
                            % (user, screensaver_user.user))    
            if DEBUG_USER_PATCH:
                logger.info('set the username: %r: %r, %r', 
                    screensaver_user.screensaver_user_id, 
                    screensaver_user.username, user.username)
            screensaver_user.username = user.username

            auth_user = user.user
            is_staff = auth_user.is_staff
            if is_staff:
                if not screensaver_user.classification:
                    screensaver_user.classification = \
                        SCHEMA.VOCAB.screensaver_user.classification.STAFF

        logger.info('Save screensaver_user: %r, %r',screensaver_user, screensaver_user.user)
        screensaver_user.save()
        logger.info('reports user saved: %r',screensaver_user)
        
        # Validate business rules for user types
        if is_staff != True:
            usergroups = deserialized.get('usergroups', None)
            if usergroups:
                raise ValidationError(
                    key='usergroups',
                    msg='May only be set for staff users')
            permissions = deserialized.get('permissions', None)
            if usergroups:
                raise ValidationError(
                    key='permissions',
                    msg='May only be set for staff users')

            if screensaver_user.classification == PI_ROLE:
                if screensaver_user.lab_affiliation is None:
                    # should already be set
                    raise ValidationError(
                        key='lab_affiliation_id',
                        msg='Must be set if classification is %r'
                            % PI_ROLE)
                if screensaver_user.lab_head != screensaver_user:
                    screensaver_user.lab_head = screensaver_user
            
            else:
                if screensaver_user.lab_affiliation is not None:
                    raise ValidationError(
                        key='lab_affiliation_id',
                        msg='may not be set unless user is a Principal Investigator')
                
                lab_head = screensaver_user.lab_head
                if lab_head is None:
                    raise ValidationError({
                        'lab_head_id': 'Must be set for non staff',
                        'classification': 
                            'Must be %r if non staff and not a lab member'
                                % PI_ROLE })
                
                lab_head_user_agreements = \
                    UserAgreement.objects.all().filter(screensaver_user=lab_head)
                
                for ua in screensaver_user.useragreement_set.all():
                    if lab_head_user_agreements.exists():
                        for lhua in lab_head_user_agreements.all():
                            if lhua.type == ua.type:
                                if lhua.data_sharing_level != ua.data_sharing_level:
                                    messages.append
                
                # Reset user dsls to lab_head values
                # TODO: warn and override
                # if screensaver_user.sm_data_sharing_level \
                #         != lab_head.sm_data_sharing_level:
                #     if lab_head.sm_data_sharing_level is not None:
                #         messages.append('%r updated from %r to %r'
                #             % ( 'sm_data_sharing_level',
                #                 screensaver_user.sm_data_sharing_level,
                #                 lab_head.sm_data_sharing_level ))
                #         screensaver_user.sm_data_sharing_level = \
                #             lab_head.sm_data_sharing_level
                # if screensaver_user.rnai_data_sharing_level \
                #         != lab_head.rnai_data_sharing_level:
                #     if lab_head.rnai_data_sharing_level is not None:
                #         messages.append('%r updated from %r to %r'
                #             % ( 'rnai_data_sharing_level',
                #                 screensaver_user.rnai_data_sharing_level,
                #                 lab_head.rnai_data_sharing_level ))
                #         screensaver_user.rnai_data_sharing_level = \
                #             lab_head.rnai_data_sharing_level
            
        screensaver_user.save()
        
        _key = 'lab_member_ids'
        if _key in initializer_dict:
            lab_member_ids = initializer_dict[_key]
            lab_members = []
            # may empty
            if lab_member_ids:
                if screensaver_user.classification != PI_ROLE:
                    raise ValidationError(
                        key='lab_member_ids',
                        msg='User classification must be %r' % PI_ROLE)
            for lab_member_id in lab_member_ids:
                try:
                    lab_member = ScreensaverUser.objects.get(
                        screensaver_user_id=lab_member_id)
                    if lab_member.classification == PI_ROLE:
                        raise ValidationError(
                            key='lab_member_ids',
                            msg='User is a %r and cannot be added as a lab member'
                                % PI_ROLE)
                    
                    lab_members.append(lab_member)
                    
                    # FIXME: need override from UI to set
                    msg='lab member data sharing level (%r) must match lab head'
                    if screensaver_user.useragreement_set.exists():
                        if lab_member.useragreement_set.exists():
                            for ua in screensaver_user.useragreement_set.all():
                                for member_ua in lab_member.useragreement_set.all():
                                    if ua.type == member_ua.type:
                                        if ua.data_sharing_level is None:
                                            continue
                                        if member_ua.data_sharing_level is None:
                                            continue
                                        if ua.date_expired is not None:
                                            continue
                                        if member_ua.date_expired is not None:
                                            continue
                                        if ua.data_sharing_level != member_ua.data_sharing_level:
                                            raise ValidationError(
                                                key='lab_member_ids',
                                                msg=msg%member_ua.data_sharing_level)
                    
                except ObjectDoesNotExist:
                    raise ValidationError(
                        key=_key,
                        msg='No such user: %r' % lab_member_id)
            screensaver_user.lab_members = lab_members
        
        logger.info('save user: %r, %r', screensaver_user, screensaver_user.user)
        
        screensaver_user.save()
                
        # Removed 20180820 - no longer needed (JAS)
        # if 'facility_usage_roles' in initializer_dict:
        #     current_roles = set([r.facility_usage_role 
        #         for r in screensaver_user.userfacilityusagerole_set.all()])
        #     new_roles = initializer_dict['facility_usage_roles']
        #     if new_roles is None:
        #         new_roles = []
        #     new_roles = set(new_roles)
        #     if DEBUG_USER_PATCH:
        #         logger.info('roles to delete: %s', current_roles - new_roles)
        #     (screensaver_user.userfacilityusagerole_set
        #         .filter(facility_usage_role__in=current_roles - new_roles)
        #         .delete())
        #     for role in new_roles - current_roles:
        #         if DEBUG_USER_PATCH:
        #             logger.info(
        #                 'create facility_usage_role: %s, %s', 
        #                 screensaver_user, role)
        #         ur = UserFacilityUsageRole.objects.create(
        #             screensaver_user=screensaver_user,
        #             facility_usage_role=role)            
        #         ur.save()

        logger.info('patch_obj done: %r', screensaver_user)

        meta = {}        
        if messages:
            meta[SCHEMA.API_MSG_RESULT] = messages

        return { 
            API_RESULT_OBJ: screensaver_user,
            API_RESULT_META: meta 
        }
            

class LibraryResourceAuthorization(UserGroupAuthorization):        

    def _is_resource_authorized(
        self, user, permission_type, **kwargs):
        authorized = super(LibraryResourceAuthorization, self)\
            ._is_resource_authorized(user, permission_type, **kwargs)
        if authorized is True:
            return True
        if not user:
            return False
        if permission_type == 'read':
            logger.info('Allow all libraries for all authenticated users at this time.')
            return user.is_active
        logger.info('resource not authorized: %r, %r, %r', 
            self.resource_name, user, permission_type)
        return False
    
    # def is_restricted_view(self, user):
    #    return not self._is_resource_authorized(user, 'read')
        
    def filter(self, user, filter_expression):
        if self.is_restricted_view(user):
            if DEBUG_AUTHORIZATION:
                logger.info('create library filter for %r', user)
            auth_filter = column('is_released') == True
            if filter_expression is not None:
                filter_expression = and_(filter_expression, auth_filter)
            else:
                filter_expression = auth_filter
 
        return filter_expression
    
    
    # NOTE: 20170908 - Not used, all authenticated users may access - JAS/CS
    def get_authorized_library_screen_types(self, user):
        pass
        # screensaver_user = ScreensaverUser.objects.get(username=user.username)
        # 
        # # TODO: review requirements
        # screen_types = set()
        # SM_TYPE = SCREEN_TYPE.SMALL_MOLECULE
        # RNAI_TYPE = 'rnai'
        # if screensaver_user.sm_data_sharing_level is not None:
        #     screen_types.add(SM_TYPE)
        # if screensaver_user.rnai_data_sharing_level is not None:
        #     screen_types.add(SM_TYPE)
        # 
        # return screen_types


class ReagentResourceAuthorization(LibraryResourceAuthorization):

    def _is_resource_authorized(
        self, user, permission_type, **kwargs):
        authorized = super(ReagentResourceAuthorization, self)\
            ._is_resource_authorized(user, permission_type, **kwargs)
        return authorized

    def filter(self, user, filter_expression):
        if self.is_restricted_view(user):
            if DEBUG_AUTHORIZATION:
                logger.info('create library filter for %r', user)
            auth_filter = column('is_released') == True
            if filter_expression is not None:
                filter_expression = and_(filter_expression, auth_filter)
            else:
                filter_expression = auth_filter
 
        return filter_expression
        
#     def get_row_property_generator(self, user, fields, extant_generator):
#         '''
#         Filter result properties based on authorization rules
#         '''
#         class Row:
#             def __init__(self, row):
#                 logger.debug(
#                     'filter screen row: %r', 
#                     [(key, row[key]) for key in row.keys()])
#                 self.row = row
# 
#             def has_key(self, key):
#                 return self.row.has_key(key)
#             def keys(self):
#                 return self.row.keys()
#             def __getitem__(self, key):
#                 if self.row[key] is None:
#                     return None
#                 else:
#                     if key == 'structure_image':
#                         if self.has_key('is_restricted_structure'):
#                             if self.row['is_restricted_structure']:
#                                 return None
#                         return self.row[key]
#                     else:
#                         return self.row[key]
# 
#         def restrict_image_property_generator(cursor):
#             if extant_generator is not None:
#                 cursor = extant_generator(cursor)
#             for row in cursor:
#                 yield Row(row)
#         
#         return restrict_image_property_generator

class ReagentResource(DbApiResource):
    
    class Meta:

        queryset = Reagent.objects.all()
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'reagent'
        authorization = ReagentResourceAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        
    def __init__(self, **kwargs):

        self.library_resource = None
        self.sr_resource = None
        self.smr_resource = None
        self.npr_resource = None
        self.data_column_resource = None
        self.study_resource = None
        super(ReagentResource, self).__init__(**kwargs)
    
    def prepend_urls(self):
        
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)/%s/(?P<%s>[\d]+)%s$" 
                % (self._meta.resource_name, SCHEMA.URI_PATH_COMPLEX_SEARCH, 
                    SCHEMA.API_PARAM_COMPLEX_SEARCH_ID,TRAILING_SLASH),
                self.wrap_view('search'), name="api_search"),
            
            url(r"^(?P<resource_name>%s)/compound_search%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_search_vendor_and_compound'), 
                name="api_search_vendor_and_compound"),            
            url(r"^(?P<resource_name>%s)/compound_search/%s/(?P<%s>[\d]+)%s$" 
                % (self._meta.resource_name, SCHEMA.URI_PATH_COMPLEX_SEARCH, 
                    SCHEMA.API_PARAM_COMPLEX_SEARCH_ID,TRAILING_SLASH),
                self.wrap_view('dispatch_search_vendor_and_compound'), 
                name="api_search_vendor_and_compound"),
                
            # url(r"^(?P<resource_name>%s)/(?P<substance_id>[^:]+)%s$" 
            #         % (self._meta.resource_name, TRAILING_SLASH),
            #     self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)/(?P<well_id>\d{1,5}\:?[a-zA-Z]{1,2}\d{1,2})%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)"
                r"/(?P<well_id>\d{1,5}\:[a-zA-Z]{1,2}\d{1,2})"
                r"/other_wells%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_other_wells'), 
                name="api_dispatch_other_wells"),
            url(r"^(?P<resource_name>%s)"
                r"/(?P<well_id>\d{1,5}\:[a-zA-Z]{1,2}\d{1,2})"
                r"/report%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_well_detail_report'), 
                name="api_dispatch_well_detail_report"),
        ]
    
    def get_debug_times(self):
        
        self.get_sr_resource().get_debug_times()
        self.get_smr_resource().get_debug_times()
        
    def get_sr_resource(self):
        if not self.sr_resource:
            self.sr_resource = SilencingReagentResource()
        return self.sr_resource
    
    def get_smr_resource(self):
        if not self.smr_resource:
            self.smr_resource = SmallMoleculeReagentResource()
        return self.smr_resource
    
    def get_npr_resource(self):
        if not self.npr_resource:
            self.npr_resource = NaturalProductReagentResource()
        return self.npr_resource
    
    def get_datacolumn_resource(self):
        if self.data_column_resource is None:
            self.data_column_resource = DataColumnResource()
        return self.data_column_resource
    
    def get_library_resource(self):
        if not self.library_resource:
            self.library_resource = LibraryResource()
        return self.library_resource
    
    def get_study_resource(self):
        if not self.study_resource:
            self.study_resource = StudyResource()
        return self.study_resource 
    
    def get_schema(self, request, **kwargs):
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        extra_dc_ids = param_hash.get(API_PARAM_DC_IDS, None)

        if not 'library_short_name' in kwargs:
            return self.build_response(
                request, 
                self.build_schema(
                    request.user,extra_dc_ids=extra_dc_ids, **param_hash), **kwargs)

        # FIXME: 20181115: not used after ReagentResource/SM/RNAi refactor
#         library_short_name = kwargs.pop('library_short_name')
#         try:
#             library = Library.objects.get(short_name=library_short_name)
#             return self.build_response(
#                 request, 
#                 self.build_schema(
#                     request.user,
#                     extra_dc_ids=extra_dc_ids,
#                     library_classification=library.classification), 
#                 **kwargs)
#             
#         except Library.DoesNotExist, e:
#             raise Http404(
#                 'Can not build schema - library def needed'
#                 'no library found for short_name: %r' % library_short_name)

                
    def build_schema(
        self, user, extra_dc_ids=None, library_classification=None, **kwargs):

        schema = deepcopy(
            super(ReagentResource, self).build_schema(user=user, **kwargs))
        
#         if library_classification:
#             logger.info('build schema: %r', library_classification)
#             if library_classification == 'rnai':
#                 sub_data = self.get_sr_resource().build_schema(user, **kwargs)
#             elif library_classification == 'natural_product':
#                 sub_data = self.get_npr_resource().build_schema(user, **kwargs)
#             elif library_classification == 'small_molecule':
#                 sub_data = self.get_smr_resource().build_schema(user, **kwargs)
#             else:
#                 raise ProgrammingError
#             logger.info('sub_resource field visibilities: %r', 
#                 [(key,'%r'%fi['visibility']) for key, fi in sub_data['fields'].items()])
#             newfields = {}
#             newfields.update(sub_data['fields'])
#             newfields.update(schema['fields'])
#             schema['fields'] = newfields
#             
#             schema['content_types'] = sub_data['content_types']
        
        if self._meta.resource_name in ['reagent', 'well']:
            # TODO: 20181115: 
            # This is a hack to build a composite schema containing all the 
            # subtype field definitions;
            # ResourceResouce code should be modified to build this
            sub_data = self.get_resource_resource()._get_resource_schema(
                'silencingreagent', user=user, **kwargs)
            for field in sub_data['fields'].values():
                field['visibility'] = []
                field['editability'] = []
            newfields = {}
            newfields.update(sub_data['fields'])
            newfields.update(schema['fields'])
            schema['fields'] = newfields
 
            sub_data = self.get_resource_resource()._get_resource_schema(
                'smallmoleculereagent', user=user, **kwargs)
            for field in sub_data['fields'].values():
                field['visibility'] = []
                field['editability'] = []
            newfields = {}
            newfields.update(sub_data['fields'])
            newfields.update(schema['fields'])
            schema['fields'] = newfields
             
            sub_data = self.get_resource_resource()._get_resource_schema(
                'naturalproductreagent', user=user, **kwargs)
            for field in sub_data['fields'].values():
                field['visibility'] = []
                field['editability'] = []
            newfields = {}
            newfields.update(sub_data['fields'])
            newfields.update(schema['fields'])
            schema['fields'] = newfields
            
        
        logger.info('build reagent schema for user: %r, resource: %r, '
            'extra_dc_ids: %r', user, self._meta.resource_name, extra_dc_ids)
        fields = schema['fields']
        max_ordinal = 0
        for fi in fields.values():
            if fi.get('ordinal', 0) > max_ordinal:
                max_ordinal = fi['ordinal']

        if extra_dc_ids:
            logger.info('user: %r, get extra_dc_ids: %r', user, extra_dc_ids)
            reported_extra_datacolumns = self.get_datacolumn_resource()\
                ._get_list_response_internal(
                    user, includes='*',
                    data_column_id__in=extra_dc_ids)
            # NOTE: filter for user_access_level_granted > 1; users may only
            # add other screen columns for mutually shared or full access screens
            # NOTE: query filtering, e.g. user_access_level_granted__gt=1 
            # is not available for the DataColumn resource
            extra_datacolumns = []
            for dc in reported_extra_datacolumns:
                logger.info('test dc for access: '
                    '{key}, {user_access_level_granted}'.format(**dc))
                if dc['user_access_level_granted'] > ACCESS_LEVEL.OVERLAPPING_ONLY: #1:
                    extra_datacolumns.append(dc)
                else:
                    logger.warn('Mutual overlapping columns are not visible '
                        'in the Reagents view: {screen_facility_id}: {key} '
                        'access level: {user_access_level_granted}'.format(**dc))
            if extra_datacolumns:
                logger.info('allowed extra datacolumns: %r', 
                    ['{screen_facility_id}: {key}, {user_access_level_granted}'\
                        .format(**dc) for dc in extra_datacolumns])
                decorated = [
                    (dc['study_type'] != None,
                     dc['screen_facility_id'],dc['ordinal'], dc) 
                        for dc in extra_datacolumns]
                decorated.sort(key=itemgetter(0,1,2))
                extra_datacolumns = [dc for sort_study,fid,ordinal,dc in decorated]
                logger.info('extra_datacolumns: %r', len(extra_datacolumns))
                datacolumn_fields = {}
                for i, dc in enumerate(extra_datacolumns):
                    dc['visibility'] = ['l','d']
                    dc['is_datacolumn'] = True
                    dc['ordinal'] = max_ordinal + i
                    if dc['user_access_level_granted'] > ACCESS_LEVEL.OVERLAPPING_ONLY: #1:
                        dc['ordering'] = True
                        dc['filtering'] = True
                    else:
                        raise PermissionDenied(
                            'Mutual Overlapping columns are not visible in the ' 
                            'Reagent view: %r' % dc)
                    datacolumn_fields[dc['key']] = dc
                    
                max_ordinal += len(datacolumn_fields)
                
                datacolumn_fields.update(schema['fields'])
                schema['fields'] = datacolumn_fields

        return schema

    def dispatch_other_wells(self, request, **kwargs):
        self.is_authenticated(request)

        resource_name = kwargs.pop('resource_name', self._meta.resource_name)
        authorized = self._meta.authorization._is_resource_authorized(
            request.user, 'read', **kwargs)
        if authorized is not True:
            raise PermissionDenied
        if request.method.lower() != 'get':
            return self.dispatch('detail', request, **kwargs)
    
        well_id = kwargs.pop('well_id', None)
        if not well_id:
            raise MissingParam('well_id')
                
        well_kwargs = {
            'exact_fields':  ['other_wells_with_reagent'],
            'well_id': well_id
            }        
        well_data = self._get_detail_response_internal(request.user, **well_kwargs)
        other_wells = well_data.get('other_wells_with_reagent')
        if not other_wells:
            raise Http404('no other wells found for well_id: %r' % well_id)
        
        kwargs['well_id__in'] = other_wells
        return self.get_list(request, **kwargs)
        
    def get_other_wells(self, user, well_id ):
        
        kwargs = {
            'exact_fields':  ['other_wells_with_reagent'],
            'well_id': well_id
            }        
        
        well_data = self._get_detail_response_internal(user, **kwargs)
        other_wells = well_data.get('other_wells_with_reagent')
        data = []
        if other_wells:
            data = self._get_list_response_internal(user,**{
                'well_id__in': other_wells })
        return data
        
    def get_annotations(self, well_id):
        # NOTE: study annotations are available to all authorized users:
        # - there is no study specific authorization

        screens = {}        
        for rv in (ResultValue.objects
                .filter(well_id=well_id)
                .filter(data_column__screen_result__screen__study_type__isnull=False)):
            screen = rv.data_column.screen_result.screen
            if screen.facility_id not in screens:
                _screen_data = screens.setdefault(
                    screen.facility_id,
                    { 
                        '1-screen_facility_id': screen.facility_id,
                        'facility_id': screen.facility_id,
                        'title': screen.title,
                        'summary': screen.summary,
                        'lead_screener_id': screen.lead_screener.screensaver_user_id,
                        'lead_screener_name': '%s %s' % (
                            screen.lead_screener.first_name, screen.lead_screener.last_name),
                        'lab_head_id': screen.lab_head.screensaver_user_id,
                        'lab_name': '%s %s' % (
                            screen.lab_head.first_name, screen.lab_head.last_name),
                        'date_created': screen.date_created,
                        'study_type': screen.study_type,
                        'values': {},
                        'fields': {}
                    })
            _screen_data = screens[screen.facility_id]
            (column_name, _dict) = \
                self.get_datacolumn_resource()._create_datacolumn_from_orm(rv.data_column)
            _screen_data['values'][column_name] = rv.value
            if _dict['data_type'] in ['decimal','integer','numeric']:
                _screen_data['values'][column_name] = rv.numeric_value
            _dict['visibility'] = ['l','d']
            _dict['title'] = _dict['name']
            _screen_data['fields'][column_name] = _dict
        
        final_data = sorted(screens.values(), key=lambda x: x['facility_id'])
        return final_data
        
    def get_duplex_data(self, request, well_id=None, schema=None):
        '''
        NOTE: authorization is handled by the schema definition - if the user 
        does not have access to the duplex_wells, then the field will not be 
        visible in the schema.
        '''
        
        if schema is None:
            well = Well.objects.get(well_id=well_id)
            logger.info('building schema for well: %r, %r', 
                well_id,well.library.classification)
            schema = self.build_schema(
                user=request.user, 
                library_classification=well.library.classification)
        
        data = {}
        if 'duplex_wells' not in schema['fields']:
            logger.info('%r not found in schema for well: %r',
                        'duplex_wells', well_id)
            logger.info('user: %r (may not have access)', request.user)
            return data
        try:
            sr = SilencingReagent.objects.get(well__well_id=well_id)
            
            if sr.well.library.is_pool is not True:
                return data
            
            _aw = self.bridge['assay_well']
            _sr = self.bridge['screen_result']
            _s = self.bridge['screen']
            srr = ScreenResultResource()
            
            # FIXME:  only allow duplex confirmation data for user_access_level_granted=2,3
            
            stmt = (
                select([_s.c.facility_id, _s.c.title, 
                        _aw.c.confirmed_positive_value, 
                        _aw.c.is_positive])
                    .select_from(_aw
                        .join(_sr, _aw.c.screen_result_id==_sr.c.screen_result_id)
                        .join(_s, _sr.c.screen_id==_s.c.screen_id))
                    .where(_s.c.study_type == None))
            fields = ['facility_id', 'title', 'confirmed_positive_value', 'is_positive']
    
            with get_engine().connect()as conn:
                
                data_per_screen = {}
                well_info = {}
                for dw in sr.duplex_wells.all():
                    
                    item = { 'well_id': dw.well_id }
                    sr = SilencingReagent.objects.get(well__well_id=dw.well_id)
                    
                    item['sequence'] = sr.sequence
                    item['vendor_id'] = '%s %s' %(sr.vendor_name,sr.vendor_identifier)
                    item['library_short_name'] = dw.library.short_name
                    well_info[dw.well_id] = item
                    
                    result = conn.execute(stmt.where(_aw.c.well_id==dw.well_id))
                    if logger.isEnabledFor(logging.DEBUG):
                        compiled_stmt = \
                            str(stmt.where(_aw.c.well_id==dw.well_id).compile(
                                dialect=postgresql.dialect(),
                                compile_kwargs={"literal_binds": True}))
                        logger.info('stmt: %r', compiled_stmt)
                        
                    for screen_cp_data in cursor_generator(result, fields):


                        if self.get_study_resource()._meta.authorization\
                            .has_screen_read_authorization(
                                request.user,screen_cp_data['facility_id']) is False:
                            logger.info('cp data denied for: %r: %r', 
                                request.user, screen_cp_data)
                        else:
                            screen_row = data_per_screen.setdefault(
                                screen_cp_data['facility_id'],
                                { 'screen_facility_id': screen_cp_data['facility_id'],
                                  'screen_title': screen_cp_data['title'],
                                 })
                            screen_row[dw.well_id] = screen_cp_data['confirmed_positive_value']
                                
                data = { 'duplex_wells': well_info,
                        'confirmed_positive_values': data_per_screen.values() }
        except ObjectDoesNotExist:
            logger.info('no silencing reagent for well: %r', well_id)
            
        return data

    @read_authorization
    def dispatch_well_detail_report(self, request, **kwargs):
        ''' 
        Special single reagent view:
        - includes annotations
        - include duplex wells report
        - bypasses the "dispatch" framework call
        -- must be authenticated and authorized
        '''
        logger.info('dispatch_well_detail_report')
        
        if request.method.lower() != 'get':
            return self.dispatch('detail', request, **kwargs)

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        content_type = self.get_serializer().get_accept_content_type(
            request,format=kwargs.get('format', None))

        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        use_raw_lists = request.GET.get(HTTP_PARAM_RAW_LISTS, False)
        if is_data_interchange or content_type == SDF_MIMETYPE:
            use_vocab = False
            use_titles = False
            use_raw_lists = True

        well_id = kwargs.get('well_id', None)
        if not well_id:
            raise MissingParam('well_id')

        try:
            well = Well.objects.get(well_id=well_id)
        except ObjectDoesNotExist:
            raise Http404('no well found for well_id: %r' % well_id)
            
        screen_type = well.library.screen_type
        schema = self.build_schema(request.user, 
                                   library_classification=screen_type)
        
        # 1. Fetch the well and reagent data using the user's schema
        includes = set()
        if screen_type == 'small_molecule':
            if content_type == SDF_MIMETYPE:
                includes.add('molfile')
            else:
                includes.add('structure_image')
        kwargs_internal = param_hash.copy()
        kwargs_internal.update({
            'well_id': well_id,
            'library_short_name': well.library.short_name,
            'includes': list(includes),
            'schema': schema,
            # set the server name so that the image url is constructed properly
            'SERVER_NAME': request.get_host(),
            'visibilities': ['report',]
            
        })
        well_data = self._get_detail_response_internal(
            user=request.user, **kwargs_internal)
        if not well_data:
            raise Http404('no well found for well_id: %r' % well_id)
        
        final_data = {
            'well_data': well_data,
        }
        
        # 2. Fetch annotation (study), duplex, and other well data
        if content_type != SDF_MIMETYPE:
            annotation_data = self.get_annotations(well_id)
            if annotation_data:
                final_data['annotations'] =  annotation_data
            duplex_data = self.get_duplex_data(request, well_id, schema=schema)
            if duplex_data:
                final_data['duplex_data'] = duplex_data
            other_wells = self.get_other_wells(request.user, well_id)
            if other_wells: 
                final_data['other_wells'] = other_wells
                
        # 3. Format the well data report
        def format_report_data(data):
            
            list_brackets = LIST_BRACKETS
            if use_raw_lists:
                list_brackets = None
            formatted_data = []
            formatted_data.append(['Field','Value'])
            
            well_data = data['well_data']

            sorted_well_keys = sorted(
                well_data.keys(),
                key = lambda field_name: 
                    schema['fields'][field_name].get('ordinal'))
            def title_function(key):
                if use_titles is True:
                    return schema['fields'][key]['title']
                else:
                    return key
                
            for i, key in enumerate(sorted_well_keys):
                val = well_data[key]
                if val is None:
                    logger.debug('skipping key: %r', key)
                    continue
                elif isinstance(val, six.string_types):
                    if not val.strip():
                        continue
                row = [
                    title_function(key),
                    csvutils.convert_list_vals(
                        val, delimiter=LIST_DELIMITER_XLS, 
                        list_brackets=list_brackets)
                ]
                formatted_data.append(row)
                
            annotations = data.get('annotations')
            if annotations:
                formatted_data.append([])
                formatted_data.append(['Study Data'])
                study_schema = \
                    self.get_study_resource().build_schema(user=request.user)
                def study_title_fun(key):
                    if use_titles is True:
                        return study_schema['fields'][key]['title']
                    else:
                        return key
                    
                study_fields = [
                    'facility_id', 'title','summary','lead_screener_name',
                    'lab_name', 'study_type', 'date_created']
                study_value_header = ['Study Field','Description','Value']
                
                for study_data in annotations:
                    for i, key in enumerate(study_fields):
                        row = [study_title_fun(key)]
                        if key == 'date_created':
                            val = parse_val(
                                study_data[key],'date_created','date')
                            if val:
                                row.append(val.strftime(DATE_FORMAT))
                        else:
                            row.append(study_data[key])
                        formatted_data.append(row)

                    formatted_data.append(study_value_header)
                    
                    for i, (key,value) in enumerate(study_data['values'].items()):
                        row = [
                            study_data['fields'][key]['name'],
                            study_data['fields'][key]['description'],
                            value
                            ]
                        formatted_data.append(row)

            duplex_data = data.get('duplex_data')
            if duplex_data:
                
                cp_vocab = self.get_vocab_resource()\
                    ._get_vocabularies_by_scope(
                        'resultvalue.confirmed_positive_indicator')
                wells = sorted(duplex_data['duplex_wells'].keys())
                screens = sorted(
                    duplex_data['confirmed_positive_values'], 
                    key= lambda x: alphanum_key(x['screen_facility_id']))
                
                formatted_data.append([])
                
                
                title_row = ['Duplex Data','',]
                formatted_data.append(title_row)

                row = []
                well_row1 = []
                well_row2 = []
                well_title_row = []
                if screens:
                    well_title_row.extend(['',''])
                    row.extend(['','',])
                    well_row1.extend(['','',])
                    well_row2.extend(['Screen','Screen Title',])

                well_title_row.extend(
                    ['Duplex Well %r' % i for i in range(1,len(wells)+1)])
                formatted_data.append(well_title_row)
                
                row.extend(wells)
                formatted_data.append(row)
                
                for j,well_id in enumerate(wells):
                    well_data = duplex_data['duplex_wells'][well_id]
                    well_row1.append(well_data['vendor_id'])
                    well_row2.append(well_data['sequence'])
                formatted_data.append(well_row1)
                formatted_data.append(well_row2)
                
                for i,screen_data in enumerate(screens):
                    screen_row = [
                        screen_data['screen_facility_id'],
                        screen_data['screen_title']]
                    for j,well_id in enumerate(wells):
                        screen_row.append(
                            cp_vocab[str(screen_data[well_id])]['title'])
                    
                    formatted_data.append(screen_row)
            
            other_wells = data.get('other_wells')
            if other_wells:
                
                formatted_data.append([])
                formatted_data.append(['Other Wells with the same Reagent'])
                
                other_well_fields = [
                    'well_id','plate_number','well_name','library_short_name',
                    'vendor_identifier','vendor_name',
                    'mg_ml_concentration','molar_concentration']

                formatted_data.append([
                    title_function(field) for field in other_well_fields ])
                for well_data in other_wells:
                    formatted_data.append([
                        well_data[field] for field in other_well_fields])
                
            return formatted_data
                
        def create_xls_report_response(data):
            with  NamedTemporaryFile(delete=False) as temp_file:
                wb = xlsxwriter.Workbook(temp_file, {'constant_memory': False})
                sheet = wb.add_worksheet('data')
                # text_wrap note: Excel will fail to resize the height for:
                # cell_format = wb.add_format({'text_wrap': True})
                sheet.set_column(0,10,30)
                
                for i,row in enumerate(data):
                    if row and row[0] == 'Structure Image':
                        sheet.write_string(i,0,row[0])
                        val = urlparse(row[1])[2]
                        write_xls_image(sheet, i, 1, val, request)
                    else:
                        sheet.write_row(i,0,row)

                logger.info('close workbook...')
                wb.close()
                
                logger.info('get file size...')
                temp_file.seek(0, os.SEEK_END)
                size = temp_file.tell()
                temp_file.seek(0)   
                
                logger.info('xls stream to response...')
                _file = file(temp_file.name)
                response = StreamingHttpResponse(FileWrapper1(_file)) 
                response['Content-Length'] = size
                response['Content-Type'] = XLSX_MIMETYPE
                return response
        
        def to_csv(data):
            raw_data = cStringIO.StringIO()
            writer = unicodecsv.writer(raw_data) 
            for row in data:
                writer.writerow(row)
            return raw_data.getvalue()     
                
        filename = 'well_%s' % '_'.join(well_id.split(':'))
            
        if content_type == XLS_MIMETYPE:
            response = create_xls_report_response(format_report_data(final_data))
            response['Content-Disposition'] = \
                'attachment; filename=%s.xlsx' % filename
        elif content_type == CSV_MIMETYPE:
            response = HttpResponse(
                content=to_csv(format_report_data(final_data)),
                content_type=content_type)
            response['Content-Disposition'] = \
                'attachment; filename=%s.csv' % filename
        elif content_type == SDF_MIMETYPE:
            response = HttpResponse(
                content=self.get_serializer().serialize(
                    well_data, content_type),
                content_type=content_type)
            response['Content-Disposition'] = \
                'attachment; filename=%s.sdf' % filename
        else:
            raise Exception('content type not supported: %r' % content_type)
        
        downloadID = request.GET.get('downloadID', None)
        if downloadID:
            logger.info('set cookie "downloadID" %r', downloadID )
            response.set_cookie('downloadID', downloadID)
        else:
            logger.debug('no downloadID: %s' % request.GET )
    
        logger.info('returning response: %r', response)
        return response;

    @read_authorization
    def get_detail(self, request, **kwargs):
        # substance_id = kwargs.get('substance_id')
        well_id = kwargs.get('well_id')
        if well_id is None: # and substance_id is None:
            raise MissingParam('well_id')

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        logger.info('visibilities: %r', kwargs['visibilities'])
        kwargs['is_for_detail'] = True
        return self.get_list(request, **kwargs)


    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    @read_authorization
    def get_preview_list(self, request, **kwargs):
        ''' 
        Preview view is a special view that will mix the preview logs, if they
        exist, in with the query.
        - Admins only
        - released libraries only
        '''
        logger.info('ReagentResource: get_preview_list...')
        library_short_name = kwargs.get('library_short_name', None)
        library = Library.objects.get(short_name=library_short_name)
        if library.is_released is not True:
            raise Exception('Library has not been released')
        library_data = self.get_library_resource()\
            ._get_detail_response_internal(**{
                'short_name': library.short_name })
        if not library_data.get('preview_log_id'):
            raise Exception('no preview found for %s', library_short_name)

        kwargs[API_PARAM_SHOW_PREVIEW] = True
        
        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        
        return self.build_list_response(request, **kwargs)

        
    def _build_result_value_column(self, field_information):
        '''
        Each result value will be added to the query as a subquery select:
        (SELECT <value_field> 
         FROM result_value
         WHERE result_value.data_column_id=<id> 
         AND rv.well_id=well.well_id limit 1) as <data_column_value_alias>
        '''
        logger.debug('build result value for field: %r', field_information)
        key = field_information['key']
        data_column_id = field_information['data_column_id']
        data_column_type = field_information.get('data_type') 
        # TODO: column to select: use controlled vocabulary
        column_to_select = None
        if(data_column_type in ['numeric', 'decimal', 'integer']):  
            column_to_select = 'numeric_value'
        else:
            column_to_select = 'value'
        logger.debug('_build_result_value_column: %r, %r, %r, %r', 
            key, column_to_select, data_column_id, column_to_select)
        
        _rv = self.bridge['result_value']
        rv_select = select([column(column_to_select)]).select_from(_rv)
        rv_select = rv_select.where(
            _rv.c.data_column_id == field_information['data_column_id'])
        rv_select = rv_select.where(_rv.c.well_id == text('well.well_id'))
        # FIXME: limit to rv's to 1 - due to the duplicated result value issue
        rv_select = rv_select.limit(1)  
        rv_select = rv_select.label(key)
        return rv_select

    def get_screening_concentration_cte(self, 
            well_base_query=None, well_ids=None, plate_numbers=None, 
            library=None, cherry_pick_request_id_screener=None, 
            cherry_pick_request_id_lab=None, alias_qualifier=None):
        _well = self.bridge['well']
        _copy = self.bridge['copy']
        _p = self.bridge['plate']
        _cw = self.bridge['copy_well']
        pc_join = (_well.join(
                    _copy,_well.c.library_id==_copy.c.library_id)
                .join(_p, and_(
                    _well.c.plate_number==_p.c.plate_number,
                    _copy.c.copy_id==_p.c.copy_id)))
                # .join(_cw,
                #       and_(_cw.c.well_id==_well.c.well_id, 
                #            _cw.c.copy_id==_copy.c.copy_id),isouter=True))
        if well_base_query is not None:
            pc_join = pc_join.join(
                well_base_query, _well.c.well_id==well_base_query.c.well_id )
        if cherry_pick_request_id_screener is not None:
            pc_join = pc_join.join(_scp,
                _scp.c.screened_well_id==_well.c.well_id)
        if cherry_pick_request_id_lab is not None:
            pc_join = pc_join.join(_lcp,
                _lcp.c.source_well_id==_well.c.well_id)
        _plate_concentrations = (
            select([
                _well.c.well_id,
                _copy.c.name,
                _copy.c.usage_type,
                _copy.c.copy_id,
                func.coalesce(
                    _cw.c.mg_ml_concentration,_p.c.mg_ml_concentration,
                    _well.c.mg_ml_concentration).label('mg_ml_concentration'),
                func.coalesce(
                    _cw.c.molar_concentration,_p.c.molar_concentration,
                    _well.c.molar_concentration).label('molar_concentration'),
                _p.c.status    
                ])
            .select_from(pc_join.join(_cw,
                and_(_cw.c.well_id==_well.c.well_id,
                     _cw.c.copy_id==_copy.c.copy_id,
                     _cw.c.plate_id==_p.c.plate_id), isouter=True))
#             .where(_cw.c.well_id==_well.c.well_id)
#             .where(_cw.c.copy_id==_copy.c.copy_id)
#             .where(_cw.c.plate_id==_p.c.plate_id)
            )
        
        
        if well_ids is not None:
            _plate_concentrations = \
                _plate_concentrations.where(_well.c.well_id.in_(well_ids))
        if plate_numbers:
            _plate_concentrations = \
                _plate_concentrations.where(_well.c.plate_number.in_(plate_numbers))
        if library:
            _plate_concentrations = \
                _plate_concentrations.where(_well.c.library_id == library.library_id) 
        if cherry_pick_request_id_screener is not None:
            _plate_concentrations = stmt.where(
                _scp.c.cherry_pick_request_id
                    ==cherry_pick_request_id_screener)
        if cherry_pick_request_id_lab is not None:
            _plate_concentrations = stmt.where(
                _plate_concentrations.c.cherry_pick_request_id
                    ==cherry_pick_request_id_lab)
        
        spcs = _plate_concentrations.where(
            _copy.c.usage_type== 'library_screening_plates')
        alias_name = 'spcs'
        if alias_qualifier:
            alias_name += '_'+ alias_qualifier
        spcs = spcs.cte(alias_name)

        # compiled_stmt = str(spcs.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)            
        
        return spcs
    
    
    def build_sqlalchemy_columns(
        self, fields, user=None, base_query_tables=None, custom_columns=None, 
        show_preview=False, show_restricted=False):

        logger.info(
            'build_sqlalchemy_columns, reagent resource: show_preview: %r, auth: %r', 
            show_preview, self._meta.authorization._is_resource_authorized(user, 'read'))
        
        columns = DbApiResource.build_sqlalchemy_columns(
            self, fields, base_query_tables=base_query_tables, custom_columns=custom_columns)
        logger.debug('columns: %r, fields: %r', columns.keys(), [f['key'] for f in fields])
        if show_preview is True \
            and self._meta.authorization._is_resource_authorized(user, 'read') \
                is True:

            # When show_preview is True, "preview" Apilog.logdiff.after values
            # are overlayed on standard fields to generate a "preview" view

            logger.info('building reagent/well preview columns...')

            _apilog = self.bridge['reports_apilog']
            _diff = self.bridge['reports_logdiff']

            # decode the diffs
            rnames = [
                SCHEMA.REAGENT.resource_name, SCHEMA.WELL.resource_name,
                SCHEMA.SMALL_MOLECULE_REAGENT.resource_name, 
                SCHEMA.SILENCING_REAGENT.resource_name, ]
                # TODO natural_product_reagent]
            # Note: all well/reagent logs are logged for the "well" resource
            logging_rname = WELL.resource_name
            for field in fields:
                key = field[FIELD.KEY]
                editability = field.get('editability', [])
                if 'u' not in editability:
                    continue
                
                if field['scope'] in ['fields.%s'% rname for rname in rnames]:
                    logger.debug('building preview field: %s', key)
                    columns[key] = func.coalesce(
                        select([_diff.c.after])
                            .select_from(_apilog.join(_diff, _apilog.c.id==_diff.c.log_id))
                            .where(_apilog.c.is_preview==True)
                            .where(_apilog.c.ref_resource_name==logging_rname)
                            .where(_apilog.c.key==literal_column('well.well_id'))
                            .where(_diff.c.field_key==key).as_scalar(),
                        cast(columns[key], sqlalchemy.sql.sqltypes.Text)).label(key)
        return columns
        
        
    def get_query(self, 
        param_hash, user, library=None,
        cherry_pick_request_id_screener=None,
        cherry_pick_request_id_lab=None, well_ids=None, 
        well_base_query=None, schema=None, show_preview=False):
        logger.info('get reagent query for user %r', user )
        
        if schema is None:
            logger.info('schema not specified, rebuilding...')
            schema = self.build_schema(user)

        # general setup
        manual_field_includes = set(param_hash.get('includes', []))
        manual_field_includes.add('screen_type')
        # always include plate number and well name for ordering
        manual_field_includes.add('plate_number')
        manual_field_includes.add('well_name')
        manual_field_includes.discard('-plate_number') 
        manual_field_includes.discard('-well_name') 
        
        plate_numbers = param_hash.pop('plate_number__in',None)

        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filename = self._get_filename(
            readable_filter_hash, schema, 
            param_hash.get('is_for_detail',False))
        
        if filter_expression is None\
            and any([well_ids,plate_numbers,library,
                well_base_query is not None, cherry_pick_request_id_lab, 
                cherry_pick_request_id_screener]) is False:
            logger.warn('No filters found: param_hash: %r', param_hash)
            logger.warn('No filters found: schema: %r, fields: %r',
                schema['key'], schema['fields'].keys())
            raise InformationError(
                key='Input filters ',
                msg='Please enter a filter expression to begin')
        filter_expression = \
            self._meta.authorization.filter(user,filter_expression)
             
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        logger.debug('field hash scopes: %r',
            set([field.get('scope', None) 
                for field in field_hash.values()]))
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
         
        # specific setup 
    
        base_query_tables = ['well', 'reagent', 'library']

        _well = self.bridge['well']
        _reagent = self.bridge['reagent']
        _or = _reagent.alias('other_reagent')
        _reagent2 = _reagent.alias('reagent2')
        _sr = self.bridge['silencing_reagent']
        _smr = self.bridge['small_molecule_reagent']
        _molfile = self.bridge['molfile']
        _library = self.bridge['library']
        _scp = self.bridge['screener_cherry_pick']
        _lcp = self.bridge['lab_cherry_pick']
        
        j = _well
        if well_base_query is not None:
            j = well_base_query
            j = j.join(_well, _well.c.well_id==well_base_query.c.well_id )
        j = j.join(
            _reagent, _well.c.well_id == _reagent.c.well_id, isouter=True)
        j = j.join(_library, _well.c.library_id == _library.c.library_id)
                
        custom_columns = {}

        # screening copy concentrations
        
        spcs = self.get_screening_concentration_cte(
            well_base_query, well_ids, plate_numbers, library, 
            cherry_pick_request_id_screener, cherry_pick_request_id_lab)
        custom_columns['screening_mg_ml_concentration'] = case([
            (_well.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                select([spcs.c.mg_ml_concentration])
                    .select_from(spcs)
                    .where(spcs.c.well_id==_well.c.well_id).limit(1).as_scalar() )],
            else_=None)
        custom_columns['screening_mg_ml_concentrations'] = case([
            (_well.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                select([func.array_to_string(func.array_agg(
                    cast(spcs.c.mg_ml_concentration,sqlalchemy.sql.sqltypes.Text)),
                    LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(spcs)
                    .where(spcs.c.well_id==_well.c.well_id).as_scalar() )],
            else_=None)
        custom_columns['screening_molar_concentration'] = case([
            (_well.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                select([spcs.c.molar_concentration])
                    .select_from(spcs)
                    .where(spcs.c.well_id==_well.c.well_id).limit(1).as_scalar() )],
            else_=None)
        custom_columns['screening_molar_concentrations'] = case([
            (_well.c.library_well_type==WELL_TYPE.EXPERIMENTAL, 
                select([func.array_to_string(
                    func.array_agg(distinct(spcs.c.molar_concentration)),
                    LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(spcs)
                    .where(spcs.c.well_id==_well.c.well_id).as_scalar() )],
            else_=None)
        
        custom_columns['other_wells_with_reagent'] = (
            select([func.array_to_string(func.array_agg(
                literal_column('well_id')), LIST_DELIMITER_SQL_ARRAY )])
            .select_from(
                select([_or.c.well_id])
                .select_from(_or)
                .where(and_(
                    _or.c.vendor_identifier != '',
                    _or.c.vendor_identifier==text('reagent.vendor_identifier'),
                    _or.c.vendor_name==text('reagent.vendor_name'),
                    _or.c.well_id != text('reagent.well_id')))
                .order_by(_or.c.well_id)
                .alias('other_reagents'))
            )
                        
        if 'pool_well' in field_hash:
            # NOTE: breaks OO inhertance composition
            # Required for performance
            _duplex_wells = self.bridge['silencing_reagent_duplex_wells']
            _pool_reagent = self.bridge['reagent']
            
            inner_join = (
                _duplex_wells.join(
                    _pool_reagent, _pool_reagent.c.reagent_id==
                        _duplex_wells.c.silencingreagent_id)
                .join(_well, _duplex_wells.c.well_id==_well.c.well_id)
                .join(_library, _well.c.library_id==_library.c.library_id))
            if well_base_query is not None:
                inner_join = inner_join.join(
                    well_base_query, _well.c.well_id==well_base_query.c.well_id )
            pool_wells = (
                select([
                    _pool_reagent.c.well_id.label('pool_well_id'),
                    _duplex_wells.c.well_id
                    ])
                .select_from(inner_join))
            if library:
                pool_wells = pool_wells.where(_well.c.library_id==library.library_id)
                    
            pool_wells = pool_wells.cte('pool_wells')
            # NOTE: array_agg must be used
            # Some duplex wells are pointed to by multiple pool wells:
            # CPR44311, lcp report shows multple rows 
            # returned from subquery
            # e.g.: 50471:G08 - has 50599:I17,50599:I17...
            # (Mouse4 Pools: Remaining Genome)
            # 50448:C18 - has 50599:I17,50599:I17... 
            # (Mouse4 Pools: Remaining Genome)

            # j = j.join(
            #     pool_wells, pool_wells.c.well_id==_well.c.well_id,
            #     isouter=True )                                
            custom_columns['pool_well'] = (
                select([func.array_to_string(
                    func.array_agg(pool_wells.c.pool_well_id),
                    LIST_DELIMITER_SQL_ARRAY)])
                    .select_from(pool_wells)
                    .where(pool_wells.c.well_id==_well.c.well_id))
        if 'is_pool' in field_hash:
            custom_columns['is_pool'] = _library.c.is_pool
            
        if cherry_pick_request_id_screener is not None:
            j = j.join(_scp,
                _scp.c.screened_well_id==_well.c.well_id)
        if cherry_pick_request_id_lab is not None:
            j = j.join(_lcp,
                _lcp.c.source_well_id==_well.c.well_id)

        for fi in field_hash.values():
            if fi.get('is_datacolumn',False) is True:
                custom_columns[fi['key']] = self._build_result_value_column(fi)

        logger.debug('build_sqlalchemy_columns: %r', base_query_tables)
        logger.debug('build_sqlalchemy_columns: includes: %r, fields: %r', 
                    manual_field_includes, field_hash.keys())

        show_restricted = parse_val(
            param_hash.get(API_PARAM_SHOW_RESTRICTED, False),
            API_PARAM_SHOW_RESTRICTED,'boolean')
        
        columns = self.build_sqlalchemy_columns(
            field_hash.values(), user=user, base_query_tables=base_query_tables,
            custom_columns=custom_columns, show_preview=show_preview,
            show_restricted=show_restricted )

        if self._meta.resource_name == 'reagent':
            # TODO: 20181115: 
            # This is a hack to build a composite schema containing all the 
            # subtype field definitions;
            # ResourceResouce code should be modified to build this?
            new_columns = {}
            new_columns.update(columns)
            sub_columns = self.get_sr_resource().build_sqlalchemy_columns(
                field_hash.values(), user=user, base_query_tables=base_query_tables,
                custom_columns=custom_columns, show_preview=show_preview,
                show_restricted=show_restricted )
            new_columns.update(sub_columns)
            sub_columns = self.get_smr_resource().build_sqlalchemy_columns(
                field_hash.values(), user=user, base_query_tables=base_query_tables,
                custom_columns=custom_columns, show_preview=show_preview,
                show_restricted=show_restricted )
            new_columns.update(sub_columns)
            sub_columns = self.get_npr_resource().build_sqlalchemy_columns(
                field_hash.values(), user=user, base_query_tables=base_query_tables,
                custom_columns=custom_columns, show_preview=show_preview,
                show_restricted=show_restricted )
            new_columns.update(sub_columns)
            columns = new_columns

        # is_released is required for the authorization filter
        columns['is_released'] = _library.c.is_released
        
        stmt = select(columns.values()).select_from(j)
        if well_ids is not None:
            stmt = stmt.where(_well.c.well_id.in_(well_ids))
        if plate_numbers:
            stmt = stmt.where(_well.c.plate_number.in_(plate_numbers))
        if library:
            stmt = stmt.where(_well.c.library_id == library.library_id) 
        if cherry_pick_request_id_screener is not None:
            stmt = stmt.where(
                _scp.c.cherry_pick_request_id
                    ==cherry_pick_request_id_screener)
        if cherry_pick_request_id_lab is not None:
            stmt = stmt.where(
                _lcp.c.cherry_pick_request_id
                    ==cherry_pick_request_id_lab)
            
        # general setup
         
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        if not order_clauses:
            stmt = stmt.order_by("plate_number", "well_name")

        # compiled_stmt = str(stmt.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)
            
        return (field_hash, columns, stmt, count_stmt,filename)
    
    @read_authorization
    def dispatch_search_vendor_and_compound(self, request, **kwargs):
        logger.info('dispatch search_vendor_and_compound...')
        # Set a flag to indicate the search type
        kwargs['vendor_and_compound_search'] = True
        
        if SCHEMA.API_PARAM_COMPLEX_SEARCH_ID in kwargs:
            return self.search(request, **kwargs)
        else:
            return self.get_list(request,**kwargs)

    @read_authorization
    def build_list_response(self, request, **kwargs):
        
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        param_hash.pop('schema', None)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        
        is_for_detail = kwargs.pop('is_for_detail', False)
        show_preview = kwargs.pop(API_PARAM_SHOW_PREVIEW, False)
        
        # well search data is raw line based text entered by the user
        well_search_data = param_hash.pop(SCHEMA.API_PARAM_SEARCH, None)
        well_base_query = None
        if well_search_data is not None:
            logger.info('well_search_data: %r', well_search_data)
            if param_hash.pop('vendor_and_compound_search', False) is True:
                well_base_query = \
                    WellResource.create_vendor_compound_name_base_query(well_search_data)
            else:
                parsed_searches = WellResource.parse_well_search(well_search_data)
                well_base_query = WellResource.create_well_base_query(parsed_searches)
        parsed_well_search_data = param_hash.pop(SCHEMA.API_PARAM_NESTED_SEARCH, None)
        if parsed_well_search_data is not None:
            if well_base_query is not None:
                raise Exception('May not specify both %r and %r', 
                    SCHEMA.API_PARAM_SEARCH, SCHEMA.API_PARAM_NESTED_SEARCH)
            else:
                well_base_query = \
                    WellResource.create_well_base_query(parsed_well_search_data)
        if well_base_query is not None:
            well_base_query = well_base_query.cte('well_base_query')

                
        # TODO: eliminate dependency on library (for schema determination)
        library = None
        library_short_name = param_hash.pop('library_short_name', None)
        if not library_short_name:
            logger.info('no library_short_name provided')
        else:
            param_hash['library_short_name__eq'] = library_short_name
            library = Library.objects.get(short_name=library_short_name)
        
        well_id = param_hash.pop('well_id', None)
        if well_id:
            param_hash['well_id__eq'] = well_id
            logger.info('well_id: %r', well_id)
            if not library:
                library = Well.objects.get(well_id=well_id).library
        cherry_pick_request_id_screener = \
            param_hash.pop('cherry_pick_request_id_screener', None)
        if cherry_pick_request_id_screener is not None:
            cpr = CherryPickRequest.objects.get(
                cherry_pick_request_id=cherry_pick_request_id_screener)
            # NOTE: breaks for viewing natural product reagents
        cherry_pick_request_id_lab = \
            param_hash.pop('cherry_pick_request_id_lab', None)
        if cherry_pick_request_id_lab is not None:
            cpr = CherryPickRequest.objects.get(
                cherry_pick_request_id=cherry_pick_request_id_lab)
            # NOTE: breaks for viewing natural product reagents
        
        # substance_id = param_hash.pop('substance_id', None)
        # if substance_id:
        #     param_hash['substance_id__eq'] = substance_id
        #     if not library:
        #         library = Reagent.objects.get(
        #             substance_id=substance_id).well.library

        extra_dc_ids = param_hash.get(API_PARAM_DC_IDS, None)
        logger.info('build list response: dc_ids: %r', extra_dc_ids)
        # Note: build schema for each request to use the subtype
        schema = self.build_schema(request.user, extra_dc_ids=extra_dc_ids)
            
        manual_field_includes = set(param_hash.get('includes', []))
        # 20180426 - eliminate substance_id from viewing (TODO: remove from api?)
        # manual_field_includes.add('-substance_id')
        
        content_type = self.get_serializer().get_accept_content_type(
            request, format=kwargs.get('format', None))
        if content_type == SDF_MIMETYPE:
            manual_field_includes.add('molfile')
        param_hash['includes'] = manual_field_includes
        
        (field_hash, columns, stmt, count_stmt, filename) = \
            self.get_query(
                param_hash, request.user,
                library=library,
                cherry_pick_request_id_screener=cherry_pick_request_id_screener,
                cherry_pick_request_id_lab=cherry_pick_request_id_lab,
                well_base_query=well_base_query, 
                schema=schema, show_preview=show_preview)
        
        rowproxy_generator = None
        
        if show_preview is True:
            # When show_preview is True, "preview" Apilog.logdiff.after values
            # are overlayed on standard fields:
            # Create a Row proxy that will decode values embedded as JSON in the field
            rnames = [
                SCHEMA.REAGENT.resource_name, SCHEMA.WELL.resource_name,
                SCHEMA.SMALL_MOLECULE_REAGENT.resource_name, 
                SCHEMA.SILENCING_REAGENT.resource_name, ]
                # TODO natural_product_reagent]
            list_preview_fields = []
            for field in field_hash.values():
                key = field[FIELD.KEY]
                if key not in columns:
                    continue
                editability = field.get('editability', [])
                if 'u' not in editability:
                    continue
                if field['scope'] in ['fields.%s'% rname for rname in rnames]:
                    if field[FIELD.DATA_TYPE] == SCHEMA.VOCAB.field.data_type.LIST:
                        list_preview_fields.append(key)
                    
            if list_preview_fields:
                rowproxy_generator = \
                    ApiLogResource.create_apilog_preview_rowproxy_generator(
                        list_preview_fields, rowproxy_generator)
            
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(
                    field_hash, rowproxy_generator)
            # use "use_vocab" as a proxy to also adjust siunits for viewing
            rowproxy_generator = DbApiResource.create_siunit_rowproxy_generator(
                field_hash, rowproxy_generator)
        # FIXME: this applies the auth row generator before the value_template,
        # and other generators have been applied; the auth gen should be applied
        # last (ok, because build_sqlalchemy_columns is being used to restrict 
        # in this case) - sde
        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)

        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
            
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash, param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None))
    
    
    def delete_reagents_for_library(self, library):
        self.get_reagent_resource(library.classification).delete_reagents(library)

    # NOTE: removed; patching will only be done in batch, from library/well
    # @transaction.atomic()    
    # def patch_obj(self, request, deserialized, **kwargs):
    #     well = kwargs.get('well', None)
    #     if not well:
    #         raise ValidationError(key='well', msg='required')
    #     
    #     reagent_resource = self.get_reagent_resource(
    #         well.library.classification)
    #     reagent = reagent_resource.patch_obj(request, deserialized, **kwargs)
    #     
    #     reagent.well = well
    #     reagent.save()
    #     return well
    
    def final_validation(self,final_data):
        ''' Perform final validations on the data generated after loading'''

        errors = {}
 
        if bool(final_data[SCHEMA.REAGENT.VENDOR_NAME]) != \
            bool(final_data[SCHEMA.REAGENT.VENDOR_IDENTIFIER]):
            msg = (
                '%s must both be specified, or neither should be specified'
                % ' and '.join([SCHEMA.REAGENT.VENDOR_NAME, 
                               SCHEMA.REAGENT.VENDOR_IDENTIFIER]))
            errors[SCHEMA.REAGENT.VENDOR_NAME] = msg
            errors[SCHEMA.REAGENT.VENDOR_IDENTIFIER] = msg
        
        return errors

class SilencingReagentResource(ReagentResource):
    
    class Meta:
    
        queryset = Reagent.objects.all()
        authentication = MultiAuthentication(
            IccblBasicAuthentication(), IccblSessionAuthentication())
        resource_name = 'silencingreagent'
        authorization = ReagentResourceAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        
    def __init__(self, **kwargs):

        super(SilencingReagentResource, self).__init__(**kwargs)
        
        # for debugging
        self.patch_elapsedtime1 = 0
        self.patch_elapsedtime2 = 0
        self.patch_elapsedtime3 = 0
        
    def get_debug_times(self):
        
        logger.info('silencing reagent times: %r, %r, %r', 
            self.patch_elapsedtime1, self.patch_elapsedtime2, self.patch_elapsedtime3)
        self.patch_elapsedtime1 = 0
        self.patch_elapsedtime2 = 0
        self.patch_elapsedtime3 = 0

    def delete_reagents(self, library):
        
        SilencingReagent.objects.all().filter(well__library=library).delete()
    
    
#     def build_gene_columns(self, custom_columns):
#         vendor_gene_columns = [
#             'vendor_entrezgene_id', 'vendor_gene_name', 'vendor_gene_species']
#         facility_gene_columns = [
#             'facility_entrezgene_id', 'facility_gene_name',
#             'facility_gene_species']
#         bridge = self.bridge
#         gene_table = bridge['gene']
#         sirna_table = bridge['silencing_reagent']
#         gene_symbol = bridge['gene_symbol']
#         genbank_acc = bridge['gene_genbank_accession_number']
#         well_table = bridge['well']
#         library_table = bridge['library']
#         
#         join_stmt = gene_table.join(sirna_table,
#             gene_table.c['gene_id'] == sirna_table.c[join_column])
#         select_stmt = select([gene_table.c[field_name]]).\
#             select_from(join_stmt)
#         select_stmt = select_stmt.where(
#             text('silencing_reagent.reagent_id=reagent.reagent_id'))
#         select_stmt = select_stmt.label(key)
#         custom_columns[key] = select_stmt
        
        
    def build_sqlalchemy_columns(
        self, fields, user=None, base_query_tables=None, custom_columns=None, 
        show_preview=False, show_restricted=False):
        '''
        @return an array of sqlalchemy.sql.schema.Column objects
        @param fields - field definitions, from the resource schema
        
        TODO: 20180426 - reimplement to remove "linked" fields from the schema:
        - sequence, anti_sense_sequence, is_restricted_sequence, 
        silencing_reagent_type
        
        '''
        logger.debug(
            'build silencing reagent columns for: %r', 
            [field['key'] for field in fields])
        DEBUG_BUILD_COLS = False or logger.isEnabledFor(logging.DEBUG)
        
        if custom_columns is None:
            custom_columns = {}

        vendor_gene_columns = [
            'vendor_entrezgene_id', 'vendor_gene_name', 'vendor_gene_species']
        vendor_gene_symbols = 'vendor_entrezgene_symbols'
        vendor_genebank_accession_numbers = 'vendor_genbank_accession_numbers'
        facility_gene_columns = [
            'facility_entrezgene_id', 'facility_gene_name',
            'facility_gene_species']
        facility_gene_symbols = 'facility_entrezgene_symbols'
        facility_genebank_accession_numbers = 'facility_genbank_accession_numbers'
        
        vendor_columns = set(vendor_gene_columns)
        vendor_columns.add(vendor_gene_symbols)
        vendor_columns.add(vendor_genebank_accession_numbers)
        
        facility_columns = set(facility_gene_columns)
        facility_columns.add(facility_gene_symbols)
        facility_columns.add(facility_genebank_accession_numbers)
        
        bridge = self.bridge
        gene_table = bridge['gene']
        sirna_table = bridge['silencing_reagent']
        gene_symbol = bridge['gene_symbol']
        genbank_acc = bridge['gene_genbank_accession_number']
        well_table = bridge['well']
        library_table = bridge['library']
        
        # Example:
        # (select gene_name from gene 
        #     join silencing_reagent on(gene_id=vendor_gene_id) 
        #     where silencing_reagent.reagent_id = reagent.reagent_id ) as vendor_gene_name
        
        for field in fields:
            field_name = field.get('field', None)
            if not field_name:
                field_name = field['key']
            key = field['key']
            if DEBUG_BUILD_COLS: 
                logger.info('field[key]: %r, %r, %r', field['key'])
            join_stmt = None
            join_column = None
            if field['key'] in vendor_columns:
                join_column = 'vendor_gene_id'
            if field['key'] in facility_columns:
                join_column = 'facility_gene_id'

            if key in vendor_gene_columns or \
                    key in facility_gene_columns:
                join_stmt = gene_table.join(sirna_table,
                    gene_table.c['gene_id'] == sirna_table.c[join_column])
                select_stmt = select([gene_table.c[field_name]]).\
                    select_from(join_stmt)
                select_stmt = select_stmt.where(
                    text('silencing_reagent.reagent_id=reagent.reagent_id'))
                select_stmt = select_stmt.label(key)
                custom_columns[key] = select_stmt

            if key == vendor_gene_symbols or \
                    key == facility_gene_symbols:
                join_stmt = gene_symbol.join(gene_table,
                    gene_symbol.c['gene_id'] == gene_table.c['gene_id'])
                join_stmt = join_stmt.join(sirna_table,
                    gene_table.c['gene_id'] == sirna_table.c[join_column])
                
                select_inner = select([gene_symbol.c[field_name]]).\
                    select_from(join_stmt)
                ordinal_field = field.get('ordinal_field', None)
                if ordinal_field:
                    select_inner = select_inner.order_by(
                        gene_symbol.c[ordinal_field])
                select_inner = select_inner.where(
                    text('silencing_reagent.reagent_id=reagent.reagent_id'))
                select_inner = select_inner.alias('a')
                select_stmt = select([
                    func.array_to_string(func.array_agg(
                        column(field_name)),LIST_DELIMITER_SQL_ARRAY)])
                select_stmt = select_stmt.select_from(select_inner)
                select_stmt = select_stmt.label(key)
                custom_columns[key] = select_stmt

            if key == vendor_genebank_accession_numbers or \
                    key == facility_genebank_accession_numbers:
                join_stmt = genbank_acc.join(gene_table,
                    genbank_acc.c['gene_id'] == gene_table.c['gene_id'])
                join_stmt = join_stmt.join(sirna_table,
                    gene_table.c['gene_id'] == sirna_table.c[join_column])
                
                select_inner = select([genbank_acc.c[field_name]]).\
                    select_from(join_stmt)
                select_inner = select_inner.where(
                    text('silencing_reagent.reagent_id=reagent.reagent_id'))
                select_inner = select_inner.alias('a')
                select_stmt = select([func.array_to_string(
                    func.array_agg(column(field_name)),LIST_DELIMITER_SQL_ARRAY)])
                select_stmt = select_stmt.select_from(select_inner)
                select_stmt = select_stmt.label(key)
                custom_columns[key] = select_stmt
            
            if key == 'duplex_wells':
                _duplex_wells = bridge['silencing_reagent_duplex_wells']
                select_inner = select([_duplex_wells.c['well_id']]).\
                    select_from(_duplex_wells)
                select_inner = select_inner.where(
                    text('silencingreagent_id=reagent.reagent_id'))
                select_inner = select_inner.order_by(_duplex_wells.c['well_id'])
                select_inner = select_inner.alias('a')
                select_stmt = select([func.array_to_string(
                    func.array_agg(column(field_name)),LIST_DELIMITER_SQL_ARRAY)])
                select_stmt = select_stmt.select_from(select_inner)
                select_stmt = select_stmt.label(key)
                custom_columns[key] = select_stmt
                
                if DEBUG_BUILD_COLS:
                    logger.info(str((select_stmt)))
            
            if key == 'pool_well':
                # NOTE: this has been moved to ReagentResource:
                pass
                # NOT Performant - not used, see ReagentResource pool_well definition
                # _duplex_wells = bridge['silencing_reagent_duplex_wells']
                # pool_reagent = bridge['reagent'].alias('pool_reagent')
                #  
                # custom_columns[key] = (
                #     select([func.array_to_string(
                #         func.array_agg(literal_column('well_id')),
                #         LIST_DELIMITER_SQL_ARRAY)])
                #     .select_from(
                #         select([pool_reagent.c.well_id])
                #         .select_from(_duplex_wells.join(
                #             pool_reagent,pool_reagent.c.reagent_id==
                #                 _duplex_wells.c.silencingreagent_id))
                #         .where(_duplex_wells.c.well_id==text('reagent.well_id')).alias('inner_pool')
                #     )
                # # Some duplex wells are pointed to by multiple pool wells:
                # # CPR44311, lcp report shows multple rows 
                # # returned from subquery
                # # e.g.: 50471:G08 - has 50599:I17,50599:I17...
                # # (Mouse4 Pools: Remaining Genome)
                # # 50448:C18 - has 50599:I17,50599:I17... 
                # # (Mouse4 Pools: Remaining Genome)
                # )
            if key == 'is_pool':
                # NOTE: this has been moved to ReagentResource:
                pass
        
        
        # 20180314 - restrict on is_restricted_sequence, is_restricted_structure
        rna_restricted_fields = ['sequence','anti_sense_sequence']
        fields_to_restrict = set(rna_restricted_fields)&set(
            [field['key'] for field in fields])
        if ( show_preview is not True 
                and self._meta.authorization.is_restricted_view(user) 
                    or show_restricted is not True):
            if fields_to_restrict:
                logger.info('RNAi fields to restrict: %r', fields_to_restrict)
                for field in fields_to_restrict:
                    if field == 'sequence':
                        custom_columns['sequence'] = (
                            select([
                                case([
                                    (sirna_table.c.is_restricted_sequence,
                                        SCHEMA.API_MSG_RESTRICTED_DATA)],
                                    else_=sirna_table.c.sequence)])
                            .select_from(sirna_table)
                            .where(sirna_table.c.reagent_id
                                == literal_column('reagent.reagent_id'))
                            )
                    if field == 'anti_sense_sequence':
                        custom_columns['anti_sense_sequence'] = (
                            select([
                                case([
                                    (sirna_table.c.is_restricted_sequence,
                                        SCHEMA.API_MSG_RESTRICTED_DATA)],
                                    else_=sirna_table.c.anti_sense_sequence)])
                            .select_from(sirna_table)
                            .where(sirna_table.c.reagent_id
                                == literal_column('reagent.reagent_id'))
                            )
        
        if DEBUG_BUILD_COLS: 
            logger.info('sirna custom_columns: %r', custom_columns.keys())
            logger.info('silencingreagent base_query_tables: %r', base_query_tables)    
        
        
        return super(SilencingReagentResource, self).build_sqlalchemy_columns(
            fields, user=user, base_query_tables=base_query_tables, 
            custom_columns=custom_columns, show_preview=show_preview)  

    @write_authorization
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'patch_obj')
        # NOTE: patching will only be done in batch, from library/well
 
    def _patch_wells(self, request, deserialized):
        ''' For bulk update: 
        - deserialized has been loaded with the well & duplex wells
        NOTE: patching will only be done in batch, from library/well
        '''
        logger.info('patch (%d) reagents for silencing_reagent ...', len(deserialized))
        
        metadata = {}
        schema = self.build_schema(request.user)
        fields = schema['fields']

        cumulative_error = CumulativeError()
            
        for i,well_data in enumerate(deserialized):
            well = well_data['well']
            logger.debug('patch sirna well: %r', well.well_id)
            is_patch = False
            if not well.reagents.exists():
                reagent = SilencingReagent(well=well)
                if well_data.get('duplex_wells', None):
                    reagent.save()
                    reagent.duplex_wells = well_data['duplex_wells']
            else:
                is_patch = True
                # TODO: only works for a single reagent
                # can search for the reagent using id_kwargs
                # reagent = well.reagents.all().filter(**id_kwargs)
                # TODO: update reagent
                reagent = well.reagents.all()[0]
                reagent = reagent.silencingreagent
                logger.debug('found reagent: %r, %r', reagent.well_id, reagent)
            try:
                self._set_reagent_values(reagent, well_data, is_patch, schema, fields)
                if (i+1) % 1000 == 0:
                    logger.info('patched %d reagents', i+1)
            except ValidationError, e:
                cumulative_error.add_error(well.well_id, e.errors, 
                    line=well_data.get(INPUT_FILE_DESERIALIZE_LINE_NUMBER_KEY, i))
            
        if cumulative_error.errors:
            raise cumulative_error
        
        logger.info('patched %d reagents', i+1)
        return metadata

    def _set_reagent_values(self, reagent, deserialized, is_patch, schema, fields):
        
        start_time = time.time()

        initializer_dict = self.parse(deserialized, fields=fields)
        errors = self.validate(initializer_dict, patch=is_patch, fields=fields)
        if errors:
            raise ValidationError(errors)

        self.patch_elapsedtime1 += (time.time() - start_time)
        start_time = time.time()
        
        related_fields = ['entrezgene_id', 'gene_name', 'species_name']
        related_fields = ['facility_%s'%field for field in related_fields] + \
            ['vendor_%s'%field for field in related_fields]
        for key, val in initializer_dict.items():
            if key not in related_fields and hasattr(reagent, key):
                setattr(reagent, key, val)
        reagent.save()
        logger.debug('patch silencing reagent: %r', reagent)
        logger.debug('patch silencing reagent: %r: %r', reagent, initializer_dict)

        self.patch_elapsedtime2 += (time.time() - start_time)
        start_time = time.time()

        # Now do the gene tables
        
        gene_key = 'entrezgene_id'
        if deserialized.get('vendor_%s' % gene_key, None):
            logger.debug('create gene for %r', reagent.well.well_id)
            reagent.vendor_gene = \
                self._create_gene(deserialized, 'vendor')
        if deserialized.get('facility_%s' % gene_key, None):
            reagent.facility_gene = \
                self._create_gene(deserialized, 'facility')
        reagent.save()
        
        self.patch_elapsedtime3 += (time.time() - start_time)
        
    def final_validation(self, final_data):
        ''' Perform final validations on the data generated after loading'''
        
        errors = {}
        logger.debug('silencingreagent final validation: %r', final_data)
        
        # NOTE: following SS1 validation rule for sequence
        if final_data[SCHEMA.SILENCING_REAGENT.SEQUENCE]:
            msg = 'Required if sequence is specified'
            if not final_data[SCHEMA.REAGENT.VENDOR_IDENTIFIER]:
                errors[SCHEMA.REAGENT.VENDOR_IDENTIFIER] = msg
            if not final_data[SCHEMA.SILENCING_REAGENT.SILENCING_REAGENT_TYPE]:
                errors[SCHEMA.SILENCING_REAGENT.SILENCING_REAGENT_TYPE] = msg
                
        errors.update(
            super(SilencingReagentResource, self).final_validation(final_data))
        return errors
    
    def _create_gene(self, data, source_type):
        
        gene_keys = ['entrezgene_id', 'gene_name', 'species_name']
        gene = Gene()
        for key in gene_keys:
            api_key = '%s_%s' % (source_type, key)
            val = data.get(api_key, None)
            if val:
                setattr(gene, key, val)
        gene.save()
        
        _key = 'entrezgene_symbols'
        if data.get('%s_%s' % (source_type, _key), None):
            symbol_list = data['%s_%s' % (source_type, _key)]
            for i, symbol in enumerate(symbol_list):
                gene_symbol = GeneSymbol()
                setattr(gene_symbol, 'entrezgene_symbol', symbol)
                setattr(gene_symbol, 'ordinal', i)
                setattr(gene_symbol, 'gene', gene)
                gene_symbol.save()
    
        _key = 'genbank_accession_numbers'
        
        if data.get('%s_%s' % (source_type, _key), None):
            _list = data['%s_%s' % (source_type, _key)]
            logger.debug('create genbank_accession_number: %r, %r', _list, gene)
            for i, num in enumerate(_list):
                accession_number = GeneGenbankAccessionNumber()
                setattr(accession_number, 'genbank_accession_number', num)
                setattr(accession_number, 'gene', gene)
                accession_number.save()
        
        return gene

class SmallMoleculeReagentResource(ReagentResource):

    class Meta:
        authentication = MultiAuthentication(
            IccblBasicAuthentication(), IccblSessionAuthentication())
        resource_name = 'smallmoleculereagent' 
        authorization = ReagentResourceAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        excludes = []  # ['json_field']
        always_return_data = True  # this makes Backbone happy

    def __init__(self, **kwargs):
        super(SmallMoleculeReagentResource, self).__init__(**kwargs)
        # for debugging
        self.patch_elapsedtime1 = 0
        self.patch_elapsedtime2 = 0
        self.patch_elapsedtime3 = 0
        
    def get_debug_times(self):
        
        logger.info('sm reagent times: %r, %r, %r', 
            self.patch_elapsedtime1, self.patch_elapsedtime2, 
            self.patch_elapsedtime3)
        self.patch_elapsedtime1 = 0
        self.patch_elapsedtime2 = 0
        self.patch_elapsedtime3 = 0

    def delete_reagents(self, library):
        
        SmallMoleculeReagent.objects.all().filter(well__library=library).delete()

    def build_sqlalchemy_columns(
        self, fields, user=None, base_query_tables=None, 
        custom_columns=None, show_preview=False, show_restricted=False):
        '''
        @return an array of sqlalchemy.sql.schema.Column objects
        @param fields - field definitions, from the resource schema
        
        FIXME: should restrict fields using the auth.rowproxy?
        '''
        
        logger.info(
            'build_sqlalchemy_columns for small_molecule_reagent: user: %r', user)
        if custom_columns is None:
            custom_columns = {}

        reagent_table = self.bridge['reagent']

        custom_columns['structure_image'] = reagent_table.c.well_id
        
        smr_restricted_fields = ['smiles', 'inchi', 'molecular_formula',
            'molecular_weight','molecular_mass','molfile','structure_image']
        fields_to_restrict = \
            set(smr_restricted_fields)&set([field[FIELD.KEY] for field in fields])
        
        logger.info('fields to restrict: %r, %r, %r, %r, %r', 
            fields_to_restrict, user, self._meta.authorization.is_restricted_view(user),
            show_preview, show_restricted)
        if ( show_preview is not True
                and (self._meta.authorization.is_restricted_view(user) 
                    or show_restricted is not True)):
            if fields_to_restrict:
                
                logger.info('SM fields to restrict: %r', fields_to_restrict)
    
                _smr = self.bridge['small_molecule_reagent']
                _reagent = self.bridge['reagent']
                _molfile = self.bridge['molfile']
                
                for field in fields_to_restrict:
                    logger.info('restricting field %r', field)
                    if field == 'smiles':
                        custom_columns['smiles'] = (
                            select([
                                case([
                                    (_smr.c.is_restricted_structure,
                                        SCHEMA.API_MSG_RESTRICTED_DATA)],
                                    else_=_smr.c.smiles)])
                            .select_from(_smr)
                            .where(_smr.c.reagent_id==_reagent.c.reagent_id)
                            )
                    if field == 'inchi':
                        custom_columns['inchi'] = (
                            select([
                                case([
                                    (_smr.c.is_restricted_structure,
                                        SCHEMA.API_MSG_RESTRICTED_DATA)],
                                    else_=_smr.c.inchi)])
                            .select_from(_smr)
                            .where(_smr.c.reagent_id==_reagent.c.reagent_id)
                            )
                    if field == 'molecular_weight':
                        custom_columns['molecular_weight'] = (
                            select([
                                case([
                                    (_smr.c.is_restricted_structure,None)],
                                    else_=_smr.c.molecular_weight)])
                            .select_from(_smr)
                            .where(_smr.c.reagent_id==_reagent.c.reagent_id)
                            )
                    if field == 'molecular_mass':
                        custom_columns['molecular_mass'] = (
                            select([
                                case([
                                    (_smr.c.is_restricted_structure,None)],
                                    else_=_smr.c.molecular_mass)])
                            .select_from(_smr)
                            .where(_smr.c.reagent_id==_reagent.c.reagent_id)
                            )
                    if field == 'molecular_formula':
                        custom_columns['molecular_formula'] = (
                            select([
                                case([
                                    (_smr.c.is_restricted_structure,
                                        SCHEMA.API_MSG_RESTRICTED_DATA)],
                                    else_=_smr.c.molecular_formula)])
                            .select_from(_smr)
                            .where(_smr.c.reagent_id==_reagent.c.reagent_id)
                            )
                    if field == 'molfile':
                        custom_columns['molfile'] = (
                            select([
                                case([
                                    (_smr.c.is_restricted_structure,None)],
                                    else_=_molfile.c.molfile)])
                            .select_from(_smr.join(
                                _molfile, _smr.c.reagent_id==_molfile.c.reagent_id))
                            .where(_smr.c.reagent_id==_reagent.c.reagent_id)
                            )
                    if field == 'structure_image':
                        custom_columns['structure_image'] = literal_column("'restricted'")

        columns = super(SmallMoleculeReagentResource, self).build_sqlalchemy_columns(
            fields, user=user, base_query_tables=base_query_tables, 
            custom_columns=custom_columns, show_preview=show_preview)  

        return columns


    @write_authorization
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'patch_obj')
        # NOTE: patching will only be done in batch, from library/well
    
    def _patch_wells(self, request, deserialized):
        ''' For bulk update: 
        - deserialized has been loaded with the wells
        '''
        logger.info('patch (%d) reagents for small_molecule ...', len(deserialized))
        
        schema = self.build_schema(request.user)
        fields = schema['fields']
        metadata = {}
        
        cumulative_error = CumulativeError()
        
        for i,well_data in enumerate(deserialized):
            well = well_data['well']
            is_patch = False
            if not well.reagents.exists():
                reagent = SmallMoleculeReagent(well=well)
                reagent.save()
            else:
                is_patch = True
                # TODO: only works for a single reagent
                # can search for the reagent using id_kwargs
                # reagent = well.reagents.all().filter(**id_kwargs)
                # TODO: update reagent
                reagent = well.reagents.all()[0]
                reagent = reagent.smallmoleculereagent
                logger.debug('found reagent: %r, %r', reagent.well_id, reagent)
            try:
                well_metadata = \
                    self._set_reagent_values(
                        reagent, well_data, is_patch, schema, fields)
                if well_metadata:
                    metadata[well_data[WELL.WELL_ID]] = well_metadata
                if (i+1) % 1000 == 0:
                    logger.info('patched %d reagents', i+1)
            except ValidationError, e:
                cumulative_error.add_error(well.well_id, e.errors, 
                          line=well_data.get(INPUT_FILE_DESERIALIZE_LINE_NUMBER_KEY, i))
            
        if cumulative_error.errors:
            raise cumulative_error

        logger.info('patched %d reagents', i+1)
        
        return metadata

    def parse(self, deserialized, create=False, fields=None, schema=None):
        
        _data = ReagentResource.parse(self, deserialized, create=create, 
            fields=fields, schema=schema)

        # Add in special parsing for the nested list integers        
        for key,val in _data.items():
            if val:
                if key in ['pubchem_cid', 'chembank_id','chembl_id']:
                    for v in val:
                        parse_val(v, key, 'integer')
        return _data


    def _set_reagent_values(self, reagent, deserialized, is_patch, schema, fields):

        start_time = time.time()
        initializer_dict = self.parse(deserialized, create=not is_patch, fields=fields)
        metadata = {}
        errors = self.validate(initializer_dict, patch=is_patch, fields=fields)
        if errors:
            raise ValidationError(errors)

        self.patch_elapsedtime1 += (time.time() - start_time)
        start_time = time.time()

        related_fields = [
            'compound_name', 'chembank_id', 'pubchem_cid', 'chembl_id',
            'molfile']
        for key, val in initializer_dict.items():
            if key not in related_fields and hasattr(reagent, key):
                setattr(reagent, key, val)
        logger.debug('patch small molecule reagent: %r', reagent)
        reagent.save()

        self.patch_elapsedtime2 += (time.time() - start_time)
        start_time = time.time()
        
        if 'compound_name' in initializer_dict:
            reagent.smallmoleculecompoundname_set.all().delete()
            # TODO: does this delete the old name entries?
            values = initializer_dict['compound_name'] or []
            for ordinal, val in enumerate(values):
                cn = SmallMoleculeCompoundName.objects.create(
                    reagent=reagent, compound_name=val, ordinal=ordinal)
                cn.save()
        if 'chembank_id' in initializer_dict:
            reagent.smallmoleculechembankid_set.all().delete()
            values = initializer_dict['chembank_id'] or []
            for id in values:
                cid = SmallMoleculeChembankId.objects.create(
                    reagent=reagent, chembank_id=id)
                cid.save()
        if 'pubchem_cid' in initializer_dict:
            reagent.smallmoleculepubchemcid_set.all().delete()
            values = initializer_dict['pubchem_cid'] or []
            for id in values:
                cid = SmallMoleculePubchemCid.objects.create(
                    reagent=reagent, pubchem_cid=id)
                cid.save()
        if 'chembl_id' in initializer_dict:
            reagent.smallmoleculechemblid_set.all().delete()
            values = initializer_dict['chembl_id'] or []
            for id in values:
                cid = SmallMoleculeChemblId.objects.create(
                    reagent=reagent, chembl_id=id)
                cid.save()
        new_molfile = deserialized.get('molfile', None)
        if new_molfile:
            # Compare md5sum for molfiles
            molquery = Molfile.objects.all().filter(reagent=reagent)
            if molquery.exists():
                old_molfile = molquery[0]
#                 m = hashlib.md5()
#                 m.update(old_molfile)
#                 oldkey = m.hexdigest()
#                 m = hashlib.md5()
#                 m.update(new_molfile)
#                 newkey = m.hexdigest()
#                 if oldkey != newkey:
#                 metadata[SMALL_MOLECULE_REAGENT.MOLFILE] = [
#                     oldkey, newkey]
                molquery.delete()
                
                molfile = Molfile.objects.create(
                    reagent=reagent, molfile=new_molfile)
                molfile.save()
            else:
                molfile = Molfile.objects.create(
                    reagent=reagent, molfile=new_molfile)
                molfile.save()
                
        self.patch_elapsedtime3 += (time.time() - start_time)
                
        logger.debug('sm patch_obj done')
        return metadata

class NaturalProductReagentResource(ReagentResource):
    # Consider folding the NaturalProductReagentResource into ReagentResource
    
    class Meta:
        
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'naturalproductreagent'
        authorization = LibraryResourceAuthorization(resource_name)
        
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        
    def __init__(self, **kwargs):
        super(NaturalProductReagentResource, self).__init__(**kwargs)

    def delete_reagents(self, library):
        
        NaturalProductReagent.objects.all().filter(well__library=library).delete()

    @write_authorization
    @transaction.atomic
    def patch_obj(self, request, deserialized, **kwargs):
        raise ApiNotImplemented(self._meta.resource_name, 'patch_obj')
        # NOTE: patching will only be done in batch, from library/well

    def _patch_wells(self, request, deserialized):
        ''' For bulk update: 
        - deserialized has been loaded with the well
        '''
        logger.info('patch (%d) reagents for natural_product ...', len(deserialized))
        metadata = {}
        schema = self.build_schema(request.user)
        fields = schema['fields']
        logger.info('natural product reagent fields: %r', fields.keys())
        cumulative_error = CumulativeError()
        for i,well_data in enumerate(deserialized):
            
            well = well_data['well']
            is_patch = False
            if not well.reagents.exists():
                reagent = NaturalProductReagent(well=well)
            else:
                is_patch = True
                # TODO: only works for a single reagent
                # can search for the reagent using id_kwargs
                # reagent = well.reagents.all().filter(**id_kwargs)
                # TODO: update reagent
                reagent = well.reagents.all()[0]
                reagent = reagent.naturalproductreagent
                logger.debug('found reagent: %r, %r', reagent.well_id, reagent)
            
            initializer_dict = self.parse(well_data, fields=fields)
            errors = self.validate(initializer_dict, patch=is_patch, fields=fields)
            if errors:
                cumulative_error.add_error(well.well_id, errors, line=line)
            
            for key, val in initializer_dict.items():
                if hasattr(reagent, key):
                    setattr(reagent, key, val)
    
            reagent.save()
            if (i+1) % 1000 == 0:
                logger.info('patched %d reagents', i+1)

        if cumulative_error.errors:
            raise cumulative_error
        
        logger.info('patched %d reagents', i+1)

        return metadata
    
class WellSerializer(LimsSerializer):
    
    def from_xls(self, content, root='objects', **kwargs):
        list_delimiters = kwargs.pop('list_delimiters', [LIST_DELIMITER_XLS, ','])
        logger.info('list delimiters: %r', list_delimiters)
        logger.info('kwargs: %r', kwargs)
        return super(WellSerializer, self).from_xls(content, root=root, 
                                        list_delimiters=list_delimiters, **kwargs)


class WellResource(DbApiResource):

    class Meta:

        queryset = Well.objects.all()
        authentication = MultiAuthentication(IccblBasicAuthentication(),
                                             IccblSessionAuthentication())
        resource_name = 'well'
        authorization = LibraryResourceAuthorization(resource_name)
        ordering = []
        filtering = {}
        serializer = WellSerializer()   
        always_return_data = True 
        max_limit = 10000

    def __init__(self, **kwargs):
        self.library_resource = None
        self.reagent_resource = None
        self.datacolumn_resource = None
        super(WellResource, self).__init__(**kwargs)

    def get_generic_reagent_resource(self):
        if not self.reagent_resource:
            self.reagent_resource = ReagentResource()
        return self.reagent_resource

    def get_library_resource(self):
        if not self.library_resource:
            self.library_resource = LibraryResource()
        return self.library_resource
    
    def get_data_column_resource(self):
        if self.datacolumn_resource is None:
            self.datacolumn_resource = DataColumnResource()
        return self.datacolumn_resource

    def prepend_urls(self):
        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)"
                r"/(?P<well_id>\d{1,5}\:[a-zA-Z]{1,2}\d{1,2})%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url(r"^(?P<resource_name>%s)"
                r"/(?P<well_id>\d{1,5}\:[a-zA-Z]{1,2}\d{1,2})"
                r"/annotations%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_well_annotations_view'), 
                    name="api_dispatch_well_annotations_view"),
            url((r"^(?P<resource_name>%s)"
                 r"/(?P<well_id>\d{1,5}\:[a-zA-Z]{1,2}\d{1,2})"
                 r"/duplex_wells%s$" )
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_well_duplex_view'), 
                    name="api_dispatch_well_duplex_view"),
        ]
        
    

    @read_authorization
    def dispatch_well_annotations_view(self, request, **kwargs):
        '''
        All Study annotations for a well
        Format is:
        [ { study 1 information, dc_1, dc_2, etc. }, { study 2 information ... }...]
        - where each study conforms to the study schema
        - each dc has:
            { dc_schema, value }
        '''
        well_id = kwargs.get('well_id', None)
        if not well_id:
            raise MissingParam('well_id')
        
        final_data = self.get_generic_reagent_resource().get_annotations(well_id)
        
        content_type = self.get_serializer().get_accept_content_type(
            request,format=kwargs.get('format', None))
        return HttpResponse(
            content=self.get_serializer().serialize(
                final_data, content_type),
            content_type=content_type)

    @read_authorization
    def dispatch_well_duplex_view(self, request, **kwargs):
        '''
        Generate a duplex well report for the given pool well
        '''
        
        well_id = kwargs.get('well_id', None)
        if not well_id:
            raise MissingParam('well_id')
        
        data = self.get_generic_reagent_resource().get_duplex_data(request, well_id)
                
        content_type = self.get_serializer().get_accept_content_type(
            request,format=kwargs.get('format', None))
        return HttpResponse(
            content=self.get_serializer().serialize(
                data, content_type),
            content_type=content_type)


    def get_schema(self, request, **kwargs):
        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        library_short_name = param_hash.get('library_short_name')
        if library_short_name is None:
            return self.build_response(request, 
                self.build_schema(request.user,**kwargs),
                **param_hash)
        
        try:
            library = Library.objects.get(short_name=library_short_name)
            return self.build_response(
                request, 
                self.build_schema(request.user,
                    library_classification=library.classification),
                **param_hash)
            
        except Library.DoesNotExist, e:
            raise Http404(
                'Can not build schema - library def needed'
                'no library found for short_name: %r' % library_short_name)
                
    def build_schema(self, user, library_classification=None, **kwargs):
        
        logger.info('build schema for library type: %r', library_classification)
        
        data = super(WellResource, self).build_schema(user=user, **kwargs)
        if library_classification:
            sub_data = self.get_reagent_resource(library_classification)\
                .build_schema(user, **kwargs)
            
            logger.info('sub_resource field visibilities: %r', 
                [(key,'%r'%fi['visibility']) for key, fi in sub_data['fields'].items()])
            newfields = {}
            newfields.update(sub_data['fields'])
            newfields.update(data['fields'])
            data['fields'] = newfields
            
            data['content_types'] = sub_data['content_types']
        else:
            pass
        logger.debug('final field visibilities: %r', 
            [(key,'%r'%fi['visibility']) for key, fi in data['fields'].items()])
        return data
    
    
    
    @read_authorization
    def get_detail(self, request, **kwargs):

        well_id = kwargs.get('well_id', None)
        if not well_id:
            raise MissingParam('well_id')

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.get_list(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):
        return self.get_generic_reagent_resource().get_list(request, **kwargs)

    @write_authorization
    @un_cache        
    @transaction.atomic
    def put_list(self, request, **kwargs):
        '''
        Put list will replace all the library reagents
        '''
        if 'library_short_name' not in kwargs:
            raise BadRequestError(key='library_short_name', msg='required')
        library = Library.objects.get(
            short_name=kwargs['library_short_name'])
        logger.info('put wells for library: %r, deleting reagents...', library)
        self.get_reagent_resource(library.classification)\
            .delete_reagents(library)

        logger.info('resetting well data...')
        well_query = Well.objects.all().filter(library=library)
        well_query.update(library_well_type=WELL_TYPE.UNDEFINED)
        well_query.update(facility_id=None)
        well_query.update(is_deprecated=False)
        well_query.update(deprecation_reason=False)
        well_query.update(molar_concentration=None)
        well_query.update(mg_ml_concentration=None)
        
        logger.info('library %s reagents removed, patching...', library)
        return self.patch_list(request, **kwargs)
         
    def post_list(self, request, **kwargs):
        return self.patch_list(request, **kwargs)

    @write_authorization
    def post_preview(self,request, **kwargs):
        '''
        If preview logs exist, apply them to the target wells.
        
        @see wrap_as_preview
        '''
        
        
        library_short_name = kwargs.get('library_short_name', None)
        
        logger.info('post_preview for library: %s', library_short_name)
        
        library = Library.objects.get(short_name=library_short_name)
        if library.is_released is not True:
            raise Exception('Library has not been released')
        library_data = self.get_library_resource()\
            ._get_detail_response_internal(**{
                'short_name': library.short_name })
        if not library_data.get(SCHEMA.LIBRARY.PREVIEW_LOG_ID):
            raise Exception('no preview found for %s', library_short_name)

        # Find the parent library log
        
        library_logs = self.get_apilog_resource()._get_list_response_internal(
            **{
                'is_preview': True,
                'ref_resource_name': SCHEMA.LIBRARY.resource_name,
                'key': library.short_name,
                'includes': '*'
            })
        if len(library_logs) != 1:
            logger.info('more than one preview library logs found: %r', library_logs)
            raise ValidationError(key='library_short_name', 
                msg='more than one preview found for: "%s"' % library.short_name)
        library_log = library_logs[0]
        logger.info('found the preview log: %r', library_log)
        
        # Get the child logs, and recreate the deserialized data
        # Note: all well/reagent logs are logged for the "well" resource
        well_logs = self.get_apilog_resource()._get_list_response_internal(
            **{ 
                SCHEMA.APILOG.PARENT_LOG_ID: library_log[SCHEMA.APILOG.ID],
                'includes': '*'
        })
        
        if not well_logs:
            raise ValidationError(
                key='library',
                msg='No preview found for "%s"' % library.short_name)
        
        deserialized = []

        errors = {}
        for log in well_logs:
            key = log[SCHEMA.APILOG.KEY]
            diffs = log.get(SCHEMA.APILOG.DIFFS, None)
            well_diff_errors = []
            if diffs:
                _repost_dict = { WELL.WELL_ID: key }
                for key, diff in diffs.items():
                    if len(diff) == 2:
                        _repost_dict[key] = diff[1]
                    else:
                        well_diff_errors.append('Diff corrupted: %r', diff)
                if well_diff_errors:
                    errors[key] = well_diff_errors
                else:
                    deserialized.append(_repost_dict)
            logger.debug('created post data: %r', _repost_dict)
        if errors:
            raise ValidationError(errors)
    
        kwargs[API_PARAM_PATCH_PREVIEW_MODE] = False
        kwargs[API_PARAM_NO_BACKGROUND] = True
        kwargs['servicing_preview'] = True
        kwargs['data'] = deserialized

        if HEADER_APILOG_COMMENT not in request.META:
            kwargs[HEADER_APILOG_COMMENT] = library_log[SCHEMA.APILOG.COMMENT]
        
        log_json_meta = library_log.get('json_field', None)
        logger.info('json_field: %r', log_json_meta)
        if log_json_meta:
            logger.info('adding json_field: %r', log_json_meta)
            kwargs['meta'] = log_json_meta
            # TODO: store the filename on the log
            # TODO: full control over the log
            
        result = self.patch_list(request, **kwargs)

        logger.info('Remove the Apilogs with the preview flag...')
        ApiLog.objects.get(id=library_log[SCHEMA.APILOG.ID]).delete()
        # Django ORM will delete the child logs with the foreign key 
        # default "ON DELETE CASCADE"
        
        return result
    
    
    def wrap_as_preview(_func):
        '''
        Wrapper function: 
        
        If the API_PARAM_PATCH_PREVIEW_MODE parameter is passed, and the 
        preview_logs are passed back, these are saved to to enable a 
        "preview" view of the commit.

        NOTE: For a "released" library, all Library Reagent uploads will be 
        committed to a "preview". 
        In a "preview" upload:
        - the server will save ApiLog "preview" logs only;
        - (all normal updates are rolled back)
        - patch logs can later be confirmed and applied by the admin.

        
        A preview contains all the upload diff data on 
        the server and may be viewed, applied ("released"), or deleted by the 
        Administrator users.
        
        @see post_preview
        '''
     
        @wraps(_func)
        def _inner(self, *args, **kwargs):
            request = args[0]
     
            param_hash = self._convert_request_to_dict(request)
            param_hash.update(kwargs)
            
            is_preview_mode = parse_val(
                param_hash.get(API_PARAM_PATCH_PREVIEW_MODE, False),
                API_PARAM_PATCH_PREVIEW_MODE, 'boolean')
            logger.info('API_PARAM_PATCH_PREVIEW_MODE: %r', is_preview_mode)
            
            preview_logs = []
            if is_preview_mode is True:
                kwargs[API_PARAM_PREVIEW_LOGS] = preview_logs    
    
            result = _func(self, *args, **kwargs)
             
            if is_preview_mode is True and preview_logs:
                
                # TODO: wrap in a transaction
                logger.info('preview logs to save: %d', len(preview_logs))
                for log in preview_logs:
                    log.is_preview = True
                    log.save()
            
            return result
         
        return _inner

    @write_authorization
    @background_job
    @wrap_as_preview
    @un_cache        
    @transaction.atomic
    def patch_list(self, request, **kwargs):
        ''' Patch a set of well/reagent definitions for the library '''
        
        
        # Workflow for well/reagent patching: 
        # SS1:
        # if a reagent changes, then create a new reagent entry and update the 
        # "latest_released_reagent"
        # SS2:
        # version 1: Ok to update reagent in place and use well id to find it, 
        # because only one reagent is associated with a well
        # FUTURE: use update of reagent by well id as special case, only allow if 
        # only one well is attached to the reagent
         
        if 'library_short_name' not in kwargs:
            raise BadRequestError(key='library_short_name', msg='required')
        library = Library.objects.get(
            short_name=kwargs['library_short_name'])
        
        library_data = self.get_library_resource()\
            ._get_detail_response_internal(**{
                'short_name': library.short_name
                })
        
        if library_data.get('preview_log_id') \
            and kwargs.get('servicing_preview',False) is not True:
            raise ValidationError(
                key='library', 
                msg='%s has a pending preview release that must be committed or deleted'
                ' before further changes may be made' % library.short_name)

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        
        is_preview_mode = parse_val(
            param_hash.get(API_PARAM_PATCH_PREVIEW_MODE, False),
            API_PARAM_PATCH_PREVIEW_MODE, 'boolean')
        
        logger.info(
            'patch_list: WellResource: library: %r, %r: %r...', 
            library.short_name, API_PARAM_PATCH_PREVIEW_MODE, is_preview_mode)

        reagent_resource = self.get_reagent_resource(library.classification)  
        schema = reagent_resource.build_schema(request.user)
        reagent_specific_fields = { 
            k:v for k,v in schema['fields'].items()
                if v['scope'] != 'fields.well' }
        logger.debug('reagent_specific_fields: %r', reagent_specific_fields.keys())

        # allow for internal data to be passed
        deserialized = kwargs.pop('data', None)
        deserialize_meta = None
        if not deserialized:
            deserialized, deserialize_meta = self.deserialize(request, schema=schema)
        if self._meta.collection_name in deserialized:
            deserialized = deserialized[self._meta.collection_name]

        # Track cumulative parsing and validation errors        
        cumulative_error = CumulativeError()
        
        # 1. Validate all patch entries and collect well_ids
        valid_data, errors = self._find_valid_entries(deserialized)
        if errors:
            cumulative_error._update_from(errors)
        if not valid_data:
            if cumulative_error.errors:
                raise cumulative_error
            else:
                raise ValidationError(key='objects', msg='no valid well IDs')
        
        # 2. Fetch original data for logging state

        # build a plate-well map for search
        plate_well_map = defaultdict(set)
        for well_id in valid_data.keys():
            plate_number = lims_utils.well_id_plate_number(well_id)
            well_name = lims_utils.well_id_name(well_id)
            plate_well_map[plate_number].add(well_name)
        
        search_data = []
        full_plates = set()
        for plate,wells in plate_well_map.items():
            if len(wells) == int(library.plate_size):
                full_plates.add(int(plate))
                del plate_well_map[plate]
            else:
                search_data.append({
                    'plates': [plate,],
                    'well_names': list(wells)
                })
        if full_plates:
            search_data.append({ 'plates': list(full_plates) })

        logger.debug('search data: %r', search_data)
        kwargs_for_log = kwargs.copy()
        kwargs_for_log[API_PARAM_SHOW_RESTRICTED] = True
        kwargs_for_log[SCHEMA.API_PARAM_NESTED_SEARCH] = search_data
        kwargs_for_log['includes'] = ['*', 'molfile','-structure_image']
        
        # UNDEFINED wells will be considered a "CREATE" instance
        # ( NOTE: if in preview mode and library is released, 
        # will generate full create logs )
        kwargs_for_log['library_well_type__ne'] = WELL_TYPE.UNDEFINED

        original_data = reagent_resource\
            ._get_list_response_internal(**kwargs_for_log)
        original_data = { data['well_id']:data for data in original_data }
        
        logger.info('fetched wells: %d for search data', len(original_data))
    
        library_log = self.make_log(request, **kwargs)
        library_log.ref_resource_name = 'library'
        library_log.key = library.short_name
        library_log.uri = '/'.join([
            library_log.ref_resource_name, library_log.key])
 
        logger.info('Cache library wells for patch...') 
        well_map = dict((well.well_id, well) 
            for well in library.well_set.all())
        if len(well_map) == 0:
            # Note: wells can only be created on library creation
            raise BadRequestError(
                key='library_short_name', msg='Library wells have not been created')
        
        # 3. Patch well specific data
         
        logger.info('patch wells, count: %d', len(valid_data))
        fields = { key:field for key,field in schema['fields'].items()
            if field['scope'] == 'fields.%s'% 'well'
                and 'u' in field['editability'] }
        reagent_data = []
        for i,(well_id,well_data) in enumerate(valid_data.items()):
            line = well_data.get(INPUT_FILE_DESERIALIZE_LINE_NUMBER_KEY, i)
            
            well_data['library_short_name'] = kwargs['library_short_name']
            well = well_map.get(well_id, None)
            if not well:
                cumulative_error.add_error(
                    well_id, {'well_id': 'Does not belong to this library'}, 
                    line=line)
                continue
            try:
                initializer_dict = self.parse(well_data, fields=fields)
            except ValidationError, e:
                cumulative_error.add_error(well_id, e.errors, line=line)
                continue
            
            errors = self.validate(initializer_dict, patch=True, fields=fields)
            if errors:
                cumulative_error.add_error(well_id, errors, line=line)
                continue

            # Check for state changes:
            new_well_type = initializer_dict.pop(WELL.LIBRARY_WELL_TYPE, None)
            if well.library_well_type == WELL_TYPE.UNDEFINED:
                
                if not new_well_type:
                    cumulative_error.add_error(well_id, {
                        WELL.LIBRARY_WELL_TYPE: 'required' }, 
                        line=line)
                    continue
                        
                well.library_well_type = new_well_type
            elif new_well_type and new_well_type != well.library_well_type:
                
#                 # FIXME: 20180713 - should allow reset to undefined
#                 if new_well_type == WELL_TYPE.UNDEFINED:
#                     add_error(well_id, {
#                         WELL.LIBRARY_WELL_TYPE : 
#                             '"%s" is not allowed' % WELL_TYPE.UNDEFINED }, 
#                         raw_data=well_data)
#                     continue
                
                # Delete the well to remove all old information
                well.delete()
                
                well = Well.objects.create(
                    well_id = well_id,
                    well_name = lims_utils.well_id_name(well_id),
                    plate_number = lims_utils.well_id_plate_number(well_id),
                    library=library, 
                    library_well_type = new_well_type
                )
                    
            well_data['well'] = well
            for key, val in initializer_dict.items():
                if hasattr(well, key):
                    setattr(well, key, val)
            
            well.save()
            
            duplex_wells = []
            if well_data.get('duplex_wells', None):
                if not library.is_pool:
                    cumulative_error.add_error(well_id, {
                        'duplex_wells':'library is not a pool libary: %r' 
                            % library.short_name}, line=line)
                    continue
                
                # TODO: batch get/set all of the duplex wells as well
                for duplex_well_id in well_data['duplex_wells']:
                    try:
                        duplex_wells.append(Well.objects.get(well_id=duplex_well_id))
                    except:
                        cumulative_error.add_error(well_id, {
                            'duplex_wells': 'well: %r not found' % duplex_well_id },
                            line=line)
                        
                well_data['duplex_wells'] = duplex_wells
                
            # Pass on the reagent patch data only for experimental wells
            REAGENT_WELL_TYPES = [
                WELL_TYPE.EXPERIMENTAL, WELL_TYPE.LIBRARY_CONTROL]
            if well.library_well_type in REAGENT_WELL_TYPES:
                reagent_data.append(well_data)
                
            else:
                # If the well is not expermimental (or control), then check
                # to make sure user is not trying to set reagent values
                reagent_test_data = reagent_resource.parse(
                    well_data, fields=reagent_specific_fields)
                if DEBUG_LIB_LOAD is True:
                    logger.info('reagent specific fields: %r', reagent_specific_fields.keys())
                    logger.info('reagent_test_data: %r', reagent_test_data)
                value_fields = [k for k,v in reagent_test_data.items() 
                                if v is False or v]
                if DEBUG_LIB_LOAD is True:
                    logger.info('value fields: %r', value_fields)
                if value_fields:
                    cumulative_error.add_error(well_id, {
                        WELL.LIBRARY_WELL_TYPE: 
                            ['Reagent fields may only be specified for a '
                            'library_well_type in: (%s) ' 
                                % ', '.join(REAGENT_WELL_TYPES),
                            'reagent fields specified: [%s]' 
                                %', '.join(value_fields)]
                        }, line=line)
                    
            if (i+1) % 1000 == 0:
                logger.info('patched %d wells', i+1)
                
        logger.info('patched %d wells', i+1)

        # 4. Patch reagent specific data
        
        reagent_patch_metadata = None
        if reagent_data:
            try:
                logger.info('patch (%d) reagents for: %r',
                            len(reagent_data), library.classification)
                reagent_patch_metadata = \
                    reagent_resource._patch_wells(request, reagent_data)
                    
                # FIXME: 20180710 - reagent_patch_metadata created but not 
                # needed to track molfile changes
            except CumulativeError, e:
                cumulative_error.update_from(e)
        else:
            logger.info('no reagent data to patch')
            
        library.save()
        logger.info(
            'put_list: WellResource: library: %r; patch completed: %d', 
            library.short_name, i+1)
        reagent_resource.get_debug_times()
        
        # 5. Update statistics 
        
        experimental_well_count = library.well_set.filter(
            library_well_type__iexact=WELL_TYPE.EXPERIMENTAL).count()
        if library.experimental_well_count != experimental_well_count:
            library_log.diffs['experimental_well_count'] = \
                [library.experimental_well_count, experimental_well_count]
            library.experimental_well_count = experimental_well_count
            library.save()
 
        library_log.save()
        
        # 6. Fetch new data, for logging
        
        logger.info('fetch new data, for logging...')
        logger.debug('get new reagent state, for logging: %r',kwargs_for_log)
        new_data = reagent_resource\
            ._get_list_response_internal(**kwargs_for_log)
        new_data = { data['well_id']:data for data in new_data }    

        # Use the API representation of each well to do final validations
        for well_id, well_data in new_data.items():
            # NOTE: if a well already has an error, it may fail to generate later 
            # validations errors due to not being loaded
            if well_id not in cumulative_error.errors:
                errors = self.final_validation(reagent_resource,well_data)
                if errors:
                    cumulative_error.add_error(well_id, errors)
        if cumulative_error.errors:
            sorted_wells = sorted(cumulative_error.errors.keys())
            sorted_errors = list()
            for well_id in sorted_wells:
                sorted_errors.append((well_id, cumulative_error.errors[well_id]))
            cumulative_error.errors = sorted_errors
            
            if deserialize_meta:
                for k,v in deserialize_meta.items():
                    cumulative_error.errors.append((k,v))
            if DEBUG_LIB_LOAD is True:
                logger.info('cumulative_errors...: %r', cumulative_error)
            raise cumulative_error

        # TODO: 20180710 efficient Test for molfile changes
        
        full_create_log = False
        if library.is_released is True:
            full_create_log = is_preview_mode
        
        logger.info('full_create_log: %r', full_create_log)
        logs = self.log_patches(
            request, original_data.values(), new_data.values(),
            excludes=['substance_id'], parent_log=library_log, 
            full_create_log=full_create_log)

        if not logs:
            if deserialize_meta:
                errors = deserialize_meta
            else:
                errors = {}
            errors.update({ 'state': 'patch file contains no updates'})
            raise ValidationError(errors)

        patch_count = len(valid_data)
        # Update: for wells, only measure what has diffed
        update_count = len([x for x in logs if x.diffs ])
        # Create: measure what is reported created (new_data), subtract updates,
        # because create actions are not included in updates for well patching.
        create_count = 0
        if not original_data:
            create_count = len(new_data) - update_count

        # 7. Update screening stats if wells have changed
                
        if update_count:
            self.update_screening_stats(library)
        
        if update_count > 0 or create_count > 0:
            prev_version = library.version_number
            if library.version_number:
                library.version_number += 1
            else:
                library.version_number = 1
            library.save()
            library_log.diffs['version_number'] = [prev_version, library.version_number]
            library_log.save()
        
        # Final result reporting
        meta = kwargs.get('meta', {})

        if deserialize_meta:
            meta.update(deserialize_meta)
        
        meta.update({ 
            SCHEMA.API_MSG_RESULT: { 
                SCHEMA.API_MSG_SUBMIT_COUNT: patch_count, 
                SCHEMA.API_MSG_UPDATED: update_count, 
                SCHEMA.API_MSG_CREATED: create_count, 
                SCHEMA.API_MSG_UNCHANGED: patch_count-update_count-create_count,
                SCHEMA.API_MSG_ACTION: library_log.api_action, 
                SCHEMA.API_MSG_COMMENTS: library_log.comment
            }
        })
        logger.info('wells patch complete: %r', meta)
        
        library_log.json_field = meta
        library_log.save()
        
        if is_preview_mode is True and library.is_released is True:
            # After release, each load will be a "preview":
            # -- only the logs will be committed until the preview is released.
            if API_PARAM_PREVIEW_LOGS in kwargs:
                kwargs[API_PARAM_PREVIEW_LOGS].append(library_log)
                kwargs[API_PARAM_PREVIEW_LOGS].extend(logs)
            
                logger.info('transaction rollback for preview mode...')
                transaction.set_rollback(True)
            else:
                raise ProgrammingError(
                    'patching in preview mode requires %r param' 
                        % API_PARAM_PREVIEW_LOGS )
        if not self._meta.always_return_data:
            return self.build_response(
                request, { API_RESULT_META: meta }, 
                response_class=HttpResponse, **kwargs)
        else:
            return self.build_response(
                request,  { API_RESULT_META: meta }, 
                response_class=HttpResponse, **kwargs)

    def _find_valid_entries(self, deserialized):  

        errors = defaultdict(dict)
        
        id_attribute = WELL.WELL_ID
        valid_data = {}
        
        DEBUG_LINE_KEY = 'line: {}'
        
        for row,data in enumerate(deserialized):
            line = data.get(INPUT_FILE_DESERIALIZE_LINE_NUMBER_KEY, row)
            debug_key = DEBUG_LINE_KEY.format(line)
            id = data.get(id_attribute,None)
            if not id:
                well_name = data.get('well_name', None)
                plate_number = data.get('plate_number', None)
                if well_name and plate_number:                
                    well_id = lims_utils.well_id(plate_number, well_name)
                    if not well_id:
                        errors[debug_key].update({
                            WELL.WELL_ID: 'parse error: {}:{}'.format(
                                plate_number, well_name)})
                        continue
                    else:
                        if well_id in valid_data:
                            errors[well_id].update({ debug_key: 'duplicate' })
                        else:
                            data[WELL.WELL_ID] = well_id
                            valid_data[well_id] = data
                else:
                    errors[debug_key].update({ WELL.WELL_ID: 'required'})
                    continue
            else:
                well_id = lims_utils.parse_well_id(id)
                if not well_id:
                    errors[debug_key].update({ WELL.WELL_ID: 
                        'Well ID: "{}" does not fit the pattern {}'\
                            .format(id, WELL.WELL_ID_PATTERN_MSG) })
                    continue
                else:
                    if well_id in valid_data:
                        errors[well_id].update({ debug_key: 'duplicate' })
                    else:
                        data[WELL.WELL_ID] = well_id
                        valid_data[well_id] = data
    
        logger.info('valid data to patch: %d', len(valid_data))
        
        return (valid_data, errors)

    def final_validation(self, reagent_resource, well_data):
        ''' Perform final validations on the data generated after loading'''
        
        errors = {}
        
        if well_data[WELL.LIBRARY_WELL_TYPE] == WELL_TYPE.EXPERIMENTAL:
            
            mgml = well_data.get(WELL.MG_ML_CONCENTRATION)
            molar = well_data.get(WELL.MOLAR_CONCENTRATION)
            msg = 'required for %s = %r' % (
                WELL.LIBRARY_WELL_TYPE, WELL_TYPE.EXPERIMENTAL)
            if mgml is None and molar is None:
                errors[WELL.MG_ML_CONCENTRATION] = msg
                errors[WELL.MOLAR_CONCENTRATION] = msg
            
        errors.update(reagent_resource.final_validation(well_data))
        
        return errors

    def update_screening_stats(self, library):

        with get_engine().connect() as conn:
            sql = (
                'update library_screening '
                'set screened_experimental_well_count = current_count '
                'from '
                ' ( '
                '    select '
                '    activity_id, '
                '    screened_experimental_well_count, '
                '    count(distinct(well_id)) current_count '
                '    from assay_plate ap '
                '    join library_screening ls on(activity_id=library_screening_id) '
                '    join ( '
                '        select distinct(library_screening_id)  '
                '        from assay_plate ap, library  '
                '        where ap.plate_number between start_plate and end_plate   '
                '        and library_id = %s ) as screenings '
                '            on(ls.activity_id=screenings.library_screening_id) '
                '    join well on(well.plate_number=ap.plate_number) '

                "    where well.library_well_type='experimental' "
                '    group by ls.activity_id, screened_experimental_well_count  '
                ') as count_comparison  '
                'where count_comparison.activity_id=library_screening.activity_id '
                'and library_screening.screened_experimental_well_count != current_count; '
                )
            logger.debug('execute sql: %r (%d)', sql, library.library_id)
            conn.execute(sql, (library.library_id))
            sql = (
                'update screen '
                'set screened_experimental_well_count = current_count, '
                '  unique_screened_experimental_well_count = current_unique_count '
                'from '
                ' ( '
                '    select '
                '    screen.screen_id, '
                '    screen.facility_id, '
                '    screen.screened_experimental_well_count, '
                '    screen.unique_screened_experimental_well_count, '
                '    count(well_id) as current_count, '
                '    count(distinct(well_id)) as current_unique_count '
                '    from well w,  '
                '    screen join assay_plate ap using(screen_id) '
                '    join plate p using(plate_id)  '
                '    join ( '
                '            select distinct(screen_id)  '
                '            from library_screening ls  '
                '            join assay_plate on(activity_id=library_screening_id) '
                '            join plate using(plate_id) '
                '            join copy using(copy_id) '
                '            where copy.library_id = %s '
                '    ) as screens on(screens.screen_id=screen.screen_id) '
                "    where w.library_well_type = 'experimental' "
                '    and ap.replicate_ordinal = 0 '
                '    and w.plate_number = p.plate_number '
                '    group by '
                '    screen.screen_id, '
                '    screen.facility_id, '
                '    screen.screened_experimental_well_count, '
                '    screen.unique_screened_experimental_well_count ) as screen_update '
                'where screen_update.screen_id=screen.screen_id '
                'and ( screen_update.screened_experimental_well_count!= current_count or  '
                '      screen_update.unique_screened_experimental_well_count != current_unique_count ); '
                )
            logger.debug('execute sql: %r (%d)', sql, library.library_id)
            conn.execute(sql, (library.library_id))


    def patch_obj(self, request, deserialized, **kwargs):
        
        raise ApiNotImplemented(self._meta.resource_name, 'patch_obj')
        
    @classmethod
    def create_vendor_compound_name_base_query(cls, well_search_data):
        
        IS_SMALL_MOLECULE_ONLY = False
        
        # Process the patterns by line
        parsed_lines = well_search_data
        if isinstance(parsed_lines, basestring):
            parsed_lines = re.split(
                lims_utils.PLATE_SEARCH_LINE_SPLITTING_PATTERN,parsed_lines)
            logger.info('found %d lines in search', len(parsed_lines))
            logger.debug('parsed_lines: %r', parsed_lines)
        if not isinstance(parsed_lines, (list,tuple)):
            well_search_data = (parsed_lines,)
        
        search_items = set()
        for _line in parsed_lines:
            if not _line:
                continue
            _line = _line.strip()
            if not _line:
                continue
            # 20180227 - require each entry on a separate line:
            # Otherwise, names containing commmas and whitespace must be quoted
            search_items.add(_line)
        
        if not search_items:
            raise ValidationError(
                key=SCHEMA.API_PARAM_SEARCH, msg='no search lines found')

        logger.info('found: %d compound or vendor names in %r', 
            len(search_items), parsed_lines)

        bridge = get_tables()
        _cn = bridge['small_molecule_compound_name']
        _smr = bridge['small_molecule_reagent']
        _r = bridge['reagent']
        _well = bridge['well']
        
        clause_cn = []
        clause_vn = []
        for term in search_items:
            clause_cn.append(_cn.c.compound_name.ilike('%{}%'.format(term)))
            clause_vn.append(_r.c.vendor_identifier==term)
        if len(clause_cn) > 1:
            clause_cn = or_(*clause_cn)
            clause_vn = or_(*clause_vn)
        else:
            clause_cn = clause_cn[0]
            clause_vn = clause_vn[0]
        querycn = (
            select([_well.c.well_id])
            .select_from(
                _well.join(_r,_well.c.well_id==_r.c.well_id)
                    .join(_cn,_r.c.reagent_id==_cn.c.reagent_id))
            .where(clause_cn))
        vjoin = _well.join(_r,_well.c.well_id==_r.c.well_id)
        if IS_SMALL_MOLECULE_ONLY is True:
            vjoin = vjoin.join(_smr, _r.c.reagent_id==_smr.c.reagent_id)
        query_vendor = (
            select([_well.c.well_id])
            .select_from(vjoin)
            .where(clause_vn))
        query = querycn.union(query_vendor)
        return query
        
    @classmethod
    def parse_well_search(cls, well_search_data):
        '''
        Parse a Well search by line into an array of search lines of the form:
        input: 
        - lines separated by a newline char 
        - space or comma separated values, 
        output:
        search_line: {
            plates: [],
            plate_ranges: [],
            wellnames: [],
            well_ids: []
        }
        '''
        # Use unquote to decode form data from a post
        if not isinstance(well_search_data, (list,tuple)):
            well_search_data = urllib.unquote(well_search_data)
        else:
            cleaned_searches = []
            for ps in well_search_data:
                if isinstance(ps, six.string_types):
                    cleaned_searches.append(urllib.unquote(ps))
                else:
                    cleaned_searches.append(ps)
            well_search_data = cleaned_searches    
        
        if DEBUG_WELL_PARSE:
            logger.info('well_search_data: %r', well_search_data)
        
        parsed_searches = []
        errors = []
        
        # Process the patterns by line; or semicolon (to support URL encoded)
        parsed_lines = well_search_data
        if isinstance(parsed_lines, basestring):
            parsed_lines = re.split(
                lims_utils.PLATE_SEARCH_LINE_SPLITTING_PATTERN,parsed_lines)
            if DEBUG_WELL_PARSE:
                logger.info('parsed_lines: %r', parsed_lines)
        
        for _line in parsed_lines:
            _line = _line.strip()
            if not _line:
                continue
            
            parts = re.compile('[\s,]+').split(_line)
            if DEBUG_WELL_PARSE:
                logger.info('parse well search: line parts: %r', parts)
            
            parsed_search = defaultdict(list)
            parsed_search['line'] = parts
            for part in parts:
                if DEBUG_WELL_PARSE:
                    logger.info('test part: %r', part)
                if PLATE_PATTERN.match(part):
                    plate_number = int(part)
                    if DEBUG_WELL_PARSE:
                        logger.info('from PLATE: %r to %d', part, plate_number)
                    parsed_search['plates'].append(plate_number)
                elif PLATE_RANGE_PATTERN.match(part):
                    match = PLATE_RANGE_PATTERN.match(part)
                    plate_range = sorted([
                        int(match.group(1)), int(match.group(2))])
                    if DEBUG_WELL_PARSE:
                        logger.info('from PLATE_RANGE: %r, %r', part, plate_range)
                    parsed_search['plate_ranges'].append(plate_range)
                elif WELL_ID_PATTERN.match(part):
                    parsed_search['well_ids'].append(lims_utils.parse_well_id(part))
                elif WELL_NAME_PATTERN.match(part):
                    match = WELL_NAME_PATTERN.match(part)
                    wellrow = match.group(1).upper()
                    wellcol = match.group(2)
                    wellname = '%s%s' % (wellrow, str(wellcol).zfill(2))
                    if DEBUG_WELL_PARSE:
                        logger.info('from WELL_NAME: %r to %s', part, wellname)
                    parsed_search['wellnames'].append(wellname)
                else:
                    errors.append('part not recognized: %r' % part)
                    
            if not errors:
                if 'plates' not in parsed_search \
                    and 'plate_ranges' not in parsed_search \
                    and 'well_ids' not in parsed_search:
                    errors.append(
                        'Must specify either a plate, plate range, or well_id: %r' % _line)
                if 'well_ids' in parsed_search \
                    and ( 'plates' in parsed_search 
                        or 'plate_ranges' in parsed_search):
                    errors.append(
                        'Well ids may not be defined on the same line with plate or '
                        'plate ranges: %r' % _line)
                # match wellnames only after plate, plate range is identified
                if 'wellnames' in parsed_search \
                    and 'plates' not in parsed_search \
                    and 'plate_ranges' not in parsed_search:
                        if 'well_ids' in parsed_search:
                            well_ids = parsed_search['well_ids']
                            if len(well_ids) > 1:
                                errors.append(
                                    'Well names may not be defined with multiple '
                                    'well_ids: %r' % _line)
                            else:
                                match = WELL_ID_PATTERN.match(well_ids[0])
                                plate = int(match.group(1))
                                wellrow = match.group(3).upper()
                                wellcol = match.group(4)
                                wellname = '%s%s' % (wellrow, str(wellcol).zfill(2)) 
                                parsed_search['wellnames'].append(wellname)
                                parsed_search['plates'].append(plate)
                                del parsed_search['well_ids']
                        else:
                            errors.append(
                                'Must specify either a plate, plate_range, or well_id'
                                'for wellnames: %r' % _line)
                if DEBUG_WELL_PARSE:
                    logger.info('parsed: %r from %r', parsed_search, _line)
                parsed_searches.append(parsed_search)
        
        if errors:
            raise ValidationError(key=SCHEMA.API_PARAM_SEARCH, msg=', '.join(errors))
        if not parsed_searches:
            raise ValidationError(key=SCHEMA.API_PARAM_SEARCH, msg='no search lines found')

        # compress if same wellnames for different plates
        plate_wells = defaultdict(set)
        compressed_searches = []
        for parsed_search in parsed_searches:
            if 'plates' in parsed_search \
                and 'wellnames' in parsed_search \
                and 'plate_ranges' not in parsed_search:
                for plate in parsed_search['plates']:
                    wellnames = plate_wells[plate]
                    wellnames.update(parsed_search.get('wellnames',[]))
                    plate_wells[plate] = wellnames
            else:
                compressed_searches.append(parsed_search)
        
        if plate_wells:
            logger.info('found plate_wells: %r', len(plate_wells))
            # create a reversed dict by wellnames
            reversed = defaultdict(set)
            for plate, wellnames in plate_wells.items():
                reversed[','.join(sorted(wellnames))].add(plate)
            logger.info('reversed: %r', reversed)
            
            for wellnames, plates in reversed.items():
                compressed_searches.append({
                    'plates': sorted(plates), 'wellnames': wellnames.split(',') })

        decorated = [(
            ','.join(s.get('well_ids',[])),
            ','.join(map(str,sorted(s.get('plates',[])))),
            ','.join([','.join(map(str,pr)) for pr in s.get('plate_ranges',[])]),
            s) 
            for s in compressed_searches]
        decorated.sort(key=itemgetter(0,1,2))
        compressed_searches = \
            [s for sort_well_ids,sort_plates,sort_ranges,s in decorated]
        return compressed_searches
        
    @classmethod
    def create_well_base_query(cls, parsed_searches):
        
        bridge = get_tables()
        _well = bridge['well']

        well_query = select([_well.c.well_id]).select_from(_well)        
        clauses = []
        plates_only = set()
        wellids = set()
        for parsed_search in parsed_searches:
            clause = []
            
            # Separate out plates only searches
            if len(parsed_search)==1 and 'plates' in parsed_search:
                plates_only.update(parsed_search['plates'])
                continue
            # Separate out wellids only searches
            if len(parsed_search)==2:
                if 'plates' in parsed_search and 'wellnames' in parsed_search:
                    plates = parsed_search['plates']
                    wellnames = parsed_search['wellnames']
                    if len(plates)==1 and len(wellnames)==1:
                        wellids.add('%s:%s' %(str(plates[0]).zfill(5),wellnames[0]))
                        continue
            if 'well_ids' in parsed_search:
                wellids.update(parsed_search['well_ids'])
                continue
            if 'plates' in parsed_search:
                clause.append(_well.c.plate_number.in_(parsed_search['plates']))
            if 'plate_ranges' in parsed_search:
                for plate_range in parsed_search['plate_ranges']:
                    clause.append(_well.c.plate_number.between(
                        *plate_range, symmetric=True))
            if len(clause) > 1:
                clause = or_(*clause)
            else:
                clause = clause[0]
            if 'wellnames' in parsed_search:
                clause = and_(
                    _well.c.well_name.in_(parsed_search['wellnames']),
                    clause)
            clauses.append(clause)
        if plates_only:
            clauses.append(_well.c.plate_number.in_(plates_only))
        if wellids:
            clauses.append(_well.c.well_id.in_(wellids))
        if not clauses:
            raise ValidationError(key='well_search_data', msg='no searches found')
        if len(clauses) == 1:    
            well_query = well_query.where(clauses[0])
        else:
            well_query = well_query.where(or_(*clauses))
        
        return well_query
    
    @classmethod
    def find_wells(cls, well_search_data):
        parsed_searches = cls.parse_well_search(well_search_data)
        
        query = cls.create_well_base_query(parsed_searches)
        with get_engine().connect() as conn:
            result = conn.execute(query)
            well_ids =  [x[0] for x in result ]
            if not well_ids:
                raise ValidationError(
                    key=SCHEMA.API_PARAM_SEARCH, msg='No wells found')
            if len(well_ids) > MAX_WELL_FINDER_COUNT:
                raise ValidationError(
                    key=SCHEMA.API_PARAM_SEARCH, 
                    msg='MAX_WELL_FINDER_COUNT: %d exceeded' 
                        % MAX_WELL_FINDER_COUNT)
            return Well.objects.filter(well_id__in=well_ids)
    
        
class LibraryResource(DbApiResource):
    
    class Meta:

        queryset = Library.objects.all()  # .order_by('facility_id')
        authentication = MultiAuthentication(
            IccblBasicAuthentication(), IccblSessionAuthentication())
        resource_name = 'library'
        authorization = LibraryResourceAuthorization(resource_name)
        
        ordering = []
        filtering = {}
        serializer = LimsSerializer()
        always_return_data = True 
        
    def __init__(self, **kwargs):
        
        self.well_resource = None
        self.apilog_resource = None
        self.screen_resource = None
        super(LibraryResource, self).__init__(**kwargs)
        
    def get_apilog_resource(self):
        if not self.apilog_resource:
            self.apilog_resource = ApiLogResource()
        return self.apilog_resource
    
    def get_well_resource(self):
        if not self.well_resource:
            self.well_resource = WellResource()
        return self.well_resource

    def get_screen_resource(self):
        if not self.screen_resource:
            self.screen_resource = ScreenResource()
        return self.screen_resource

    def get_reagent_schema(self, request, **kwargs):
        if not 'short_name' in kwargs:
            raise Http404(
                'The reagent schema requires a library short name'
                ' in the URI, as in /library/[short_name]/reagent/schema/')
        short_name = kwargs.pop('short_name')
        library = Library.objects.get(short_name=short_name)
        return self.get_reagent_resource(library.screen_type).get_schema(request, **kwargs)    
  
    def prepend_urls(self):

        return [
            url(r"^(?P<resource_name>%s)/schema%s$" 
                % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_schema'), name="api_get_schema"),
            url(r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)%s$" 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_detail'), name="api_dispatch_detail"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/copy/(?P<copy_name>[^/]+)"
                 r"/plate/(?P<plate_number>[^/]+)%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_copyplateview'),
                name="api_dispatch_library_copyplateview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/copy/(?P<copy_name>[^/]+)"
                 r"/plate%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_copyplateview'),
                name="api_dispatch_library_copyplateview"),
            
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/copy/(?P<copy_name>[^/]+)"
                 r"/copywell/(?P<well_id>\d{1,5}\:[a-zA-Z]{1,2}\d{1,2})%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_copywellview'),
                name="api_dispatch_library_copywellview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/copy/(?P<copy_name>[^/]+)"
                 r"/copywellhistory/(?P<well_id>\d{1,5}\:[a-zA-Z]{1,2}\d{1,2})%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_copywellhistoryview'),
                name="api_dispatch_library_copywellhistoryview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/copy/(?P<copy_name>[^/]+)"
                 r"/copywell%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_copywellview'),
                name="api_dispatch_library_copywellview"),

            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/copy/(?P<copy_name>[^/]+)%s$") 
                 % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_copyview'),
                name="api_dispatch_library_copyview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/copy%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_copyview'),
                name="api_dispatch_library_copyview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/plate%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_copyplateview'),
                name="api_dispatch_library_copyplateview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/plate/(?P<plate_number>[^/]+)%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_copyplateview'),
                name="api_dispatch_library_copyplateview"),

            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/well%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_wellview'),
                name="api_dispatch_library_wellview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/well/apply_preview%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_well_apply_preview'),
                name="api_dispatch_library_well_apply_preview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/well/delete_preview%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_well_delete_preview'),
                name="api_dispatch_library_well_apply_preview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/well/preview%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_well_preview_view'),
                name="api_dispatch_library_well_preview"),
            
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/reagent%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_reagentview'),
                name="api_dispatch_library_reagentview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/reagent/preview/%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_library_well_preview_view'),
                name="api_dispatch_library_reagent_preview"),
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/reagent/schema%s$") 
                    % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('get_reagent_schema'),
                name="api_get_reagent_schema"),
            
            url((r"^(?P<resource_name>%s)/(?P<short_name>[\w.\-\+: ]+)"
                 r"/version%s$") % (self._meta.resource_name, TRAILING_SLASH),
                self.wrap_view('dispatch_libraryversionview'),
                name="api_dispatch_libraryversionview"),
        ]    
    
    def dispatch_library_copyview(self, request, **kwargs):
        kwargs['library_short_name'] = kwargs.pop('short_name')
        return LibraryCopyResource().dispatch('list', request, **kwargs)    
 
    def dispatch_library_copyplateview(self, request, **kwargs):
        kwargs['library_short_name'] = kwargs.pop('short_name')
        return LibraryCopyPlateResource().dispatch('list', request, **kwargs)   

    def dispatch_library_copywellview(self, request, **kwargs):
        kwargs['library_short_name'] = kwargs.pop('short_name')
        return CopyWellResource().dispatch('list', request, **kwargs)   

    def dispatch_library_copywellhistoryview(self, request, **kwargs):
        kwargs['library_short_name'] = kwargs.pop('short_name')
        return CopyWellHistoryResource().dispatch('list', request, **kwargs)   

    def dispatch_library_wellview(self, request, **kwargs):
        kwargs['library_short_name'] = kwargs.pop('short_name')
        return self.get_well_resource().dispatch('list', request, **kwargs)    

    def dispatch_library_well_apply_preview(self, request, **kwargs):
        kwargs['library_short_name'] = kwargs.pop('short_name')
        return self.get_well_resource().dispatch('preview', request, **kwargs)    

    def dispatch_library_well_preview_view(self, request, **kwargs):
#         kwargs['library_short_name'] = kwargs.pop('short_name')
#         return self.get_well_resource().dispatch('preview', request, **kwargs)    
        short_name = kwargs.pop('short_name')
        kwargs['library_short_name'] = short_name
        library = Library.objects.get(short_name=short_name)
        return self.get_reagent_resource(library.screen_type)\
            .dispatch('preview_list', request, **kwargs)
   

    @write_authorization
    @transaction.atomic
    def dispatch_library_well_delete_preview(self, request, **kwargs):
        short_name = kwargs.pop('short_name')
        kwargs['library_short_name'] = short_name
        
        if not request.method.lower() == 'post':
            raise BadRequestError(key='method', msg='Only POST is allowed')
        
        library_data = self._get_detail_response_internal(
            short_name=short_name)
        if not library_data:
            raise Http404
        
        preview_log_id = library_data.get(SCHEMA.LIBRARY.PREVIEW_LOG_ID)
        if not preview_log_id:
            raise Http404('No preview found for library %r' % short_name)
        try:
            parent_preview_log = ApiLog.objects.get(id=preview_log_id)
            parent_preview_log.delete()
            self.clear_cache(request)
            self.get_apilog_resource().clear_cache(request)
        except ObjectNotFound, e:
            raise Http404('Preview: %r for library %r not located' % (
                preview_log_id, short_name))
         
        return HttpResponse(status=204)
                    
    def dispatch_library_reagentview(self, request, **kwargs):
        short_name = kwargs.pop('short_name')
        kwargs['library_short_name'] = short_name
        library = Library.objects.get(short_name=short_name)
        return self.get_reagent_resource(library.screen_type)\
            .dispatch('list', request, **kwargs)

    @read_authorization
    def get_detail(self, request, **kwargs):

        library_short_name = kwargs.pop('short_name', None)
        if not library_short_name:
            raise MissingParam('library_short_name')
        else:
            kwargs['short_name__eq'] = library_short_name

        kwargs['visibilities'] = kwargs.get('visibilities', ['d'])
        kwargs['is_for_detail'] = True
        return self.build_list_response(request, **kwargs)
        
    @read_authorization
    def get_list(self, request, **kwargs):

        kwargs['visibilities'] = kwargs.get('visibilities', ['l'])
        return self.build_list_response(request, **kwargs)

    def build_list_response(self, request, schema=None, **kwargs):

        param_hash = self._convert_request_to_dict(request)
        param_hash.update(kwargs)
        is_data_interchange = param_hash.get(HTTP_PARAM_DATA_INTERCHANGE, False)
        use_vocab = param_hash.get(HTTP_PARAM_USE_VOCAB, False)
        use_titles = param_hash.get(HTTP_PARAM_USE_TITLES, False)
        if is_data_interchange:
            use_vocab = False
            use_titles = False
        if schema is None:
            raise Exception('schema not initialized')
        
        show_archived = parse_val(
            param_hash.get(API_PARAM_SHOW_ARCHIVED, False),
            API_PARAM_SHOW_ARCHIVED,'boolean')

        is_for_detail = kwargs.pop('is_for_detail', False)
        if is_for_detail is True:
            show_archived = True

        for_screen_facility_id = param_hash.pop('for_screen_facility_id', None)
        if for_screen_facility_id is not None:
            if self.get_screen_resource()._meta.authorization\
                .has_screen_read_authorization(
                    request.user,for_screen_facility_id) is False:
                raise PermissionDenied
            show_archived = True
        
        # general setup
        exact_fields = set(param_hash.get('exact_fields',[]))
        manual_field_includes = set(param_hash.get('includes', []))
        if exact_fields:
            if 'comment_array' in exact_fields:
                manual_field_includes.add('comment_array')
        else:
            manual_field_includes.add('comment_array')
            
        if is_for_detail:
            manual_field_includes.add('concentration_types')
        
        (filter_expression, filter_hash, readable_filter_hash) = \
            SqlAlchemyResource.build_sqlalchemy_filters(
                schema, param_hash=param_hash)
        filename = self._get_filename(
            readable_filter_hash, schema, is_for_detail)
        filter_expression = \
            self._meta.authorization.filter(request.user,filter_expression)
                
        order_params = param_hash.get('order_by', [])
        field_hash = self.get_visible_fields(
            schema['fields'], filter_hash.keys(), manual_field_includes,
            param_hash.get('visibilities'),
            exact_fields=set(param_hash.get('exact_fields', [])),
            order_params=order_params)
        
        order_clauses = SqlAlchemyResource.build_sqlalchemy_ordering(
            order_params, field_hash)
            
        rowproxy_generator = None
        if use_vocab is True:
            rowproxy_generator = \
                DbApiResource.create_vocabulary_rowproxy_generator(field_hash)
        rowproxy_generator = \
            self._meta.authorization.get_row_property_generator(
                request.user, field_hash, rowproxy_generator)

        # specific setup
        _l = self.bridge['library']
        _comment_apilogs = ApiLogResource.get_resource_comment_subquery(
            self._meta.resource_name)
        _comment_apilogs = _comment_apilogs.cte('_comment_apilogs')
        _well = self.bridge['well']
        _plate = self.bridge['plate']
        _copy = self.bridge['copy']
        _screen = self.bridge['screen']
        _ap = self.bridge['assay_plate']
        _aw = self.bridge['assay_well']
        _sr = self.bridge['screen_result']
        _apilog = self.bridge['reports_apilog']
        
        # NOTE: this query is slow
        screens_data_loaded = (
            select([_well.c.library_id, _sr.c.screen_id])
            .select_from(_aw.join(_well, _aw.c.well_id==_well.c.well_id)
                .join(_sr, _aw.c.screen_result_id==_sr.c.screen_result_id)
                .join(_screen, _sr.c.screen_id==_screen.c.screen_id))
            .where(_screen.c.study_type==None)
            .group_by(_well.c.library_id, _sr.c.screen_id)
            ).cte('screens_data_loaded')
        

        custom_columns = {
            'comment_array': (
                select([func.array_to_string(
                    func.array_agg(
                        _concat(                            
                            cast(_comment_apilogs.c.name,
                                sqlalchemy.sql.sqltypes.Text),
                            LIST_DELIMITER_SUB_ARRAY,
                            cast(_comment_apilogs.c.date_time,
                                sqlalchemy.sql.sqltypes.Text),
                            LIST_DELIMITER_SUB_ARRAY,
                            _comment_apilogs.c.comment)
                    ), 
                    LIST_DELIMITER_SQL_ARRAY) ])
                .select_from(_comment_apilogs)
                .where(_comment_apilogs.c.key==_l.c.short_name)),
            'preview_log_id': (
                select([_apilog.c.id])
                    .select_from(_apilog)
                    .where(_apilog.c.ref_resource_name==SCHEMA.LIBRARY.resource_name)
                    .where(_apilog.c.key == _l.c.short_name)
                    .where(_apilog.c.is_preview == True)
                    .limit(1)
                ),
            'preview_log_key': (
                select([_concat_with_sep((_apilog.c.ref_resource_name,
                    _apilog.c.key,func.to_char(_apilog.c.date_time,
                        'YYYY-MM-DD\"T\"HH24:MI:SS.MS')), '/')])
                    .select_from(_apilog)
                    .where(_apilog.c.ref_resource_name==SCHEMA.LIBRARY.resource_name)
                    .where(_apilog.c.key == _l.c.short_name)
                    .where(_apilog.c.is_preview == True)
                    .limit(1)
                ),
            'copy_plate_count': literal_column(
                '(select count(distinct(p.plate_id))'
                '    from plate p join copy c using(copy_id)'
                '    where c.library_id=library.library_id)'
                ).label('plate_count'),
            'plate_count': literal_column(
                '(select count(distinct(w.plate_number))'
                '    from well w'
                '    where w.library_id=library.library_id)'
                ).label('plate_count'),
            'copies': literal_column(
                "(select array_to_string(array_agg(c1.name),'%s') "
                '    from ( select c.name from copy c '
                '    where c.library_id=library.library_id '
                '    order by c.name) as c1 )' % LIST_DELIMITER_SQL_ARRAY
                ).label('copies'),
            'screening_copies': literal_column(
                "(select array_to_string(array_agg(c1.name),'%s') "
                '    from ( select distinct(c.name) from copy c '
                '    join plate p using(copy_id) '
                '    where c.library_id=library.library_id '
                "    and c.usage_type='library_screening_plates' "
                "    and p.status in ('available') "
                '    order by c.name) as c1 )' % LIST_DELIMITER_SQL_ARRAY
                ).label('copies'),
            # TODO: copies2 is the same in all respects, except that it is 
            # used differently in the UI - not displayed as a list of links
            'copies2': literal_column(
                "(select array_to_string(array_agg(c1.name),'%s') "
                '    from ( select c.name from copy c '
                '    where c.library_id=library.library_id '
                '    order by c.name) as c1 )' % LIST_DELIMITER_SQL_ARRAY
                ).label('copies2'),
            'owner': literal_column(
                "(select u.first_name || ' ' || u.last_name "
                '    from screensaver_user u '
                '    where u.screensaver_user_id=library.owner_screener_id)'
                ).label('owner'),
            'concentration_types': literal_column(
                '(select array_to_string(ARRAY['
                ' case when exists(select null from well '
                '  where well.library_id=library.library_id '
                "  and well.mg_ml_concentration is not null limit 1) then 'mg_ml' end, "
                ' case when exists(select null from well '
                '  where well.library_id=library.library_id '
                "  and well.molar_concentration is not null limit 1) then 'molar' end "
                "], '%s' ) )" % LIST_DELIMITER_SQL_ARRAY
                ),
            'molar_concentrations': (
                select([func.array_agg(func.distinct(_well.c.molar_concentration))])
                .select_from(_well)
                .where(_well.c.library_id==_l.c.library_id)
                ),
            'mg_ml_concentrations': (
                select([func.array_agg(func.distinct(_well.c.mg_ml_concentration))])
                .select_from(_well)
                .where(_well.c.library_id==_l.c.library_id)
                ),
            'date_screenable': (
                select([func.min(_plate.c.date_plated)])
                .select_from(_plate.join(_copy, _plate.c.copy_id==_copy.c.copy_id))
                .where(_copy.c.library_id == _l.c.library_id)
                ),
            'screens_screening': (
                select([func.array_to_string(
                    func.array_agg(literal_column('facility_id')),
                    LIST_DELIMITER_SQL_ARRAY)])
                .select_from(
                    select([_screen.c.facility_id])
                    .select_from(_screen.join(
                        _ap, _screen.c.screen_id==_ap.c.screen_id))
                    .where(_ap.c.plate_number.between(
                        literal_column('library.start_plate'),
                        literal_column('library.end_plate'), symmetric=True))
                    .group_by(_screen.c.facility_id)
                    .order_by(
                        "(substring({field_name}, '^[0-9]+'))::int asc nulls first " # cast to integer
                        ",substring({field_name}, '[^0-9_].*$')  asc nulls first"  # works as text
                        .format(field_name='facility_id'))
                    .alias('inner_screens_scrn')
                    )
                ),
            'screens_data_loaded': (
                select([func.array_to_string(
                    func.array_agg(literal_column('facility_id')),
                    LIST_DELIMITER_SQL_ARRAY)])
                .select_from(
                    select([_screen.c.facility_id])
                    .select_from(_screen.join(
                        screens_data_loaded, _screen.c.screen_id==screens_data_loaded.c.screen_id))
                    .where(screens_data_loaded.c.library_id==literal_column('library.library_id'))
                    .order_by(
                        "(substring({field_name}, '^[0-9]+'))::int asc nulls first " # cast to integer
                        ",substring({field_name}, '[^0-9_].*$')  asc nulls first"  # works as text
                        .format(field_name='facility_id'))
                    .alias('inner_screens_dl')
                    )
                ),
            }
                     
        base_query_tables = ['library']

        columns = self.build_sqlalchemy_columns(
            field_hash.values(), base_query_tables=base_query_tables,
            custom_columns=custom_columns)
        
        # is_released is required for the authorization filter
        columns['is_released'] = _l.c.is_released
        columns['is_archived'] = _l.c.is_archived
        # build the query statement

        j = _l
        stmt = select(columns.values()).select_from(j)

        if for_screen_facility_id:
            stmt = stmt.where(_l.c.library_id.in_(
                self.get_screen_library_ids(for_screen_facility_id)))

        if show_archived is not True and is_for_detail is not True:
            # Allow archived libraries to be shown for detail viewing (user
            # has requested the URI specifically)
            stmt = stmt.where(_l.c.is_archived == False)

        # general setup
         
        (stmt, count_stmt) = self.wrap_statement(
            stmt, order_clauses, filter_expression)
        
        if not order_clauses:
            stmt = stmt.order_by("short_name")

        # compiled_stmt = str(stmt.compile(
        #     dialect=postgresql.dialect(),
        #     compile_kwargs={"literal_binds": True}))
        # logger.info('compiled_stmt %s', compiled_stmt)
            
        title_function = None
        if use_titles is True:
            def title_function(key):
                return field_hash[key]['title']
        if is_data_interchange:
            title_function = DbApiResource.datainterchange_title_function(
                field_hash,schema['id_attribute'])
            
        if False:
            logger.info(
                'stmt: %s',
                str(stmt.compile(
                    dialect=postgresql.dialect(),
                    compile_kwargs={"literal_binds": True})))
        
        return self.stream_response_from_statement(
            request, stmt, count_stmt, filename,
            field_hash=field_hash,
            param_hash=param_hash,
            is_for_detail=is_for_detail,
            rowproxy_generator=rowproxy_generator,
            title_function=title_function, meta=kwargs.get('meta', None),
            use_caching=True)
             
    @classmethod
    def get_screen_library_ids(cls, for_screen_facility_id):
        
        bridge = get_tables()
        _screen = bridge['screen']
        _activity = bridge['activity']
        _library_screening = bridge['library_screening']
        _assay_plate = bridge['assay_plate']
        _plate = bridge['plate']
        _copy = bridge['copy']
        
        j = _screen
        j = j.join(
            _activity,
            _activity.c.screen_id == _screen.c.screen_id)
        j = j.join(
            _library_screening,
            _library_screening.c.activity_id
                == _activity.c.activity_id)
        j = j.join(
            _assay_plate,
            _library_screening.c.activity_id
                == _assay_plate.c.library_screening_id)
        j = j.join(_plate, _assay_plate.c.plate_id == _plate.c.plate_id)
        j = j.join(_copy, _copy.c.copy_id == _plate.c.copy_id)
        with get_engine().connect() as conn:
            query = (
                select([
                    distinct(_copy.c.library_id).label('library_id')])
                .select_from(j)
                .where(_screen.c.facility_id == for_screen_facility_id))
            library_ids = [x[0] for x in 
                conn.execute(query)]
            return library_ids
       
    @write_authorization
    @un_cache        
    @transaction.atomic    
    def delete_obj(self, request, deserialized, **kwargs):
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        
        id_kwargs = self.get_id(deserialized, schema=schema, **kwargs)
        Library.objects.get(**id_kwargs).delete()
    
    
    def validate(
        self, _dict, patch=False, schema=None, fields=None, library=None):
        
        errors = DbApiResource.validate(
            self, _dict, patch=patch, schema=schema, fields=fields)
    
        if patch is False:
            start_plate = _dict.get(SCHEMA.LIBRARY.START_PLATE)
            end_plate = _dict.get(SCHEMA.LIBRARY.END_PLATE)
            if not start_plate:
                errors[SCHEMA.LIBRARY.START_PLATE] = ['required',]
            elif not end_plate:
                errors[SCHEMA.LIBRARY.END_PLATE] = ['required',]
            else:
                if start_plate > end_plate:
                    raise ValidationError(
                        key=SCHEMA.LIBRARY.START_PLATE, 
                        msg='start and end plate range is out of order')
                well_test = Well.objects.all().filter(
                    plate_number__range=(start_plate, end_plate))
                if well_test.exists():
                    msg = 'part of range is already allocated to another library'
                    errors[SCHEMA.LIBRARY.START_PLATE] = [msg]
                    errors[SCHEMA.LIBRARY.END_PLATE] = [msg]
        else:
            is_released = _dict.get(SCHEMA.LIBRARY.IS_RELEASED)
            if library.is_released is True and is_released is False:
                # TODO: original_data may be passed from ApiResource.patch
                library_data = \
                    self._get_detail_response_internal(short_name=library.short_name)
                preview_log_id = library_data.get(SCHEMA.LIBRARY.PREVIEW_LOG_ID)
                if preview_log_id:
                    errors[SCHEMA.LIBRARY.IS_RELEASED] = \
                        'Pending well import preview must be released or deleted'
                        
                # check for extant data: if assay wells are loaded, 
                # IS_RELEASED may not be false
                
        return errors
    
    @write_authorization
    @transaction.atomic    
    def patch_obj(self, request, deserialized, **kwargs):
        schema = kwargs.pop('schema', None)
        if not schema:
            raise Exception('schema not initialized')
        
        id_kwargs = self.get_id(deserialized, validate=True, schema=schema, **kwargs)
        # create/update the library
        create = False
        library = None
        try:
            library = Library.objects.get(**id_kwargs)
        except ObjectDoesNotExist, e:
            create = True
            logger.info('Library %s does not exist, creating', id_kwargs)
            library = Library(**id_kwargs)

        initializer_dict = self.parse(deserialized, create=create, schema=schema)
        errors = self.validate(
            initializer_dict, schema=schema,patch=not create, library=library)
        if errors:
            raise ValidationError(errors)
        
        for key, val in initializer_dict.items():
            if hasattr(library, key):
                setattr(library, key, val)
        
        library.save()

        # now create the wells
        if create is True:
            plate_size = int(library.plate_size)
    
            try:
                i = 0
                logger.info('bulk create wells: plates: %s-%s, plate_size: %d', 
                    library.start_plate, library.end_plate, plate_size)
                for plate in range(
                        int(library.start_plate), int(library.end_plate) + 1):
                    bulk_create_wells = []
                    for index in range(0, plate_size):
                        well = Well()
                        well.well_name = lims_utils.well_name_from_index(
                            index, plate_size)
                        well.well_id = lims_utils.well_id(plate, well.well_name)
                        well.library = library
                        well.plate_number = plate
                        well.library_well_type = WELL_TYPE.UNDEFINED
                        bulk_create_wells.append(well)
                        i += 1
                        if i % 1000 == 0:
                            logger.info('queued %d wells to create', i)
                    Well.objects.bulk_create(bulk_create_wells)
                logger.info(
                    'created %d wells for library %r, %r',
                    i, library.short_name, library.library_id)
            except Exception, e:
                logger.exception('on library wells create')
                raise e

        logger.info('patch_obj done')

        # clear the cached schema because plate range have updated
        self.get_cache().delete(self._meta.resource_name + ':schema')
        
        return { API_RESULT_OBJ: library }
            

class ResourceResource(reports.api.ResourceResource):
    '''
    Motivation: to extend the reports.ResourceResource with "db" specific
    extensions.
    '''
    
    def _build_resources(self, user=None):
        
        resources = None
        if self.use_cache:
            resources = self.get_cache().get('dbresources')
        if not resources:

            resources = \
                super(ResourceResource, self)._build_resources(
                    user=user, use_cache=False)
        
            for key,resource in resources.items():
                self.extend_resource_specific_data(resource)
                
            self.get_cache().set('dbresources', resources)
    
        return resources
    
    def extend_resource_specific_data(self, resource_data):
        try:
            key = resource_data['key']
            if key == 'library':
                ranges = (Library.objects.all()
                    .order_by('start_plate')
                    .values_list('start_plate', 'end_plate'))
                plate_ranges = []
                temp = 0
                for s, e in ranges:
                    if temp == 0:
                        plate_ranges.append(s)
                    if s > temp+1:
                        plate_ranges.append(temp)
                        plate_ranges.append(s)
                    temp = e
                plate_ranges.append(temp)
                resource_data['library_plate_ranges'] = plate_ranges
     
                temp = [ 
                    x.library_type 
                    for x in Library.objects.all().distinct('library_type')]
                resource_data['extraSelectorOptions'] = { 
                    'label': 'Type',
                    'searchColumn': 'library_type',
                    'options': temp }
                
            elif key == 'librarycopyplate':
                temp = [ x for x in 
                    Plate.objects.all().distinct('status')
                        .values_list('status', flat=True)]
                resource_data['extraSelectorOptions'] = { 
                    'label': 'Type', 'searchColumn': 'status', 'options': temp }
            
            elif key == 'screen':
                temp = [ x.screen_type 
                    for x in Screen.objects.all().distinct('screen_type')]
                resource_data['extraSelectorOptions'] = { 
                    'label': 'Type', 'searchColumn': 'screen_type', 'options': temp }
        except Exception, e:
            # TODO: remove after migration
            logger.exception('catch error on extend_resource_specific_data (migration)')
        
#         elif key == 'labcherrypick':
#             resource_data['extraSelectorOptions'] = {
#                 'label': 'Status',
#                 'searchColumn': 'status',
#                 'options': ['unfulfilled','selected','plated','not_selected']}
            
